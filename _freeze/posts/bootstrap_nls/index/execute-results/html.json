{
  "hash": "cabd7f51f0fd09896cf0676f76ede22d",
  "result": {
    "markdown": "---\ntitle: Bootstrap nls models in R\ndescription: |\n  Bootstrap non-linear least squares regression in R with purrr and car\nauthor: Daniel Padfield\ndate: '2018-01-21'\ncategories:\n  - R\n  - nonlinear regression\n  - car\n  - tidyverse\nimage: \"/posts/bootstrap_nls/preview.png\"\n---\n\n\n\n\n_This post was updated to reflect the improvement of using car to bootstrap nonlinear regressions._\n\n## Introduction\n\nFor my first academic publication, a reviewer asked for the $r^{2}$ values of the thermal performance curves I fitted using non-linear regression. I bowed to the request as is often the case with reviewer comments, but would now resist as the $r^{2}$ is not necessarily an effective goodness of fit measure for non-linear regression (see this [SO answer](https://stackoverflow.com/questions/14530770/calculating-r2-for-a-nonlinear-model)). It does raise the question of how to determine how well a biologically meaningful model fits the data it is fitted to. I generally just plot every curve to its data, but it tells me nothing of the uncertainty around the curve.\n\nStep forward the bootstrap! (Non-parametric) bootstrapping is a robust way of computing parameter and model prediction confidence intervals. Bootstrapping involves simulating \"new\" datasets produced from either the original data (case resampling) or from the original model (residual resampling). \n\nThe same model is then fitted separately on each individual bootstrapped dataset. Doing this over and over allows us to visualise uncertainty of predictions and produce confidence intervals of estimated parameters. When previously implementing this, I used methods similar to previous blog posts by [Andrew MacDonald](https://rstudio-pubs-static.s3.amazonaws.com/19698_a4c472606e3c43e4b94720506e49bb7b.html) and [Hadley Wickham](https://github.com/tidyverse/dplyr/issues/269), as well as a [broom vignette](https://cran.r-project.org/web/packages/broom/vignettes/bootstrapping.html).\n\nHowever, I have since applied a more efficient method using the package **car**, which contains the function `Boot()` that provides a wrapper for the widely used function `boot::boot()` that is tailored to bootstrapping regression models.\n\n## Case resampling: Resampling the original data with replacement\n\nBootstrapping using case resampling involves simulating “new” datasets produced from the existing data by sampling with replacement. \n\n#### Case resampling: When it works\n\nWe will demonstrate an example of when this case resampling approach works using data from a recent paper by Padfield _et al._ (2020), that measures the thermal performance of the bacteria, _Pseudomonas fluorescens_, in the presence and absence of its phage, $\\phi 2$. In this study, each single growth rate estimate is a technical replicate. As such, all the data points within each phage treatment can be used to estimate the same curve. The data is in the R package [**rTPC**](https://padpadpadpad.github.io/rTPC/index.html) and we can visualise one of the curves using **ggplot2**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(boot)\nlibrary(car)\nlibrary(rTPC) #remotes::install_github('padpadpadpad/rTPC')\nlibrary(nls.multstart)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(minpack.lm)\n\n# load in data\ndata(\"bacteria_tpc\")\n\n# keep just a single curve\nd <- filter(bacteria_tpc, phage == 'nophage')\n\n# show the data\nggplot(d, aes(temp, rate)) +\n  geom_point(size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/setup-1.png){width=672}\n:::\n:::\n\n\n\n\nAs in the study, we can fit the Sharpe-Schoolfield model to the data. I take advantage of the packages **nls.mulstart** and **rTPC** to do this.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit Sharpe-Schoolfield model\nd_fit <- nest(d, data = c(temp, rate)) %>%\n  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = .x,\n                        iter = c(3,3,3,3),\n                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,\n                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,\n                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        supp_errors = 'Y',\n                        convergence_count = FALSE)),\n         # create new temperature data\n         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),\n         # predict over that data,\n         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))\n\n# unnest predictions\nd_preds <- select(d_fit, preds) %>%\n  unnest(preds)\n\n# plot data and predictions\nggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fit_and_plot-1.png){width=576}\n:::\n:::\n\n\n\n\n`nls_multstart()` is designed to fit models across a wide possible parameter space, but as it samples multiple start parameters for each model, using it with bootstrapping becomes computationally expensive. Instead, we refit the model using **minpack.lm::nlsLM()**, using the coefficients of `nls_multstart()` as the start values. The **Boot()** function then refits the model 999 times and stores the model coefficients.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# refit model using nlsLM\nfit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = d,\n                        start = coef(d_fit$sharpeschoolhigh[[1]]),\n                        lower = get_lower_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                        weights = rep(1, times = nrow(d)))\n\n# bootstrap using case resampling\nboot1 <- Boot(fit_nlsLM, method = 'case')\n\n# look at the data\nhead(boot1$t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        r_tref         e       eh       th\n[1,] 0.2387067 0.9452492 2.367318 29.24874\n[2,] 0.2511824 0.7655751 2.547971 31.27085\n[3,] 0.3173423 0.7499641 2.081992 29.59478\n[4,] 0.2586626 0.7592931 2.564117 31.18532\n[5,] 0.2871170 0.6627851 2.483048 31.94504\n[6,] 0.2434468 0.8695365 2.432169 30.15619\n```\n:::\n:::\n\n\n\nThe parameters of each bootstrapped refit are returned. All methods that are available for `boot()` and `Boot()` are supported for these objects. This includes the `hist.boot()` function which looks at the distribution of each parameter.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(boot1, layout = c(2,2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/hist_boot-1.png){width=768}\n:::\n:::\n\n\n\n\nWe can easily create predictions for each of these models and through this confidence intervals around the original fitted predictions. We can then plot (1) the bootstrapped fits and (2) the confidence regions around the model predictions.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create predictions of each bootstrapped model\nboot1_preds <- boot1$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d$temp), max(d$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot1_conf_preds <- group_by(boot1_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot1_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot1_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_boots-1.png){width=768}\n:::\n:::\n\n\n\n\nThis method works well here, because there are many points beyond the peak of the curve and multiple independent points at each temperature.\n\n#### Case resampling: When it struggles\n\nThis method becomes more problematic when there is a small sample size and the coverage of temperature values beyond the optimum temperature is small. This means that many of the bootstrapped datasets will not have any points beyond the optimum, which is problematic for mathematical models that expect a unimodal shape. The effect of this can be seen by case resampling a curve from the `chlorella_tpc` dataset also in **rTPC**. Here we again fit the model using `nls_multstart()`, refit the model using `nlsLM()`, then bootstrap the model using `Boot()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load in chlorella data\ndata('chlorella_tpc') \n\nd2 <- filter(chlorella_tpc, curve_id == 1)\n\n# fit Sharpe-Schoolfield model to raw data\nd_fit <- nest(d2, data = c(temp, rate)) %>%\n  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = .x,\n                        iter = c(3,3,3,3),\n                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,\n                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,\n                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        supp_errors = 'Y',\n                        convergence_count = FALSE)),\n         # create new temperature data\n         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),\n         # predict over that data,\n         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))\n\n# refit model using nlsLM\nfit_nlsLM2 <- nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = d2,\n                        start = coef(d_fit$sharpeschoolhigh[[1]]),\n                        lower = get_lower_lims(d2$temp, d2$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(d2$temp, d2$rate, model_name = 'sharpeschoolhigh_1981'),\n                        control = nls.lm.control(maxiter=500),\n                        weights = rep(1, times = nrow(d2)))\n\n# bootstrap using case resampling\nboot2 <- Boot(fit_nlsLM2, method = 'case')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Number of bootstraps was 995 out of 999 attempted \n```\n:::\n:::\n\n\nWe can then create predictions for each bootstrapped model and calculate 95% confidence intervals around the predictions. Models that don't fit and return `NA` for the parameter estimates are dropped.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# unnest predictions of original model fit\nd_preds <- select(d_fit, preds) %>%\n  unnest(preds)\n\n# predict over new data\nboot2_preds <- boot2$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d2$temp), max(d2$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot2_conf_preds <- group_by(boot2_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot2_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot2_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/bootstrap_case2_plot-1.png){width=768}\n:::\n:::\n\n\n\n\nAs can be seen, bootstrapping-with-replacement with only a single point at each temperature can lead to a large variety of fits. In the second panel, we can see the variation of the curve fits, clustering around 4 possible paths for the decrease in rate beyond the optimum temperature. This occurs because in many instances there are no points sampled at the very high temperatures, leading to this clustering in curve fits.\n\n## Residual resampling\n\nCase resampling is the most common way of thinking about bootstrapping. However, bootstrapping ordinary least squares regression models is often done using bootstrapping residuals. This method - where the values of the predictors in a study remain fixed during resampling - is especially useful in a designed experiment where the values of the predictors are set by the experimenter.\n\nRe-sampling residuals, at its heart, follows a simple set of steps:\n\n1. Fit the model and for each data point, $i$, retain the fitted values $\\hat{y_{i}}$ and the residuals, $\\hat{e_{i}} = y_{i} - \\hat{y_{i}}$\n2. For each data pair, ($x_i$, $y_i$), where $x_i$ is the measured temperature value, we add a randomly re-sampled residual, $\\hat{e}$ to the fitted value $\\hat{y_i}$. This becomes the new $y_i$ value, such that $y_i = \\hat{y_i} + \\hat{e}$. The new response variable is created based on the random re-allocation of the variation around the original model fit\n3. The model is refit using the newly created $y_i$ response variable\n4. Repeat steps 2 and 3 a number of times\n\nThis method makes the assumption that the original model fit is a good representation of the data, and that the error terms in the model are normally distributed and independent. If the model is incorrectly specified – for example, if there is unmodelled non-linearity, non-constant error variance, or outliers – these characteristics will not carry over into the re-sampled data sets.\n\n**car::Boot()** has an argument that allows us to easily implement residual resampling instead of case resampling, by setting `method = 'residual'`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# bootstrap using residual resampling\nboot3 <- Boot(fit_nlsLM2, method = 'residual')\n\n# predict over new data\nboot3_preds <- boot3$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d2$temp), max(d2$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot3_conf_preds <- group_by(boot3_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot3_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot3_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/residual_resample_data-1.png){width=768}\n:::\n:::\n\n\n\n\n## Calculating confidence intervals of estimated parameters\n\nBootstrapping can be used to estimate confidence intervals of the parameters explicitly modelled in the regression. We can compare these approaches to profiled confidence intervals (using **confint-MASS**) and asymptotic confidence intervals (using **nlstools::confint2()**). For the bootstrapped parameter distributions, **confint.boot()** supports the calculation of BCa, basic, normal, and percentile confidence intervals. We use BCa here, and we will calculate all CIs on the two models done previously in this vignette. First with the bacteria TPC.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First for the bacteria\n\n# get parameters of fitted model\nparam_bact <- broom::tidy(fit_nlsLM) %>%\n  select(param = term, estimate)\n\n# calculate confidence intervals of models\nci_bact1 <- nlstools::confint2(fit_nlsLM, method = 'asymptotic') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'asymptotic')\nci_bact2 <- confint(fit_nlsLM) %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'profile')\n\n# CIs from case resampling\nci_bact3 <- confint(boot1, method = 'bca') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'case bootstrap')\n\n# CIs from residual resampling\nci_bact4 <- Boot(fit_nlsLM, method = 'residual') %>%\n  confint(., method = 'bca') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'residual bootstrap')\n\nci_bact <- bind_rows(ci_bact1, ci_bact2, ci_bact3, ci_bact4) %>%\n  left_join(., param_bact)\n\n# plot\nggplot(ci_bact, aes(forcats::fct_relevel(method, c('profile', 'asymptotic')), estimate, col = method)) +\n  geom_hline(aes(yintercept = conf_lower), linetype = 2, filter(ci_bact, method == 'profile')) +\n  geom_hline(aes(yintercept = conf_upper), linetype = 2, filter(ci_bact, method == 'profile')) +\n  geom_point(size = 4) +\n  geom_linerange(aes(ymin = conf_lower, ymax = conf_upper)) +\n  theme_bw() +\n  facet_wrap(~param, scales = 'free') +\n  scale_x_discrete('', labels = function(x) stringr::str_wrap(x, width = 10)) +\n  labs(title = 'Calculation of confidence intervals for model parameters',\n       subtitle = 'For the bacteria TPC; dashed lines are CI of profiling method')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/confint_bact-1.png){width=672}\n:::\n:::\n\n\n\n\nEach panel is a different explicitly modelled parameter. The dashed lines represent the 95% intervals for the profiling method. In general, the different bootstrap methods are similar to the profiled intervals, but not all parameters are the same. For example, `r_tref` and `e` give wider (and asymmetric) confidence intervals using the case resampling method. The residual method gives estimates that are more similar to those calculated from profiling.\n\n## Calculate confidence intervals of generated parameters\n\nCrucially, bootstrapping allows the calculation of confidence intervals for parameters derived from the model that were not present in the initial fitting process. For example, the optimum temperature of a thermal performance curve, $T_{opt}$ is calculated as:\n\n$$T_{opt} = \\frac{E_{h}T_{h}}{E_{h} + k T_{h} ln(\\frac{E_{h}}{E} - 1)}$$\nWe can calculate $T_{opt}$ by writing a custom function that we feed into `Boot()`. We will do this using the case resampling approach for the first curve in this blog post.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_topt <- function(model){\n  coefs = coef(model)\n  \n  e = coefs[names(coefs) == 'e']\n  eh = coefs[names(coefs) == 'eh']\n  th = coefs[names(coefs) == 'th']\n  \n  return(((eh*(th + 273.15))/(eh + (8.62e-05 *(th + 273.15)*log((eh/e) - 1)))) - 273.15)\n}\n\ntopt <- Boot(fit_nlsLM, f = function(x){get_topt(x)}, labels = 'topt', R = 999, method = 'case')\n\nhist(topt, legend = 'none')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/ci_calc_param-1.png){width=672}\n:::\n:::\n\n\nThis approach of using **purrr**, **nls.multstart**, and **car** can easily be scaled up to multiple curves.\n\n## References\n\n- Padfield, D., Castledine, M., & Buckling, A. (2020). Temperature-dependent changes to host–parasite interactions alter the thermal performance of a bacterial host. The ISME Journal, 14(2), 389-398.\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}