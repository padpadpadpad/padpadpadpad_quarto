[
  {
    "objectID": "posts/rolling_regression/index.html",
    "href": "posts/rolling_regression/index.html",
    "title": "Rolling regression to estimate microbial growth rate",
    "section": "",
    "text": "Introduction\nMicrobial ecologists often measure the growth rates of their favourite microbes, commonly using an OD (optical density) reader, with growth being related to the increasing OD of the sample through time. There are loads of ways to analyse these curves. Most of these, like growthcurver and the methods I used in my recent ISME paper (nls.multstart and functions from nlsMicrobio), fit models to logistic growth. Both methods can easily be scaled up to fit multiple curves at the same time.\nHowever, sometimes these methods do not do a good job. The most common example when this happens is if the wee critters do not reach stationary phase. Without stationary phase, most of the models will struggle to calculate carrying capacity, which also means the estimated exponential growth rate is poor. Similar things can occur if the bacteria form biofilms. This can result in increasingly noisy measurements at higher OD readings, again making the estimation of carrying capacity and growth rates more difficult.\nAs an alternative, we can bin off trying to model the entire growth curve, and instead implement a rolling regression, where we fit a linear regression on a shifting window of points. On natural-log transformed OD, the slope of the regression between \\(logOD\\) and time is equivalent to the exponential growth rate.\n\n\nGet started\nWe will load (and install) all the packages needed to run the example here. We will use the example data from growthcurver as example OD readings from a 96-well plate. Time is in hours, and I have created stacked all the wells into a single column for OD. Finally I created a column for \\(logOD\\), which is needed for the rolling regression, and \\(log_{10}OD\\) which is needed for fitting the modified gompertz growth model from nlsMicrobio.\n\n# load packages\nlibrary(tidyverse) #install.packages(tidyverse)\nlibrary(zoo) #install.packages(zoo)\nlibrary(broom) #install.packages(broom)\nlibrary(growthcurver) # install.packages(growthcurver)\nlibrary(nls.multstart) # install.packages(nls.multstart)\n# remotes::install_github('padpadpadpad/MicrobioUoE)\n\n# load example data\nd <- growthcurver::growthdata %>%\n  gather(., well, od, -time) %>%\n  mutate(ln_od = log(od),\n         log10_od = log10(od))\n# have a look at the data\nglimpse(d)\n\nRows: 13,920\nColumns: 5\n$ time     <dbl> 0.0000000, 0.1666667, 0.3333333, 0.5000000, 0.6666667, 0.8333…\n$ well     <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"…\n$ od       <dbl> 0.05348585, 0.04800336, 0.05587451, 0.05131749, 0.04516719, 0…\n$ ln_od    <dbl> -2.928338, -3.036484, -2.884647, -2.969724, -3.097384, -2.938…\n$ log10_od <dbl> -1.271761, -1.318728, -1.252786, -1.289735, -1.345177, -1.276…\n\n\n\n\nFit modified gompertz model for bacterial growth\nWe will first demonstrate rolling regression against the modified gompertz model for growth. I like the inclusion of the lag parameter in this model, especially for OD readers where the initial inoculate can often be so low that the OD reader cannot measure it. It means that exponential growth is only calculated at OD readings that we are confident represent changes in biomass of the bacteria.\nTo do this, we’ll filter the data for just the first well, A1. Then we will fit the modified gompertz model and plot the results.\n\n# filter for just a single well\nd_a1 <- filter(d, well == 'A1')\n# define gompertz growth model\ngompertz <- function(log10_nmax, log10_n0, mumax, t, lag){\n  log10_n0 + (log10_nmax - log10_n0) * exp(-exp(mumax * exp(1) * (lag - t)/((log10_nmax - log10_n0) * log(10)) + 1))\n}\n# fit gompertz model\nfit_gomp <- nls.multstart::nls_multstart(log10_od ~ gompertz(log10_nmax, log10_n0, mumax, t = time, lag),\n           data = d_a1,\n           start_lower = c(log10_nmax = -0.75, log10_n0 = -3, mumax = 0, lag = 0),\n           start_upper = c(log10_nmax = 0.5, log10_n0 = -1, mumax = 10, lag = 25),\n           lower = c(log10_nmax = -0.6, log10_n0 = -2, mumax = 0, lag = 0),\n           iter = 500,\n           supp_errors = 'Y')\n# get predictions\ngomp_preds <- augment(fit_gomp)\n# plot on original scale\nggplot(d_a1, aes(time, od)) +\n  geom_line(aes(time, 10^.fitted), gomp_preds, col = 'red') +\n  geom_point() +\n  theme_bw(base_size = 16) +\n  labs(x = 'time (hours)',\n       y = 'OD') +\n  annotate(geom = 'text', x = 0, y = 0.37, label = paste('µ = ', round(coef(fit_gomp)[3], 2), ' hr-1', sep = ''), hjust = 0, size = MicrobioUoE::pts(16))\n\n\n\n\nHowever, lets say our measurements end at 10 or 11 hours. When we have not reached stationary phase, the traditional bacterial growth models are likely to have trouble fitting.\n\nd_a1 <- filter(d_a1, time < 10.5)\n# plot data without stationary phase\nggplot(d_a1, aes(time, od)) +\n  geom_point() +\n  theme_bw(base_size = 16) +\n  labs(x = 'time (hours)',\n       y = 'OD')\n\n\n\n\n\n\nUsing rolling regression\nRolling regression allows us to calculate exponential growth rate even when we do not have the whole curve. First, we need to create our own rolling regression function. This method is mainly taken from G. Grothendieck’s StackOverflow answer. In the function, we specify our output from a standard lm object. So if you know how to access the output of lm(), you can add any extra details you want.\nOne of the big decisions in rolling regression is deciding how many points you are going to calculate the growth rate over. In this example, measurements are taken every 0.167 hours, about every ten minutes. I want a shifting window to span a minimum of 1.5 hours, so I calculate num_points to define the number of points the rolling regression will act on.\nWe then run the rolling regression, using zoo::rollapplyr() and dplyr::do(). Finally, in order to illustrate what the rolling regression is doing, I created a predictions dataframe for every single linear model that is fitted.\n\n# create the rolling regression function\nroll_regress <- function(x){\n  temp <- data.frame(x)\n  mod <- lm(temp)\n  temp <- data.frame(slope = coef(mod)[[2]],\n                     slope_lwr = confint(mod)[2, ][[1]],\n                     slope_upr = confint(mod)[2, ][[2]],\n                     intercept = coef(mod)[[1]],\n                     rsq = summary(mod)$r.squared, stringsAsFactors = FALSE)\n  return(temp)\n}\n# define window - here every ~1.5 hours\nnum_points = ceiling(1.5*60/(60*0.167)) \n\n# run rolling regression on ln od ~ time\nmodels <- d_a1 %>%\n  do(cbind(model = dplyr::select(., ln_od, time) %>% \n           zoo::rollapplyr(width = num_points, roll_regress, by.column = FALSE, fill = NA, align = 'center'),\n           time = dplyr::select(., time),\n           ln_od = dplyr::select(., ln_od))) %>%\n  rename_all(., gsub, pattern = 'model.', replacement = '')\n# create predictions\npreds <- models %>%\n  filter(., !is.na(slope)) %>%\n  group_by(time) %>%\n  do(data.frame(time2 = c(.$time - 2, .$time + 2))) %>%\n  left_join(., models) %>%\n  mutate(pred = (slope*time2) + intercept)\n\nWe can plot the rolling regression through time. I have extracted the exponential growth rate as the maximum slope of any of the regressions. Reassuringly the value of \\(\\mu\\) we get is very similar to that of using the gompertz model. I have also plotted the time at which the maximum slope occurred. It looks pretty close to mid-log to me.\n\n# calculate the exponential growth rate\ngrowth_rate <- filter(models, slope == max(slope, na.rm = TRUE))\n# plot rolling regression\nggplot(d_a1, aes(time, ln_od)) +\n  geom_point() +\n  geom_line(aes(time2, pred, group = time), col = 'red', preds, alpha = 0.5) +\n  theme_bw(base_size = 16) +\n  geom_segment(aes(x = time, y = -3, xend = time, yend = ln_od), growth_rate) +\n  geom_segment(aes(x = 0, y = ln_od, xend = time, yend = ln_od), growth_rate) +\n  annotate(geom = 'text', x = 0, y = -1, label = paste('µ = ', round(growth_rate$slope, 2), ' hr-1\\n95%CI:(',round(growth_rate$slope_lwr, 2), '-', round(growth_rate$slope_upr, 2), ')', sep = ''), hjust = 0, size = MicrobioUoE::pts(16)) +\n  labs(x = 'time (hours)',\n       y = 'OD')\n\n\n\n\n\n\nThe opportunities are endless\nThe great thing about this approach is its flexibility. It can easily be rolled out over all the wells in that plate, using group_by().\n\n# run rolling regression on ln od_cor ~ time\nmodels <- d %>%\n  group_by(well) %>%\n  do(cbind(model = select(., ln_od, time) %>% \n           zoo::rollapplyr(width = num_points, roll_regress, by.column = FALSE, fill = NA, align = 'center'),\n           time = select(., time),\n           ln_od = select(., ln_od))) %>%\n  rename_all(., gsub, pattern = 'model.', replacement = '')\n# calculate growth rate for each one\ngrowth_rates <- models %>%\n  filter(slope == max(slope, na.rm = TRUE)) %>%\n  ungroup()\n\n|===========================================|100% ~0 s remaining \n\nglimpse(models)\n\nRows: 13,920\nColumns: 8\nGroups: well [96]\n$ well      <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", …\n$ slope     <dbl> NA, NA, NA, NA, -0.077427705, -0.028208492, -0.021245636, 0.…\n$ slope_lwr <dbl> NA, NA, NA, NA, -0.199773163, -0.160658493, -0.157480637, -0…\n$ slope_upr <dbl> NA, NA, NA, NA, 0.04491775, 0.10424151, 0.11498936, 0.131953…\n$ intercept <dbl> NA, NA, NA, NA, -2.944791, -2.976248, -2.967393, -3.024750, …\n$ rsq       <dbl> NA, NA, NA, NA, 0.2423791383, 0.0349643445, 0.0190560342, 0.…\n$ time      <dbl> 0.0000000, 0.1666667, 0.3333333, 0.5000000, 0.6666667, 0.833…\n$ ln_od     <dbl> -2.928338, -3.036484, -2.884647, -2.969724, -3.097384, -2.93…\n\n\nThese growth rates can then be used for downstream analyses, and the method can easily be used over multiple plates and for many different types of data. Finally, you could also filter the regressions by \\(R^{2}\\) values, making sure you only kept good fitting regressions. Or do a sensitivity analysis of different sized window sizes to make sure your chosen window is suitable.\nHow do you get your data off of the OD reader? I have written scripts to collate hundreds of plate readings into a single dataframe in R. Let me know if you would like me to do a blog post on that process! Thanks for reading."
  },
  {
    "objectID": "posts/nlsmultstart/index.html",
    "href": "posts/nlsmultstart/index.html",
    "title": "Doing robust nls regression in R",
    "section": "",
    "text": "With my research, I often use non-linear least squares regression to fit a model with biologically meaningful parameters to data. Specifically, I measure the thermal performance of phytoplankon growth, respiration and photosynthesis over a wide range of assay temperatures to see how the organisms are adapted to the temperatures they live at.\nThese thermal performance curves generally follow a unimodal shape and parameters for which are widely used in climate change research to predict whether organisms will be able to cope with increasing temperatures.\n\n\n\nExample Thermal Performance Curve\n\n\nThese curves can be modelled with a variety of equations, such as the Sharpe-Schoolfield equation, which I have log-transformed here:\n\\[log(rate) = lnc + E(\\frac{1}{T_{c}} - \\frac{1}{kT}) - ln(1 + e^{E_h(\\frac{1}{kT_h} - \\frac{1}{kT})})\\] where \\(lnc\\) is a normalisation constant at a common temperature, \\(T_{c}\\), \\(E\\) is an activation energy that describes the rate of increase before the optimum temperature, \\(T_{opt}\\). \\(k\\) is Boltzmann’s constant, \\(E_{h}\\) is the deactivation energy that controls the decline in rate past the optimum temperature and \\(T_{h}\\) is the temperature where, after the optimu, the rate is half of the maximal rate.\nSay I want to fit the same equation to 10, 50, or 100s of these curves. I could loop through a call to nls(), nlsLM(), or use nlsList() from nlme. However, non-linear least squares regression in R is sensitive to the start parameters, meaning that different start parameters can give different “best estimated parameters”. This becomes more likely when fitting more curves with only a single set of start parameters, where the variation in estimated parameter values is likely to be much larger. For example, some curves could have much higher rates (\\(lnc\\)), higher optimum temperatures (i.e. \\(T_{h}\\)) or have different values of temperature-dependence (\\(E\\)).\nTo combat this, I wrote an R package which allows for multiple start parameters for non-linear regression. I wrapped this method in an R package called nlsLoop and submitted it to The Journal of Open Source Software. Everything was good with the world and I went to a Christmas party.\nThe next day, I had an epiphany surrounding the redundancies and needless complexities of my R package, withdrew my submission and rewrote the entire package in a weekend to give rise to a single function package, nls.multstart::nls_multstart(). Essentially since I first wrote nlsLoop ~3 years ago I have realised that broom and purrr can do what I wrote clunkier functions to achieve. In contrast, nls.multstart works perfectly with the tools of the tidyverse to fit multiple models."
  },
  {
    "objectID": "posts/nlsmultstart/index.html#multiple-model-fitting-in-practice",
    "href": "posts/nlsmultstart/index.html#multiple-model-fitting-in-practice",
    "title": "Doing robust nls regression in R",
    "section": "Multiple model fitting in practice",
    "text": "Multiple model fitting in practice\nLoad in all packages that are used in this analysis. Packages can be installed from GitHub using devtools.\n\n# load packages\nlibrary(nls.multstart)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(nlstools)\n\nWe can then load in the data and have a look at it using glimpse(). Here we shall use a dataset of thermal performance curves of metabolism of Chlorella vulgaris from Padfield et al. 2016.\n\n# load in example data set\ndata(\"Chlorella_TRC\")\nglimpse(Chlorella_TRC)\n\nRows: 649\nColumns: 7\n$ curve_id    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2…\n$ growth.temp <dbl> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20…\n$ process     <chr> \"acclimation\", \"acclimation\", \"acclimation\", \"acclimation\"…\n$ flux        <chr> \"respiration\", \"respiration\", \"respiration\", \"respiration\"…\n$ temp        <dbl> 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 16, 19, 22…\n$ K           <dbl> 289.15, 292.15, 295.15, 298.15, 301.15, 304.15, 307.15, 31…\n$ ln.rate     <dbl> -2.06257833, -1.32437939, -0.95416807, -0.79443675, -0.182…\n\n\nNext we define the Sharpe-Schoolfield equation discussed earlier.\n\n# define the Sharpe-Schoolfield equation\nschoolfield_high <- function(lnc, E, Eh, Th, temp, Tc) {\n  Tc <- 273.15 + Tc\n  k <- 8.62e-5\n  boltzmann.term <- lnc + log(exp(E/k*(1/Tc - 1/temp)))\n  inactivation.term <- log(1/(1 + exp(Eh/k*(1/Th - 1/temp))))\n  return(boltzmann.term + inactivation.term)\n  }\n\nThere are 60 curves in this dataset, 30 each of photosynthesis and respiration. The treatments are growth temperature (20, 23, 27, 30, 33 ºC) and adaptive process (acclimation or adaptation) that reflects the number of generations cultures were grown at each temperature.\nWe can see how nls_multstart() works by subsetting the data for a single curve.\n\n# subset dataset\nd_1 <- subset(Chlorella_TRC, curve_id == 1)\n# run nls_multstart\nfit <- nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),\n                     data = d_1,\n                     iter = 500,\n                     start_lower = c(lnc = -10, E = 0.1, Eh = 0.2, Th = 285),\n                     start_upper = c(lnc = 10, E = 2, Eh = 5, Th = 330),\n                     supp_errors = 'Y',\n                     na.action = na.omit,\n                     lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))\nfit\n\nNonlinear regression model\n  model: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)\n   data: data\n     lnc        E       Eh       Th \n -1.3462   0.9877   4.3326 312.1887 \n residual sum-of-squares: 7.257\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 1.49e-08\n\n\nnls_multstart() allows boundaries for each parameter to be set. A uniform distribution between these values is created and start values for each iteration of the fitting process are then picked randomly. The function returns the best available model by picking the model with the lowest AIC score. Additional info on the function can be found here or by typing ?nls_multstart into the R console.\nThis fit can then be “tidied” in various ways using the R package broom. Each different function in broom returns a different set of information. tidy() returns the estimated parameters, augment() returns the predictions and glance() returns information about the model such as the AIC score and whether the model has reached convergence. Confidence intervals of non-linear regression can also be estimated using nlstools::confint2()\nThe amazing thing about these tools is the ease at which they can then be used on multiple curves at once, an approach Hadley Wickham has previously written about. The approach nests the data based on grouping variables using nest(), then creates a list column of the best fit for each curve using map().\n\n# fit over each set of groupings\nfits <- Chlorella_TRC %>%\n  group_by(., flux, growth.temp, process, curve_id) %>%\n  nest() %>%\n  mutate(fit = purrr::map(data, ~ nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),\n                                   data = .x,\n                                   iter = 1000,\n                                   start_lower = c(lnc = -10, E = 0.1, Eh = 0.2, Th = 285),\n                                   start_upper = c(lnc = 10, E = 2, Eh = 5, Th = 330),\n                                   supp_errors = 'Y',\n                                   na.action = na.omit,\n                                   lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))))\n\nIf you are confused, then you are not alone. This took me a long time to understand and I imagine there are still better ways for me to do it! However, to check it has worked, we can look at a single fit to check it looks ok. We can also look at fits to see that there is now a fit list column containing each of the non-linear fits for each combination of our grouping variables.\n\n# look at a single fit\nsummary(fits$fit[[1]])\n\n\nFormula: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)\n\nParameters:\n    Estimate Std. Error t value Pr(>|t|)    \nlnc  -1.3462     0.4656  -2.891   0.0202 *  \nE     0.9877     0.4521   2.185   0.0604 .  \nEh    4.3326     1.4878   2.912   0.0195 *  \nTh  312.1887     3.8782  80.499 6.32e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9524 on 8 degrees of freedom\n\nNumber of iterations to convergence: 19 \nAchieved convergence tolerance: 1.49e-08\n\n# look at output object\nselect(fits, curve_id, data, fit)\n\nAdding missing grouping variables: `flux`, `growth.temp`, `process`\n\n\n# A tibble: 60 × 6\n# Groups:   flux, growth.temp, process, curve_id [60]\n   flux        growth.temp process     curve_id data              fit   \n   <chr>             <dbl> <chr>          <dbl> <list>            <list>\n 1 respiration          20 acclimation        1 <tibble [12 × 3]> <nls> \n 2 respiration          20 acclimation        2 <tibble [12 × 3]> <nls> \n 3 respiration          23 acclimation        3 <tibble [12 × 3]> <nls> \n 4 respiration          27 acclimation        4 <tibble [9 × 3]>  <nls> \n 5 respiration          27 acclimation        5 <tibble [12 × 3]> <nls> \n 6 respiration          30 acclimation        6 <tibble [12 × 3]> <nls> \n 7 respiration          30 acclimation        7 <tibble [12 × 3]> <nls> \n 8 respiration          33 acclimation        8 <tibble [10 × 3]> <nls> \n 9 respiration          33 acclimation        9 <tibble [8 × 3]>  <nls> \n10 respiration          20 acclimation       10 <tibble [10 × 3]> <nls> \n# … with 50 more rows\n\n\nThese fits can be cleaned up using the broom functions and purrr::map() to iterate over the grouping variables.\n\n# get summary info\ninfo <- fits %>%\n  mutate(., info = map(fit, glance)) %>%\n  unnest(info)\n\n# get params\nparams <- fits %>%\n  mutate(., params = map(fit, tidy)) %>%\n  unnest(params)\n  \n# get confidence intervals\nCI <- fits %>% \n  mutate(., CI = map(fit, function(x)data.frame(confint2(x)))) %>%\n  unnest(CI) %>%\n  select(-data, -fit) %>%\n  rename(., conf.low = `X2.5..`, conf.high = `X97.5..`) %>%\n  group_by(., curve_id) %>%\n  mutate(., term = c('lnc', 'E', 'Eh', 'Th')) %>%\n  ungroup()\n# merge parameters and CI estimates\nparams <- merge(params, CI, by = intersect(names(params), names(CI)))\n\n# get predictions\npreds <- fits %>%\n  mutate(., preds = map(fit, augment)) %>%\n  unnest(preds)\n\nLooking at info allows us to see if all the models converged.\n\nungroup(info) %>% select(., curve_id, logLik, AIC, BIC, deviance, df.residual)\n\n# A tibble: 60 × 6\n   curve_id  logLik   AIC   BIC deviance df.residual\n      <dbl>   <dbl> <dbl> <dbl>    <dbl>       <int>\n 1        1 -14.0   38.0  40.4     7.26            8\n 2        2  -1.20  12.4  14.8     0.858           8\n 3        3  -7.39  24.8  27.2     2.41            8\n 4        4  -0.523 11.0  12.0     0.592           5\n 5        5 -10.8   31.7  34.1     4.29            8\n 6        6  -8.52  27.0  29.5     2.91            8\n 7        7  -1.29  12.6  15.0     0.871           8\n 8        8 -13.4   36.7  38.2     8.48            6\n 9        9   1.82   6.36  6.76    0.297           4\n10       10  -1.27  12.5  14.1     0.755           6\n# … with 50 more rows\n\n\nWhen plotting non-linear fits, I prefer to have a smooth curve, even when there are not many points underlying the fit. This can be achieved by including newdata in the augment() function and creating a higher resolution set of predictor values.\nHowever, when predicting for many different fits, it is not certain that each curve has the same range of predictor variables. We can get around this by setting the limits of each prediction by the min() and max() of the predictor variables.\n\n# new data frame of predictions\nnew_preds <- Chlorella_TRC %>%\n  do(., data.frame(K = seq(min(.$K), max(.$K), length.out = 150), stringsAsFactors = FALSE))\n# max and min for each curve\nmax_min <- group_by(Chlorella_TRC, curve_id) %>%\n  summarise(., min_K = min(K), max_K = max(K), .groups = 'drop')\n\n# create new predictions\npreds2 <- fits %>%\n  mutate(preds = map(fit, augment, newdata = new_preds)) %>%\n  unnest(preds) %>%\n  merge(., max_min, by = 'curve_id') %>%\n  group_by(., curve_id) %>%\n  filter(., K > unique(min_K) & K < unique(max_K)) %>%\n  rename(., ln.rate = .fitted) %>%\n  ungroup()\n\nThese can then be plotted using ggplot2.\n\n# plot\nggplot() +\n  geom_point(aes(K - 273.15, ln.rate, col = flux), size = 2, Chlorella_TRC) +\n  geom_line(aes(K - 273.15, ln.rate, col = flux, group = curve_id), alpha = 0.5, preds2) +\n  facet_wrap(~ growth.temp + process, labeller = labeller(.multi_line = FALSE)) +\n  scale_colour_manual(values = c('green4', 'black')) +\n  theme_bw(base_size = 12, base_family = 'Helvetica') +\n  ylab('log Metabolic rate') +\n  xlab('Assay temperature (ºC)') +\n  theme(legend.position = c(0.9, 0.15))\n\n\n\n\nThe confidence intervals of each parameter for each curve fit can also be easily visualised.\n\n# plot\nggplot(params, aes(col = flux)) +\n  geom_point(aes(curve_id, estimate)) +\n  facet_wrap(~ term, scale = 'free_x', ncol = 4) +\n  geom_linerange(aes(curve_id, ymin = conf.low, ymax = conf.high)) +\n  coord_flip() +\n  scale_color_manual(values = c('green4', 'black')) +\n  theme_bw(base_size = 12, base_family = 'Helvetica') +\n  theme(legend.position = 'top') +\n  xlab('curve') +\n  ylab('parameter estimate')\n\n\n\n\nThis method of modelling can be used for different data, different non-linear models (and linear models for that matter) and combined with the tidyverse can make very useful visualisations.\nThe next stage of these curve fits is to try and better understand the uncertainty of these curve fits and their predictions. One approach to achieve this could be bootstrapping new datasets from the existing data. I hope to demonstrate how this could be done soon in another post."
  },
  {
    "objectID": "posts/nlsmultstart/index.html#references",
    "href": "posts/nlsmultstart/index.html#references",
    "title": "Doing robust nls regression in R",
    "section": "References",
    "text": "References\n[1] Padfield, D., Yvon-durocher, G., Buckling, A., Jennings, S. & Yvon-durocher, G. (2016). Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton. Ecology Letters, 19(2), 133-142."
  },
  {
    "objectID": "posts/animate_rstrava/index.html",
    "href": "posts/animate_rstrava/index.html",
    "title": "rStrava and gganimate",
    "section": "",
    "text": "rStrava is an R package that allows you to access data from Strava using the Strava API. Some of the functions of rStrava scrape data from the public Strava website but to access your own data you will need a Strava profile and an authentication token. Details on obtaining your unique token can be found on the rStrava GitHub In addition to this key, we use rgbif::elevation() to calculate the elevation of each route. This requires a Google API key which can be created here.\nGot a Strava authentication token? Got a Google API key? We are ready to create some animations! To create our animations, we use gganimate that requires ImageMagick to be installed."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#loading-packages-and-defining-tokens",
    "href": "posts/animate_rstrava/index.html#loading-packages-and-defining-tokens",
    "title": "rStrava and gganimate",
    "section": "Loading packages and defining tokens",
    "text": "Loading packages and defining tokens\nFirst load the packages that are used in the script and our Strava and Google authentication tokens. The app_scope argument in strava_oauth() has to be one of “read” , “read_all”, “profile:read_all”, “profile:write”, “activity:read”, “activity:read_all” or “activity:write”. To access your activities, activity:read_all has to be included.\n\n# load packages ####\nlibrary(rStrava) # devtools::install_github('fawda123/rStrava')\nlibrary(gganimate)\nlibrary(tidyverse)\nlibrary(sp)\nlibrary(ggmap)\nlibrary(raster)\n\n# initial setup ####\n# Strava key\napp_name <- 'xxxx'\napp_client_id <- 'xxxxx'\napp_secret <- '\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"'\n\n# create strava token\nmy_token <- httr::config(token = strava_oauth(app_name, app_client_id, app_secret, app_scope = 'read_all,activity:read_all'))\n\n# Google elevation API key\nGoogleAPI <- 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n\nA browser window should open at this point saying Authentication complete. Please close this page and return to R. This means everything is going well!"
  },
  {
    "objectID": "posts/animate_rstrava/index.html#download-your-data",
    "href": "posts/animate_rstrava/index.html#download-your-data",
    "title": "rStrava and gganimate",
    "section": "Download your data",
    "text": "Download your data\nWe can then download our personal activity data using the rStrava::get_activity_list(). This function needs your strava token and your strava athlete id. For example, my strava id is 2140248.\n\n# download strava data\nmy_acts <- get_activity_list(my_token)\n\nlength(my_acts)\n\n[1] 1664\n\n\nThis returns a large list of all your previous activities. Mine has 1028 previous entries. If you want to explore your list, you can use View(my_acts) in RStudio which opens the Data Viewer window."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#compile-your-data-into-tidy-dataframe",
    "href": "posts/animate_rstrava/index.html#compile-your-data-into-tidy-dataframe",
    "title": "rStrava and gganimate",
    "section": "Compile your data into “tidy” dataframe",
    "text": "Compile your data into “tidy” dataframe\nrStrava has a function that compiles the information stored in the output of get_activity_list() to a “tidy” dataframe, with one row for each activity. compile_activities() finds all the columns across all activities and returns NA when a column is not present in a given activity. This means that if HR was not measured across all your strava activities, the function will still work!\n\n# compile activities into a tidy dataframe\nmy_acts <- compile_activities(my_acts)\n\n# have a look at the dataframe\ndplyr::glimpse(my_acts)\n\nRows: 1,664\nColumns: 60\n$ achievement_count             <dbl> 1, 11, 1, 23, 3, 2, 0, 0, 28, 10, 0, 0, …\n$ athlete_count                 <dbl> 1, 1, 1, 51, 1, 1, 1, 1, 1, 7, 1, 1, 1, …\n$ athlete.id                    <chr> \"2140248\", \"2140248\", \"2140248\", \"214024…\n$ athlete.resource_state        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ average_cadence               <chr> \"79.5\", \"81.3\", \"80.1\", \"84.9\", \"82.2\", …\n$ average_heartrate             <chr> \"142.1\", \"144.7\", \"153.4\", \"163.2\", \"134…\n$ average_speed                 <dbl> 12.1248, 12.7764, 12.2328, 15.4836, 12.7…\n$ comment_count                 <dbl> 2, 0, 0, 3, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0…\n$ commute                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ display_hide_heartrate_option <chr> \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", …\n$ distance                      <dbl> 6.3254, 15.3099, 3.0110, 5.0749, 3.9523,…\n$ elapsed_time                  <dbl> 1923, 4469, 895, 1180, 1597, 3372, 5257,…\n$ elev_high                     <dbl> 75.9, 102.7, 75.9, 53.6, 75.9, 22.5, 200…\n$ elev_low                      <dbl> 68.3, 45.1, 55.6, 49.9, 49.9, 3.4, 14.8,…\n$ end_latlng1                   <dbl> 51.91218, 51.91868, 51.91910, 51.91133, …\n$ end_latlng2                   <dbl> -2.053098, -2.048171, -2.049546, -2.0769…\n$ external_id                   <chr> \"garmin_ping_258943346472\", \"garmin_ping…\n$ flagged                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ from_accepted_tag             <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ gear_id                       <chr> \"g12286967\", \"g12438516\", \"g12286967\", \"…\n$ has_heartrate                 <chr> \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", …\n$ has_kudoed                    <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ heartrate_opt_out             <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ id                            <dbl> 8519266303, 8507709513, 8501999743, 8501…\n$ kudos_count                   <dbl> 3, 27, 5, 26, 4, 11, 14, 20, 37, 28, 1, …\n$ location_country              <chr> \"United Kingdom\", \"United Kingdom\", \"Uni…\n$ manual                        <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ map.id                        <chr> \"a8519266303\", \"a8507709513\", \"a85019997…\n$ map.resource_state            <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ map.summary_polyline          <chr> \"ec{{H|aoKPHP@bAb@h@L|ATh@@ZJb@?b@CJCrBA…\n$ max_heartrate                 <chr> \"166\", \"173\", \"169\", \"174\", \"147\", \"173\"…\n$ max_speed                     <dbl> 19.9656, 16.5240, 23.7168, 19.1880, 19.2…\n$ moving_time                   <dbl> 1878, 4314, 886, 1180, 1119, 3148, 5185,…\n$ name                          <chr> \"Morning Run\", \"Morning Run\", \"Morning R…\n$ photo_count                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pr_count                      <chr> \"0\", \"7\", \"0\", \"9\", \"0\", \"1\", \"0\", \"0\", …\n$ private                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ resource_state                <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ sport_type                    <chr> \"Run\", \"Run\", \"Run\", \"Run\", \"Run\", \"Run\"…\n$ start_date                    <chr> \"2023-02-07T08:29:54Z\", \"2023-02-05T08:2…\n$ start_date_local              <chr> \"2023-02-07T08:29:54Z\", \"2023-02-05T08:2…\n$ start_latlng1                 <dbl> 51.91904, 51.91906, 51.90978, 51.91065, …\n$ start_latlng2                 <dbl> -2.049192, -2.049080, -2.068779, -2.0750…\n$ suffer_score                  <chr> \"33\", \"86\", \"27\", \"55\", \"14\", \"50\", \"29\"…\n$ timezone                      <chr> \"(GMT+00:00) Europe/London\", \"(GMT+00:00…\n$ total_elevation_gain          <dbl> 15.0, 121.2, 25.1, 9.9, 5.1, 4.7, 463.0,…\n$ total_photo_count             <dbl> 0, 3, 0, 3, 0, 3, 0, 5, 3, 1, 0, 0, 0, 0…\n$ trainer                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ type                          <chr> \"Run\", \"Run\", \"Run\", \"Run\", \"Run\", \"Run\"…\n$ upload_id                     <chr> \"9143955806\", \"9131036041\", \"9124588035\"…\n$ upload_id_str                 <chr> \"9143955806\", \"9131036041\", \"9124588035\"…\n$ utc_offset                    <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", …\n$ visibility                    <chr> \"everyone\", \"everyone\", \"everyone\", \"eve…\n$ average_temp                  <chr> NA, NA, NA, NA, NA, NA, \"9\", NA, NA, NA,…\n$ average_watts                 <dbl> NA, NA, NA, NA, NA, NA, 130.8, NA, NA, N…\n$ device_watts                  <chr> NA, NA, NA, NA, NA, NA, \"FALSE\", NA, NA,…\n$ kilojoules                    <dbl> NA, NA, NA, NA, NA, NA, 678.2, NA, NA, N…\n$ location_city                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ location_state                <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ workout_type                  <chr> NA, \"0\", NA, \"0\", NA, \"0\", NA, \"0\", \"0\",…\n\n\nThere are so many columns here, so I remove some columns I am not interested in for this post and do some data transformations to get the date in a correct format. I also do not want to run the example on all my rides, instead I filter them for just 2020.\n\n# columns to keep\ndesired_columns <- c('distance', 'elapsed_time', 'moving_time', 'start_date', 'start_date_local', 'type', 'map.summary_polyline', 'location_city', 'upload_id')\n\n# keep only desired columns\nmy_acts2 <- dplyr::select(my_acts, any_of(desired_columns))\n\n# transformations ####\nmy_acts <- mutate(my_acts,\n                  activity_no = seq(1,n(), 1),\n                  elapsed_time = elapsed_time/60/60,\n                  moving_time = moving_time/60/60, \n                  date = gsub(\"T.*$\", '', start_date) %>%\n                    as.POSIXct(., format = '%Y-%m-%d'),\n                  EUdate = format(date, '%d/%m/%Y'),\n                  month = format(date, \"%m\"),\n                  day = format(date, \"%d\"),\n                  year = format(date, \"%Y\")) %>%\n  mutate(., across(c(month, day), as.numeric)) %>%\n  filter(.,year == '2020')"
  },
  {
    "objectID": "posts/animate_rstrava/index.html#get-latitude-and-longitude-for-each-activity",
    "href": "posts/animate_rstrava/index.html#get-latitude-and-longitude-for-each-activity",
    "title": "rStrava and gganimate",
    "section": "Get latitude and longitude for each activity",
    "text": "Get latitude and longitude for each activity\nEach activity has a bunch of data associated with it. For mapping, I am interested in the map.summary_polyline, which is a Google polyline which allows the encoding of multiple latitude and longitude points as a single string. We can get the latitude and longitude for each of the activities by using get_latlon() which decodes the polylines and using dplyr and purrr to iterate over every activity in the dataframe. I add my Google API key as a column so that map() can easily find it.\n\n# get lat lon and distance of every ride ####\nlat_lon <- my_acts %>%\n  filter(!is.na(map.summary_polyline)) %>%\n  filter(type == 'Ride') %>%\n  mutate(key = GoogleAPI) %>%\n  group_by(activity_no) %>%\n  nest() %>%\n  mutate(coords = map(data, ~get_latlon(.$map.summary_polyline, key = .$key)),\n         dist = map(coords, ~get_dists(.x$lon, .x$lat))) %>%\n  unnest(., data) %>%\n  unnest(., c(coords, dist))\n\nHaving got the latitude and longitude for every ride, we can now get the elevation of each point and then calculate the gradient between points. To do this I use elevation() in the R package rgbif. To use this, you need to get a GeoNames username by registering for an account at http://www.geonames.org/login.\n\n# get elevation and calculate gradient between points\nlat_lon <- ungroup(lat_lon) %>%\n  mutate(., ele = rgbif::elevation(latitude = .$lat, longitude = .$lon, user = 'YOUR USERNAME HERE', model = 'srtm1')$elevation_geonames)\n\nlat_lon <- group_by(lat_lon, activity_no) %>%\n  mutate(., ele_diff = c(0, diff(ele)),\n         dist_diff = c(0, diff(dist)),\n         grad = c(0, (ele_diff[2:n()]/10)/dist_diff[2:n()])) %>%\n  ungroup() %>%\n  dplyr::select(., -c(ele_diff, dist_diff))\n\n\n\n\nThis now gives us a data frame of all my rides from 2020 with the latitude, longitude, cumulative distance, elevation and gradient. It would now be super easy to create elevation profiles, but I will save that for another post."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#create-a-gif-of-a-single-ride",
    "href": "posts/animate_rstrava/index.html#create-a-gif-of-a-single-ride",
    "title": "rStrava and gganimate",
    "section": "Create a gif of a single ride",
    "text": "Create a gif of a single ride\nWe now have almost all the components to create a gif of a single ride.\n\nlat_lon_single <- filter(lat_lon, activity_no == min(activity_no))\nnrow(lat_lon_single)\n\n[1] 343\n\n\nHowever, Google polylines do not give a consistent number of latitude and longitude points. This means it might be hard to get a smooth elevation profile for the ride and also for smooth transitions in a gif. To create a set number of points from the current polyline, we can use geospatial packages such as sp and raster to interpolate a desired number of points from the current ones. Here I create 250 points.\n\n# reorder columns so lat lon are first\nlat_lon_single <- dplyr::select(lat_lon_single, lat, lon, everything())\n\n# make new data with by interpolation\ninterp <- raster::spLines(as.matrix(lat_lon_single[,1:2])) %>%\n  sp::spsample(., n = 250, type = 'regular') %>%\n  data.frame() %>%\n  mutate(., dist = get_dists(lon, lat),\n         ele = rgbif::elevation(latitude = .$lat, longitude = .$lon, user = 'padpadpadpad', model = 'srtm1')$elevation_geoname,\n         ele_diff = c(0, diff(ele)),\n         dist_diff = c(0, diff(dist)),\n         grad = c(0, (ele_diff[2:n()]/10)/dist_diff[2:n()]),\n         n = row_number())\n\nWe can now put the gif together, using ggmap and ggplot2. We use gganimate to make the plot animated.\n\n# make bbox\nbbox <- ggmap::make_bbox(lon, lat, data = lat_lon_single, f = 1.3)\n\n# download map\nmap <- get_map(location = bbox, source = 'google', maptype = 'terrain')\n\nsingle_ride <- ggmap(map, darken = 0.15) +\n  geom_path(aes(x = lon, y = lat,  col = grad, group = 1), data = interp, size = 2, alpha = 1) +\n  scale_color_distiller('Gradient (%)', palette = 'Spectral') +\n  labs(title = '') +\n  coord_cartesian() +\n  ggforce::theme_no_axes(theme_bw(base_size = 16)) +\n  transition_reveal(dist)\n\n# animate plot\ngganimate::anim_save('where_to_save.gif', single_ride, width = 1000, height = 700)\n\nThe output of this code can be seen below."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#create-a-gif-of-all-the-rides",
    "href": "posts/animate_rstrava/index.html#create-a-gif-of-all-the-rides",
    "title": "rStrava and gganimate",
    "section": "Create a gif of ALL the rides",
    "text": "Create a gif of ALL the rides\nWe can also make a gif of multiple activities. I will filter my activities to only be bike rides over 15km.\n\n# get a bbox for Cornwall\nbbox <- ggmap::make_bbox(lat_lon_single$lon, lat_lon_single$lat, f = 1.2)\n\n# add column for frame and total distance per ride\nlat_lon <- group_by(lat_lon, activity_no) %>%\n  mutate(n = 1:n(),\n         tot_dist = max(distance)) %>%\n  ungroup()\n\n# filter lat_lon for when points are within this\nlat_lon <- filter(lat_lon, between(start_longitude, bbox[1], bbox[3]) & between(start_latitude, bbox[2], bbox[4]) & type == 'Ride' & tot_dist > 15)\n\n# add column for frame\nlat_lon <- group_by(lat_lon, activity_no) %>%\n  mutate(n = 1:n()) %>%\n  ungroup()\n\n# make bbox again\nbbox <- ggmap::make_bbox(lon, lat, data = lat_lon, f = 0.1)\n\n# download map\nmap <- get_map(location = bbox, source = 'google', maptype = 'terrain')\n\nall_the_rides <- ggmap(map, darken = 0.15) +\n  geom_path(aes(x = lon, y = lat, group = activity_no), col = 'red', data = lat_lon, size = 1.25, alpha = 0.5) +\n  labs(title = 'All the rides') +\n  coord_cartesian() +\n  ggforce::theme_no_axes(theme_bw(base_size = 16)) +\n  theme(legend.position = 'none') +\n  transition_reveal(n)\n\n# animate plot\ngganimate::anim_save('where_to_save.gif', all_the_rides, width = 750, height = 700)\n\n\nAnd there we have it. A relatively simple way to animate your strava activities in R. I personally find that saving the output as .mp4 rather than .gif gives smaller and higher quality files when uploading them to Instagram, but these options are easy to change. Take back your own data and get plotting!\nThere are loads of other functions and uses for the rStrava package. I hope to blog more about them soon."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html",
    "href": "posts/bootstrap_nls/index.html",
    "title": "Bootstrap nls models in R",
    "section": "",
    "text": "This post was updated to reflect the improvement of using car to bootstrap nonlinear regressions."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#introduction",
    "href": "posts/bootstrap_nls/index.html#introduction",
    "title": "Bootstrap nls models in R",
    "section": "Introduction",
    "text": "Introduction\nFor my first academic publication, a reviewer asked for the \\(r^{2}\\) values of the thermal performance curves I fitted using non-linear regression. I bowed to the request as is often the case with reviewer comments, but would now resist as the \\(r^{2}\\) is not necessarily an effective goodness of fit measure for non-linear regression (see this SO answer). It does raise the question of how to determine how well a biologically meaningful model fits the data it is fitted to. I generally just plot every curve to its data, but it tells me nothing of the uncertainty around the curve.\nStep forward the bootstrap! (Non-parametric) bootstrapping is a robust way of computing parameter and model prediction confidence intervals. Bootstrapping involves simulating “new” datasets produced from either the original data (case resampling) or from the original model (residual resampling).\nThe same model is then fitted separately on each individual bootstrapped dataset. Doing this over and over allows us to visualise uncertainty of predictions and produce confidence intervals of estimated parameters. When previously implementing this, I used methods similar to previous blog posts by Andrew MacDonald and Hadley Wickham, as well as a broom vignette.\nHowever, I have since applied a more efficient method using the package car, which contains the function Boot() that provides a wrapper for the widely used function boot::boot() that is tailored to bootstrapping regression models."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#case-resampling-resampling-the-original-data-with-replacement",
    "href": "posts/bootstrap_nls/index.html#case-resampling-resampling-the-original-data-with-replacement",
    "title": "Bootstrap nls models in R",
    "section": "Case resampling: Resampling the original data with replacement",
    "text": "Case resampling: Resampling the original data with replacement\nBootstrapping using case resampling involves simulating “new” datasets produced from the existing data by sampling with replacement.\n\nCase resampling: When it works\nWe will demonstrate an example of when this case resampling approach works using data from a recent paper by Padfield et al. (2020), that measures the thermal performance of the bacteria, Pseudomonas fluorescens, in the presence and absence of its phage, \\(\\phi 2\\). In this study, each single growth rate estimate is a technical replicate. As such, all the data points within each phage treatment can be used to estimate the same curve. The data is in the R package rTPC and we can visualise one of the curves using ggplot2.\n\n# load packages\nlibrary(boot)\nlibrary(car)\nlibrary(rTPC) #remotes::install_github('padpadpadpad/rTPC')\nlibrary(nls.multstart)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(minpack.lm)\n\n# load in data\ndata(\"bacteria_tpc\")\n\n# keep just a single curve\nd <- filter(bacteria_tpc, phage == 'nophage')\n\n# show the data\nggplot(d, aes(temp, rate)) +\n  geom_point(size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n\n\n\nAs in the study, we can fit the Sharpe-Schoolfield model to the data. I take advantage of the packages nls.mulstart and rTPC to do this.\n\n# fit Sharpe-Schoolfield model\nd_fit <- nest(d, data = c(temp, rate)) %>%\n  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = .x,\n                        iter = c(3,3,3,3),\n                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,\n                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,\n                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        supp_errors = 'Y',\n                        convergence_count = FALSE)),\n         # create new temperature data\n         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),\n         # predict over that data,\n         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))\n\n# unnest predictions\nd_preds <- select(d_fit, preds) %>%\n  unnest(preds)\n\n# plot data and predictions\nggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n\n\n\nnls_multstart() is designed to fit models across a wide possible parameter space, but as it samples multiple start parameters for each model, using it with bootstrapping becomes computationally expensive. Instead, we refit the model using minpack.lm::nlsLM(), using the coefficients of nls_multstart() as the start values. The Boot() function then refits the model 999 times and stores the model coefficients.\n\n# refit model using nlsLM\nfit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = d,\n                        start = coef(d_fit$sharpeschoolhigh[[1]]),\n                        lower = get_lower_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                        weights = rep(1, times = nrow(d)))\n\n# bootstrap using case resampling\nboot1 <- Boot(fit_nlsLM, method = 'case')\n\n# look at the data\nhead(boot1$t)\n\n        r_tref         e       eh       th\n[1,] 0.2387067 0.9452492 2.367318 29.24874\n[2,] 0.2511824 0.7655751 2.547971 31.27085\n[3,] 0.3173423 0.7499641 2.081992 29.59478\n[4,] 0.2586626 0.7592931 2.564117 31.18532\n[5,] 0.2871170 0.6627851 2.483048 31.94504\n[6,] 0.2434468 0.8695365 2.432169 30.15619\n\n\nThe parameters of each bootstrapped refit are returned. All methods that are available for boot() and Boot() are supported for these objects. This includes the hist.boot() function which looks at the distribution of each parameter.\n\nhist(boot1, layout = c(2,2))\n\n\n\n\nWe can easily create predictions for each of these models and through this confidence intervals around the original fitted predictions. We can then plot (1) the bootstrapped fits and (2) the confidence regions around the model predictions.\n\n# create predictions of each bootstrapped model\nboot1_preds <- boot1$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d$temp), max(d$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot1_conf_preds <- group_by(boot1_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot1_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot1_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2\n\n\n\n\nThis method works well here, because there are many points beyond the peak of the curve and multiple independent points at each temperature.\n\n\nCase resampling: When it struggles\nThis method becomes more problematic when there is a small sample size and the coverage of temperature values beyond the optimum temperature is small. This means that many of the bootstrapped datasets will not have any points beyond the optimum, which is problematic for mathematical models that expect a unimodal shape. The effect of this can be seen by case resampling a curve from the chlorella_tpc dataset also in rTPC. Here we again fit the model using nls_multstart(), refit the model using nlsLM(), then bootstrap the model using Boot().\n\n# load in chlorella data\ndata('chlorella_tpc') \n\nd2 <- filter(chlorella_tpc, curve_id == 1)\n\n# fit Sharpe-Schoolfield model to raw data\nd_fit <- nest(d2, data = c(temp, rate)) %>%\n  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = .x,\n                        iter = c(3,3,3,3),\n                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,\n                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,\n                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        supp_errors = 'Y',\n                        convergence_count = FALSE)),\n         # create new temperature data\n         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),\n         # predict over that data,\n         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))\n\n# refit model using nlsLM\nfit_nlsLM2 <- nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = d2,\n                        start = coef(d_fit$sharpeschoolhigh[[1]]),\n                        lower = get_lower_lims(d2$temp, d2$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(d2$temp, d2$rate, model_name = 'sharpeschoolhigh_1981'),\n                        control = nls.lm.control(maxiter=500),\n                        weights = rep(1, times = nrow(d2)))\n\n# bootstrap using case resampling\nboot2 <- Boot(fit_nlsLM2, method = 'case')\n\n\n Number of bootstraps was 995 out of 999 attempted \n\n\nWe can then create predictions for each bootstrapped model and calculate 95% confidence intervals around the predictions. Models that don’t fit and return NA for the parameter estimates are dropped.\n\n# unnest predictions of original model fit\nd_preds <- select(d_fit, preds) %>%\n  unnest(preds)\n\n# predict over new data\nboot2_preds <- boot2$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d2$temp), max(d2$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot2_conf_preds <- group_by(boot2_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot2_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot2_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2\n\n\n\n\nAs can be seen, bootstrapping-with-replacement with only a single point at each temperature can lead to a large variety of fits. In the second panel, we can see the variation of the curve fits, clustering around 4 possible paths for the decrease in rate beyond the optimum temperature. This occurs because in many instances there are no points sampled at the very high temperatures, leading to this clustering in curve fits."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#residual-resampling",
    "href": "posts/bootstrap_nls/index.html#residual-resampling",
    "title": "Bootstrap nls models in R",
    "section": "Residual resampling",
    "text": "Residual resampling\nCase resampling is the most common way of thinking about bootstrapping. However, bootstrapping ordinary least squares regression models is often done using bootstrapping residuals. This method - where the values of the predictors in a study remain fixed during resampling - is especially useful in a designed experiment where the values of the predictors are set by the experimenter.\nRe-sampling residuals, at its heart, follows a simple set of steps:\n\nFit the model and for each data point, \\(i\\), retain the fitted values \\(\\hat{y_{i}}\\) and the residuals, \\(\\hat{e_{i}} = y_{i} - \\hat{y_{i}}\\)\nFor each data pair, (\\(x_i\\), \\(y_i\\)), where \\(x_i\\) is the measured temperature value, we add a randomly re-sampled residual, \\(\\hat{e}\\) to the fitted value \\(\\hat{y_i}\\). This becomes the new \\(y_i\\) value, such that \\(y_i = \\hat{y_i} + \\hat{e}\\). The new response variable is created based on the random re-allocation of the variation around the original model fit\nThe model is refit using the newly created \\(y_i\\) response variable\nRepeat steps 2 and 3 a number of times\n\nThis method makes the assumption that the original model fit is a good representation of the data, and that the error terms in the model are normally distributed and independent. If the model is incorrectly specified – for example, if there is unmodelled non-linearity, non-constant error variance, or outliers – these characteristics will not carry over into the re-sampled data sets.\ncar::Boot() has an argument that allows us to easily implement residual resampling instead of case resampling, by setting method = 'residual'.\n\n# bootstrap using residual resampling\nboot3 <- Boot(fit_nlsLM2, method = 'residual')\n\n# predict over new data\nboot3_preds <- boot3$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d2$temp), max(d2$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot3_conf_preds <- group_by(boot3_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot3_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot3_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2"
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#calculating-confidence-intervals-of-estimated-parameters",
    "href": "posts/bootstrap_nls/index.html#calculating-confidence-intervals-of-estimated-parameters",
    "title": "Bootstrap nls models in R",
    "section": "Calculating confidence intervals of estimated parameters",
    "text": "Calculating confidence intervals of estimated parameters\nBootstrapping can be used to estimate confidence intervals of the parameters explicitly modelled in the regression. We can compare these approaches to profiled confidence intervals (using confint-MASS) and asymptotic confidence intervals (using nlstools::confint2()). For the bootstrapped parameter distributions, confint.boot() supports the calculation of BCa, basic, normal, and percentile confidence intervals. We use BCa here, and we will calculate all CIs on the two models done previously in this vignette. First with the bacteria TPC.\n\n# First for the bacteria\n\n# get parameters of fitted model\nparam_bact <- broom::tidy(fit_nlsLM) %>%\n  select(param = term, estimate)\n\n# calculate confidence intervals of models\nci_bact1 <- nlstools::confint2(fit_nlsLM, method = 'asymptotic') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'asymptotic')\nci_bact2 <- confint(fit_nlsLM) %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'profile')\n\n# CIs from case resampling\nci_bact3 <- confint(boot1, method = 'bca') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'case bootstrap')\n\n# CIs from residual resampling\nci_bact4 <- Boot(fit_nlsLM, method = 'residual') %>%\n  confint(., method = 'bca') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'residual bootstrap')\n\nci_bact <- bind_rows(ci_bact1, ci_bact2, ci_bact3, ci_bact4) %>%\n  left_join(., param_bact)\n\n# plot\nggplot(ci_bact, aes(forcats::fct_relevel(method, c('profile', 'asymptotic')), estimate, col = method)) +\n  geom_hline(aes(yintercept = conf_lower), linetype = 2, filter(ci_bact, method == 'profile')) +\n  geom_hline(aes(yintercept = conf_upper), linetype = 2, filter(ci_bact, method == 'profile')) +\n  geom_point(size = 4) +\n  geom_linerange(aes(ymin = conf_lower, ymax = conf_upper)) +\n  theme_bw() +\n  facet_wrap(~param, scales = 'free') +\n  scale_x_discrete('', labels = function(x) stringr::str_wrap(x, width = 10)) +\n  labs(title = 'Calculation of confidence intervals for model parameters',\n       subtitle = 'For the bacteria TPC; dashed lines are CI of profiling method')\n\n\n\n\nEach panel is a different explicitly modelled parameter. The dashed lines represent the 95% intervals for the profiling method. In general, the different bootstrap methods are similar to the profiled intervals, but not all parameters are the same. For example, r_tref and e give wider (and asymmetric) confidence intervals using the case resampling method. The residual method gives estimates that are more similar to those calculated from profiling."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#calculate-confidence-intervals-of-generated-parameters",
    "href": "posts/bootstrap_nls/index.html#calculate-confidence-intervals-of-generated-parameters",
    "title": "Bootstrap nls models in R",
    "section": "Calculate confidence intervals of generated parameters",
    "text": "Calculate confidence intervals of generated parameters\nCrucially, bootstrapping allows the calculation of confidence intervals for parameters derived from the model that were not present in the initial fitting process. For example, the optimum temperature of a thermal performance curve, \\(T_{opt}\\) is calculated as:\n\\[T_{opt} = \\frac{E_{h}T_{h}}{E_{h} + k T_{h} ln(\\frac{E_{h}}{E} - 1)}\\] We can calculate \\(T_{opt}\\) by writing a custom function that we feed into Boot(). We will do this using the case resampling approach for the first curve in this blog post.\n\nget_topt <- function(model){\n  coefs = coef(model)\n  \n  e = coefs[names(coefs) == 'e']\n  eh = coefs[names(coefs) == 'eh']\n  th = coefs[names(coefs) == 'th']\n  \n  return(((eh*(th + 273.15))/(eh + (8.62e-05 *(th + 273.15)*log((eh/e) - 1)))) - 273.15)\n}\n\ntopt <- Boot(fit_nlsLM, f = function(x){get_topt(x)}, labels = 'topt', R = 999, method = 'case')\n\nhist(topt, legend = 'none')\n\n\n\n\nThis approach of using purrr, nls.multstart, and car can easily be scaled up to multiple curves."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#references",
    "href": "posts/bootstrap_nls/index.html#references",
    "title": "Bootstrap nls models in R",
    "section": "References",
    "text": "References\n\nPadfield, D., Castledine, M., & Buckling, A. (2020). Temperature-dependent changes to host–parasite interactions alter the thermal performance of a bacterial host. The ISME Journal, 14(2), 389-398."
  },
  {
    "objectID": "publications/articles/castledine_2022a.html",
    "href": "publications/articles/castledine_2022a.html",
    "title": "Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro",
    "section": "",
    "text": "Castledine, M., Padfield, D., Sierocinski, P., Pascual, J. S., Hughes, A., Mäkinen, L., Friman, V.P., Pirnay, J.P., Merabishvili, M., De Vos, D., & Buckling, A. (2022). Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro. Elife, 11, e73679."
  },
  {
    "objectID": "publications/articles/castledine_2022a.html#abstract",
    "href": "publications/articles/castledine_2022a.html#abstract",
    "title": "Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro",
    "section": "Abstract",
    "text": "Abstract\nWith rising antibiotic resistance, there has been increasing interest in treating pathogenic bacteria with bacteriophages (phage therapy). One limitation of phage therapy is the ease at which bacteria can evolve resistance. Negative effects of resistance may be mitigated when resistance results in reduced bacterial growth and virulence, or when phage coevolves to overcome resistance. Resistance evolution and its consequences are contingent on the bacteria-phage combination and their environmental context, making therapeutic outcomes hard to predict. One solution might be to conduct ‘in vitro evolutionary simulations’ using bacteria-phage combinations from the therapeutic context. Overall, our aim was to investigate parallels between in vitro experiments and in vivo dynamics in a human participant. Evolutionary dynamics were similar, with high levels of resistance evolving quickly with limited evidence of phage evolution. Resistant bacteria—evolved in vitro and in vivo—had lower virulence. In vivo, this was linked to lower growth rates of resistant isolates, whereas in vitro phage resistant isolates evolved greater biofilm production. Population sequencing suggests resistance resulted from selection on de novo mutations rather than sorting of existing variants. These results highlight the speed at which phage resistance can evolve in vivo, and how in vitro experiments may give useful insights for clinical evolutionary outcomes."
  },
  {
    "objectID": "publications/articles/bartlett_2022.html",
    "href": "publications/articles/bartlett_2022.html",
    "title": "A comprehensive list of bacterial pathogens infecting humans",
    "section": "",
    "text": "Bartlett, A., Padfield, D., Lear, L., Bendall, R., & Vos, M. (2022). A comprehensive list of bacterial pathogens infecting humans. Microbiology, 168(12), 001269."
  },
  {
    "objectID": "publications/articles/bartlett_2022.html#abstract",
    "href": "publications/articles/bartlett_2022.html#abstract",
    "title": "A comprehensive list of bacterial pathogens infecting humans",
    "section": "Abstract",
    "text": "Abstract\nThere exists an enormous diversity of bacteria capable of human infection, but no up-to-date, publicly accessible list is available. Combining a pragmatic definition of pathogenicity with an extensive search strategy, we report 1513 bacterial pathogens known to infect humans described pre-2021. Of these, 73 % were regarded as established (have infected at least three persons in three or more references) and 27 % as putative (fewer than three known cases). Pathogen species belong to 10 phyla and 24 classes scattered throughout the bacterial phylogeny. We show that new human pathogens are discovered at a rapid rate. Finally, we discuss how our results could be expanded to a database, which could provide a useful resource for microbiologists. Our list is freely available and archived on GitHub and Zenodo and we have provided walkthroughs to facilitate access and use."
  },
  {
    "objectID": "publications/articles/castledine_2022b.html",
    "href": "publications/articles/castledine_2022b.html",
    "title": "Greater phage genotypic diversity constrains arms-race coevolution",
    "section": "",
    "text": "Castledine, M., Sierocinski, P., Inglis, M., Kay, S., Hayward, A., Buckling, A., & Padfield, D. (2022). Greater phage genotypic diversity constrains arms-race coevolution. Frontiers in Cellular and Infection Microbiology, 168."
  },
  {
    "objectID": "publications/articles/castledine_2022b.html#abstract",
    "href": "publications/articles/castledine_2022b.html#abstract",
    "title": "Greater phage genotypic diversity constrains arms-race coevolution",
    "section": "Abstract",
    "text": "Abstract\nAntagonistic coevolution between hosts and parasites, the reciprocal evolution of host resistance and parasite infectivity, has important implications in ecology and evolution. The dynamics of coevolution – notably whether host or parasite has an evolutionary advantage – is greatly affected by the relative amount of genetic variation in host resistance and parasite infectivity traits. While studies have manipulated genetic diversity during coevolution, such as by increasing mutation rates, it is unclear how starting genetic diversity affects host-parasite coevolution. Here, we (co)evolved the bacterium Pseudomonas fluorescens SBW25 and two bacteriophage genotypes of its lytic phage SBW25ɸ2 in isolation (one phage genotype) and together (two phage genotypes). Bacterial populations rapidly evolved phage resistance and phage reciprocally increased their infectivity in response. When phage populations were evolved with bacteria in isolation, bacterial resistance and phage infectivity increased through time, indicative of arms-race coevolution. In contrast, when both phage genotypes were together, bacteria did not increase their resistance in response to increasing phage infectivity. This was likely due to bacteria being unable to evolve resistance to both phage via the same mutations. These results suggest that increasing initial parasite genotypic diversity can give parasites an evolutionary advantage that arrests long-term coevolution. This study has important implications for the applied use of phage in phage therapy and in understanding host-parasite dynamics in broader ecological and evolutionary theory."
  },
  {
    "objectID": "publications/articles/barneche_2021.html",
    "href": "publications/articles/barneche_2021.html",
    "title": "Warming impairs trophic transfer efficiency in a long-term field experiment",
    "section": "",
    "text": "Barneche, D. R., Hulatt, C. J., Dossena, M., Padfield, D., Woodward, G., Trimmer, M., & Yvon-Durocher, G. (2021). Warming impairs trophic transfer efficiency in a long-term field experiment. Nature, 592(7852), 76-79."
  },
  {
    "objectID": "publications/articles/barneche_2021.html#abstract",
    "href": "publications/articles/barneche_2021.html#abstract",
    "title": "Warming impairs trophic transfer efficiency in a long-term field experiment",
    "section": "Abstract",
    "text": "Abstract\nIn ecosystems, the efficiency of energy transfer from resources to consumers determines the biomass structure of food webs. As a general rule, about 10% of the energy produced in one trophic level makes it up to the next1,2,3. Recent theory suggests that this energy transfer could be further constrained if rising temperatures increase metabolic growth costs4, although experimental confirmation in whole ecosystems is lacking. Here we quantify nitrogen transfer efficiency—a proxy for overall energy transfer—in freshwater plankton in artificial ponds that have been exposed to seven years of experimental warming. We provide direct experimental evidence that, relative to ambient conditions, 4 °C of warming can decrease trophic transfer efficiency by up to 56%. In addition, the biomass of both phytoplankton and zooplankton was lower in the warmed ponds, which indicates major shifts in energy uptake, transformation and transfer5,6. These findings reconcile observed warming-driven changes in individual-level growth costs and in carbon-use efficiency across diverse taxa4,7,8,9,10 with increases in the ratio of total respiration to gross primary production at the ecosystem level11,12,13. Our results imply that an increasing proportion of the carbon fixed by photosynthesis will be lost to the atmosphere as the planet warms, impairing energy flux through food chains, which will have negative implications for larger consumers and for the functioning of entire ecosystems."
  },
  {
    "objectID": "publications/articles/lear_2022a.html",
    "href": "publications/articles/lear_2022a.html",
    "title": "Disturbance‐mediated invasions are dependent on community resource abundance",
    "section": "",
    "text": "Lear, L., Padfield, D., Inamine, H., Shea, K., & Buckling, A. (2022). Disturbance‐mediated invasions are dependent on community resource abundance. Ecology, 103(8), e3728."
  },
  {
    "objectID": "publications/articles/lear_2022a.html#abstract",
    "href": "publications/articles/lear_2022a.html#abstract",
    "title": "Disturbance‐mediated invasions are dependent on community resource abundance",
    "section": "Abstract",
    "text": "Abstract\nDisturbances can facilitate biological invasions, with the associated increase in resource availability being a proposed cause. Here, we experimentally tested the interactive effects of disturbance regime (different frequencies of biomass removal at equal intensities) and resource abundance on invasion success using a factorial design containing five disturbance frequencies and three resource levels. We invaded populations of the bacterium Pseudomonas fluorescens with two ecologically different invader morphotypes: a fast-growing “colonizer” type and a slower growing “competitor” type. As resident populations were altered by the treatments, we additionally tested their effect on invader success. Disturbance frequency and resource abundance interacted to affect the success of both invaders, but this interaction differed between the invader types. The success of the colonizer type was positively affected by disturbance under high resources but negatively under low. However, disturbance negatively affected the success of the competitor type under high resource abundance but not under low or medium. Resident population changes did not alter invader success beyond direct treatment effects. We therefore demonstrate that the same disturbance regime can either be beneficial or detrimental for an invader depending on both community resource abundance and its life history. These results may help to explain some of the inconsistencies found in the disturbance-invasion literature."
  },
  {
    "objectID": "publications/articles/padfield_2021.html",
    "href": "publications/articles/padfield_2021.html",
    "title": "rTPC and nls.multstart: A new pipeline to fit thermal performance curves in R",
    "section": "",
    "text": "Padfield, D., O’Sullivan, H., & Pawar, S. (2021). rTPC and nls. multstart: a new pipeline to fit thermal performance curves in R. Methods in Ecology and Evolution, 12(6), 1138-1143."
  },
  {
    "objectID": "publications/articles/padfield_2021.html#abstract",
    "href": "publications/articles/padfield_2021.html#abstract",
    "title": "rTPC and nls.multstart: A new pipeline to fit thermal performance curves in R",
    "section": "Abstract",
    "text": "Abstract\n\nQuantifying thermal performance curves (TPCs) for biological rates has many applications to important problems such as predicting responses of biological systems—from individuals to communities—to directional climate change or climatic fluctuations.\nCurrent software tools for fitting TPC models to data are not adequate for dealing with the immense size of new datasets that are increasingly becoming available. We require tools capable of tackling this issue in a simple, reproducible and accessible way.\nWe present a new, reproducible pipeline in r that allows for relatively simple fitting of 24 different TPC models using nonlinear least squares (NLLS) regression. The pipeline consists of two packages—rTPC and nls.multstart—that provide functions which conveniently address common problems with NLLS fitting such as the NLLS parameter starting values problem. rTPC also includes functions to set starting values, estimate key TPC parameters and calculate uncertainty around parameter estimates as well as the fitted model as a whole.\nWe demonstrate how this pipeline can be combined with other packages in r to robustly and reproducibly fit multiple mathematical models to multiple TPC datasets at once. In addition, we show how model selection or averaging, weighted model fitting and bootstrapping can be easily implemented within the pipeline.\nThis new pipeline provides a flexible and reproducible approach that makes the challenging task of fitting multiple TPC models to data accessible to a wide range of users across ecology and evolution."
  },
  {
    "objectID": "publications/articles/sierocinski_2021.html",
    "href": "publications/articles/sierocinski_2021.html",
    "title": "The impact of propagule pressure on whole community invasions in biomethane-producing communities",
    "section": "",
    "text": "Sierocinski, P., Pascual, J. S., Padfield, D., Salter, M., & Buckling, A. (2021). The impact of propagule pressure on whole community invasions in biomethane-producing communities. Iscience, 24(6), 102659."
  },
  {
    "objectID": "publications/articles/sierocinski_2021.html#abstract",
    "href": "publications/articles/sierocinski_2021.html#abstract",
    "title": "The impact of propagule pressure on whole community invasions in biomethane-producing communities",
    "section": "Abstract",
    "text": "Abstract\nMicrobes can invade as whole communities, but the ecology of whole community invasions is poorly understood. Here, we investigate how invader propagule pressure (the number of invading organisms) affects the composition and function of invaded laboratory methanogenic communities. An invading community was equally successful at establishing itself in a resident community regardless of propagule pressure, which varied between 0.01 and 10% of the size resident community. Invasion resulted in enhanced biogas production (to the level of the pure invading community) but only when propagule pressure was 1% or greater. This inconsistency between invasion success and changes in function can be explained by a lower richness of invading taxa at lower propagule pressures, and an important functional role of the taxa that were absent. Our results highlight that whole community invasion ecology cannot simply be extrapolated from our understanding of single species invasions. Moreover, we show that methane production can be enhanced by invading poorly performing reactors with a better performing community at levels that may be practical in industrial settings."
  },
  {
    "objectID": "publications/articles/sierocinski_2021.html#graphical-abstract",
    "href": "publications/articles/sierocinski_2021.html#graphical-abstract",
    "title": "The impact of propagule pressure on whole community invasions in biomethane-producing communities",
    "section": "Graphical Abstract",
    "text": "Graphical Abstract"
  },
  {
    "objectID": "publications/articles/swan_2022.html",
    "href": "publications/articles/swan_2022.html",
    "title": "Associations between abundances of free-roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season",
    "section": "",
    "text": "Swan, G. J., Bearhop, S., Redpath, S. M., Silk, M. J., Padfield, D., Goodwin, C. E., & McDonald, R. A. (2022). Associations between abundances of free‐roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season. Ecology and Evolution, 12(5), e8877."
  },
  {
    "objectID": "publications/articles/swan_2022.html#abstract",
    "href": "publications/articles/swan_2022.html#abstract",
    "title": "Associations between abundances of free-roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season",
    "section": "Abstract",
    "text": "Abstract\nReleasing gamebirds in large numbers for sport shooting may directly or indirectly influence the abundance, distribution and population dynamics of native wildlife. The abundances of generalist predators have been positively associated with the abundance of gamebirds. These relationships have implications for prey populations, with the potential for indirect impacts of gamebird releases on wider biodiversity. To understand the basis of these associations, we investigated variation in territory size, prey provisioning to chicks, and breeding success of common buzzards Buteo buteo, and associations with variation in the abundances of free-roaming gamebirds, primarily pheasants Phasianus colchicus, and of rabbits Oryctolagus cuniculus and field voles Microtus agrestis, as important prey for buzzards. The relative abundance of gamebirds, but not those of rabbits or voles, was weakly but positively correlated with our index of buzzard territory size. Gamebirds were rarely brought to the nest. Rabbits and voles, and not gamebirds, were provisioned to chicks in proportion to their relative abundance. The number of buzzard chicks increased with provisioning rates of rabbits, in terms of both provisioning frequency and biomass, but not with provisioning rates for gamebirds or voles. Associations between the abundances of buzzards and gamebirds may not be a consequence of the greater availability of gamebirds as prey during the buzzard breeding season. Instead, the association may arise either from habitat or predator management leading to higher densities of alternative prey (in this instance, rabbits), or from greater availability of gamebirds as prey or carrion during the autumn and winter shooting season. The interactions between gamebird releases and associated practices of predator control and shooting itself require better understanding to more effectively intervene in any one aspect of this complex social-ecological system."
  },
  {
    "objectID": "publications/articles/lear_2022.html",
    "href": "publications/articles/lear_2022.html",
    "title": "Bacterial colonisation dynamics of household plastics in a coastal environment",
    "section": "",
    "text": "Lear, L., Padfield, D., Dowsett, T., Jones, M., Kay, S., Hayward, A., & Vos, M. (2022). Bacterial colonisation dynamics of household plastics in a coastal environment. Science of the Total Environment, 838, 156199."
  },
  {
    "objectID": "publications/articles/lear_2022.html#abstract",
    "href": "publications/articles/lear_2022.html#abstract",
    "title": "Bacterial colonisation dynamics of household plastics in a coastal environment",
    "section": "Abstract",
    "text": "Abstract\nAccumulation of plastics in the marine environment has widespread detrimental consequences for ecosystems and wildlife. Marine plastics are rapidly colonised by a wide diversity of bacteria, including human pathogens, posing potential risks to health. Here, we investigate the effect of polymer type, residence time and estuarine location on bacterial colonisation of common household plastics, including pathogenic bacteria. We submerged five main household plastic types: low-density PE (LDPE), high-density PE (HDPE), polypropylene (PP), polyvinyl chloride (PVC) and polyethylene terephthalate (PET) at an estuarine site in Cornwall (U.K.) and tracked bacterial colonisation dynamics. Using both culture-dependent and culture-independent approaches, we found that bacteria rapidly colonised plastics irrespective of polymer type, reaching culturable densities of up to 1000 cells cm3 after 7 weeks. Community composition of the biofilms changed over time, but not among polymer types. The presence of pathogenic bacteria, quantified using the insect model Galleria mellonella, increased dramatically over a five-week period, with Galleria mortality increasing from 4% in week one to 65% in week five. No consistent differences in virulence were observed between polymer types. Pathogens isolated from plastic biofilms using Galleria enrichment included Serratia and Enterococcus species and they harboured a wide range of antimicrobial resistance genes. Our findings show that plastics in coastal waters are rapidly colonised by a wide diversity of bacteria independent of polymer type. Further, our results show that marine plastic biofilms become increasingly associated with virulent bacteria over time."
  },
  {
    "objectID": "publications/articles/lear_2022.html#graphical-abstract",
    "href": "publications/articles/lear_2022.html#graphical-abstract",
    "title": "Bacterial colonisation dynamics of household plastics in a coastal environment",
    "section": "Graphical Abstract",
    "text": "Graphical Abstract"
  },
  {
    "objectID": "article_template.html#abstract",
    "href": "article_template.html#abstract",
    "title": "DanPadLab",
    "section": "Abstract",
    "text": "Abstract"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DanPadLab’s website",
    "section": "",
    "text": "I am Daniel Padfield and I know this is a terrible name for a research group but it will do for now. My group is based at the University of Exeter (at the Cornwall campus by the sea) and we are broadly interested in understanding how microbial communities respond to environmental change. To do this, we use a variety of sequencing, experimental, and modelling approaches.\nWe are also committed to reproducible and open science. All of the code and data from our projects are freely available online through GitHub and archived on Zenodo.\nThe aim of this website is to allow us to document our work and to provide a space to share useful code and walk-throughs. I also hope to use this as an outlet to document my academic journey as I attempt to manage being an academic alongside being a long-time carer."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "DanPadLab",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n  \n    A comprehensive list of bacterial pathogens infecting humans\n    Abigail Bartlett, Daniel Padfield, Luke Lear, Richard Bendall and Michiel Vos\n    Microbiology\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Associations between abundances of free-roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season\n    George JF Swan, Stuart Bearhop, Stephen M Redpath, Matthew J Silk, Daniel Padfield, Cecily ED Goodwin, Robbie A McDonald\n    Ecology & Evolution\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n  \n  \n    Bacterial colonisation dynamics of household plastics in a coastal environment\n    Luke Lear, Daniel Padfield, Tirion Dowsett, Maia Jones, Suzanne Kay, Alex Hayward, Michiel Vos\n    Science of the Total Environment\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Disturbance‐mediated invasions are dependent on community resource abundance\n    Luke Lear, Daniel Padfield, Hidetoshi Inamine, Katriona Shea, Angus Buckling\n    Ecology\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Greater phage genotypic diversity constrains arms-race coevolution\n    Meaghan Castledine, Pawel Sierocinski, Mhairi Inglis, Suzanne Kay, Alex Hayward, Angus Buckling, Daniel Padfield\n    Frontiers in Cellular and Infection Microbiology\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro\n    Meaghan Castledine, Daniel Padfield, Pawel Sierocinski, Jesica Soria Pascual, Adam Hughes, Lotta Mäkinen, Ville-Petri Friman, Jean-Paul Pirnay, Maya Merabishvili, Daniel De Vos, Angus Buckling\n    eLife\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    The impact of propagule pressure on whole community invasions in biomethane-producing communities\n    Pawel Sierocinski, Jesica Soria Pascual, Daniel Padfield, Mike Salter, Angus Buckling\n    iScience\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Data\n    \n    \n  \n  \n    Warming impairs trophic transfer efficiency in a long-term field experiment\n    Diego R Barneche, Chris J Hulatt, Matteo Dossena, Daniel Padfield, Guy Woodward, Mark Trimmer, Gabriel Yvon-Durocher\n    Nature\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Open Access\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    rTPC and nls.multstart: A new pipeline to fit thermal performance curves in R\n    Daniel Padfield, Hannah O'Sullivan, Samraat Pawar\n    Methods in Ecology & Evolution\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Daniel Padfield",
    "section": "",
    "text": "I am a microbial ecologist working at the University of Exeter in Penryn, Cornwall. I have broad interests that span climate change, evolution, and community ecology. My research has concentrated on how environmental change alters the interplay of ecological and evolutionary dynamics in microbial communities. I do this by combining theory and simulations with experimental evolution. In January 2023 I started a NERC Independent Research Fellowship at the University of Exeter to explore the links between climate warming and antibiotic resistance in microbial communities.\nI am an avid R programmer - authoring the R packages nls.multstart and rTPC - and specialise in the wrangling and manipulation of large datasets and statistical analysis. I also have experience in bioinformatics from 16S sequencing to de novo genome assembly, the latter using bash scripting.\nI am also a carer for my partner. She has full-body neuropathic pain, severe ME/CFS and has been bed-bound for around 4 years. She lives with her parents in Cheltenham and I visit for long weekends every three to four weeks.\nIf you are interested in collaborating or have any questions or comments I would love to hear from you."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Rolling regression to estimate microbial growth rate\n\n\n\n\n\nCalculating microbial growth rates from OD measurements using rolling regression in the tidyverse\n\n\n\n\n\n\n2019-11-15\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nrStrava and gganimate\n\n\n\n\n\nAnimate your Strava activities in R using rStrava and gganimate\n\n\n\n\n\n\n2018-05-18\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap nls models in R\n\n\n\n\n\nBootstrap non-linear least squares regression in R with purrr and car\n\n\n\n\n\n\n2018-01-21\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nDoing robust nls regression in R\n\n\n\n\n\nFitting non-linear regressions with broom, purrr and nls.multstart\n\n\n\n\n\n\n2018-01-07\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  }
]