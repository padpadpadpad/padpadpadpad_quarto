[
  {
    "objectID": "posts/rolling_regression/index.html",
    "href": "posts/rolling_regression/index.html",
    "title": "Rolling regression to estimate microbial growth rate",
    "section": "",
    "text": "Introduction\nMicrobial ecologists often measure the growth rates of their favourite microbes, commonly using an OD (optical density) reader, with growth being related to the increasing OD of the sample through time. There are loads of ways to analyse these curves. Most of these, like growthcurver and the methods I used in my recent ISME paper (nls.multstart and functions from nlsMicrobio), fit models to logistic growth. Both methods can easily be scaled up to fit multiple curves at the same time.\nHowever, sometimes these methods do not do a good job. The most common example when this happens is if the wee critters do not reach stationary phase. Without stationary phase, most of the models will struggle to calculate carrying capacity, which also means the estimated exponential growth rate is poor. Similar things can occur if the bacteria form biofilms. This can result in increasingly noisy measurements at higher OD readings, again making the estimation of carrying capacity and growth rates more difficult.\nAs an alternative, we can bin off trying to model the entire growth curve, and instead implement a rolling regression, where we fit a linear regression on a shifting window of points. On natural-log transformed OD, the slope of the regression between \\(logOD\\) and time is equivalent to the exponential growth rate.\n\n\nGet started\nWe will load (and install) all the packages needed to run the example here. We will use the example data from growthcurver as example OD readings from a 96-well plate. Time is in hours, and I have created stacked all the wells into a single column for OD. Finally I created a column for \\(logOD\\), which is needed for the rolling regression, and \\(log_{10}OD\\) which is needed for fitting the modified gompertz growth model from nlsMicrobio.\n\n# load packages\nlibrary(tidyverse) #install.packages(tidyverse)\nlibrary(zoo) #install.packages(zoo)\nlibrary(broom) #install.packages(broom)\nlibrary(growthcurver) # install.packages(growthcurver)\nlibrary(nls.multstart) # install.packages(nls.multstart)\n# remotes::install_github('padpadpadpad/MicrobioUoE)\n\n# load example data\nd <- growthcurver::growthdata %>%\n  gather(., well, od, -time) %>%\n  mutate(ln_od = log(od),\n         log10_od = log10(od))\n# have a look at the data\nglimpse(d)\n\nRows: 13,920\nColumns: 5\n$ time     <dbl> 0.0000000, 0.1666667, 0.3333333, 0.5000000, 0.6666667, 0.8333…\n$ well     <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"…\n$ od       <dbl> 0.05348585, 0.04800336, 0.05587451, 0.05131749, 0.04516719, 0…\n$ ln_od    <dbl> -2.928338, -3.036484, -2.884647, -2.969724, -3.097384, -2.938…\n$ log10_od <dbl> -1.271761, -1.318728, -1.252786, -1.289735, -1.345177, -1.276…\n\n\n\n\nFit modified gompertz model for bacterial growth\nWe will first demonstrate rolling regression against the modified gompertz model for growth. I like the inclusion of the lag parameter in this model, especially for OD readers where the initial inoculate can often be so low that the OD reader cannot measure it. It means that exponential growth is only calculated at OD readings that we are confident represent changes in biomass of the bacteria.\nTo do this, we’ll filter the data for just the first well, A1. Then we will fit the modified gompertz model and plot the results.\n\n# filter for just a single well\nd_a1 <- filter(d, well == 'A1')\n# define gompertz growth model\ngompertz <- function(log10_nmax, log10_n0, mumax, t, lag){\n  log10_n0 + (log10_nmax - log10_n0) * exp(-exp(mumax * exp(1) * (lag - t)/((log10_nmax - log10_n0) * log(10)) + 1))\n}\n# fit gompertz model\nfit_gomp <- nls.multstart::nls_multstart(log10_od ~ gompertz(log10_nmax, log10_n0, mumax, t = time, lag),\n           data = d_a1,\n           start_lower = c(log10_nmax = -0.75, log10_n0 = -3, mumax = 0, lag = 0),\n           start_upper = c(log10_nmax = 0.5, log10_n0 = -1, mumax = 10, lag = 25),\n           lower = c(log10_nmax = -0.6, log10_n0 = -2, mumax = 0, lag = 0),\n           iter = 500,\n           supp_errors = 'Y')\n# get predictions\ngomp_preds <- augment(fit_gomp)\n# plot on original scale\nggplot(d_a1, aes(time, od)) +\n  geom_line(aes(time, 10^.fitted), gomp_preds, col = 'red') +\n  geom_point() +\n  theme_bw(base_size = 16) +\n  labs(x = 'time (hours)',\n       y = 'OD') +\n  annotate(geom = 'text', x = 0, y = 0.37, label = paste('µ = ', round(coef(fit_gomp)[3], 2), ' hr-1', sep = ''), hjust = 0, size = MicrobioUoE::pts(16))\n\n\n\n\nHowever, lets say our measurements end at 10 or 11 hours. When we have not reached stationary phase, the traditional bacterial growth models are likely to have trouble fitting.\n\nd_a1 <- filter(d_a1, time < 10.5)\n# plot data without stationary phase\nggplot(d_a1, aes(time, od)) +\n  geom_point() +\n  theme_bw(base_size = 16) +\n  labs(x = 'time (hours)',\n       y = 'OD')\n\n\n\n\n\n\nUsing rolling regression\nRolling regression allows us to calculate exponential growth rate even when we do not have the whole curve. First, we need to create our own rolling regression function. This method is mainly taken from G. Grothendieck’s StackOverflow answer. In the function, we specify our output from a standard lm object. So if you know how to access the output of lm(), you can add any extra details you want.\nOne of the big decisions in rolling regression is deciding how many points you are going to calculate the growth rate over. In this example, measurements are taken every 0.167 hours, about every ten minutes. I want a shifting window to span a minimum of 1.5 hours, so I calculate num_points to define the number of points the rolling regression will act on.\nWe then run the rolling regression, using zoo::rollapplyr() and dplyr::do(). Finally, in order to illustrate what the rolling regression is doing, I created a predictions dataframe for every single linear model that is fitted.\n\n# create the rolling regression function\nroll_regress <- function(x){\n  temp <- data.frame(x)\n  mod <- lm(temp)\n  temp <- data.frame(slope = coef(mod)[[2]],\n                     slope_lwr = confint(mod)[2, ][[1]],\n                     slope_upr = confint(mod)[2, ][[2]],\n                     intercept = coef(mod)[[1]],\n                     rsq = summary(mod)$r.squared, stringsAsFactors = FALSE)\n  return(temp)\n}\n# define window - here every ~1.5 hours\nnum_points = ceiling(1.5*60/(60*0.167)) \n\n# run rolling regression on ln od ~ time\nmodels <- d_a1 %>%\n  do(cbind(model = dplyr::select(., ln_od, time) %>% \n           zoo::rollapplyr(width = num_points, roll_regress, by.column = FALSE, fill = NA, align = 'center'),\n           time = dplyr::select(., time),\n           ln_od = dplyr::select(., ln_od))) %>%\n  rename_all(., gsub, pattern = 'model.', replacement = '')\n# create predictions\npreds <- models %>%\n  filter(., !is.na(slope)) %>%\n  group_by(time) %>%\n  do(data.frame(time2 = c(.$time - 2, .$time + 2))) %>%\n  left_join(., models) %>%\n  mutate(pred = (slope*time2) + intercept)\n\nWe can plot the rolling regression through time. I have extracted the exponential growth rate as the maximum slope of any of the regressions. Reassuringly the value of \\(\\mu\\) we get is very similar to that of using the gompertz model. I have also plotted the time at which the maximum slope occurred. It looks pretty close to mid-log to me.\n\n# calculate the exponential growth rate\ngrowth_rate <- filter(models, slope == max(slope, na.rm = TRUE))\n# plot rolling regression\nggplot(d_a1, aes(time, ln_od)) +\n  geom_point() +\n  geom_line(aes(time2, pred, group = time), col = 'red', preds, alpha = 0.5) +\n  theme_bw(base_size = 16) +\n  geom_segment(aes(x = time, y = -3, xend = time, yend = ln_od), growth_rate) +\n  geom_segment(aes(x = 0, y = ln_od, xend = time, yend = ln_od), growth_rate) +\n  annotate(geom = 'text', x = 0, y = -1, label = paste('µ = ', round(growth_rate$slope, 2), ' hr-1\\n95%CI:(',round(growth_rate$slope_lwr, 2), '-', round(growth_rate$slope_upr, 2), ')', sep = ''), hjust = 0, size = MicrobioUoE::pts(16)) +\n  labs(x = 'time (hours)',\n       y = 'OD')\n\n\n\n\n\n\nThe opportunities are endless\nThe great thing about this approach is its flexibility. It can easily be rolled out over all the wells in that plate, using group_by().\n\n# run rolling regression on ln od_cor ~ time\nmodels <- d %>%\n  group_by(well) %>%\n  do(cbind(model = select(., ln_od, time) %>% \n           zoo::rollapplyr(width = num_points, roll_regress, by.column = FALSE, fill = NA, align = 'center'),\n           time = select(., time),\n           ln_od = select(., ln_od))) %>%\n  rename_all(., gsub, pattern = 'model.', replacement = '')\n# calculate growth rate for each one\ngrowth_rates <- models %>%\n  filter(slope == max(slope, na.rm = TRUE)) %>%\n  ungroup()\n\n|===========================================|100% ~0 s remaining \n\nglimpse(models)\n\nRows: 13,920\nColumns: 8\nGroups: well [96]\n$ well      <chr> \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", …\n$ slope     <dbl> NA, NA, NA, NA, -0.077427705, -0.028208492, -0.021245636, 0.…\n$ slope_lwr <dbl> NA, NA, NA, NA, -0.199773163, -0.160658493, -0.157480637, -0…\n$ slope_upr <dbl> NA, NA, NA, NA, 0.04491775, 0.10424151, 0.11498936, 0.131953…\n$ intercept <dbl> NA, NA, NA, NA, -2.944791, -2.976248, -2.967393, -3.024750, …\n$ rsq       <dbl> NA, NA, NA, NA, 0.2423791383, 0.0349643445, 0.0190560342, 0.…\n$ time      <dbl> 0.0000000, 0.1666667, 0.3333333, 0.5000000, 0.6666667, 0.833…\n$ ln_od     <dbl> -2.928338, -3.036484, -2.884647, -2.969724, -3.097384, -2.93…\n\n\nThese growth rates can then be used for downstream analyses, and the method can easily be used over multiple plates and for many different types of data. Finally, you could also filter the regressions by \\(R^{2}\\) values, making sure you only kept good fitting regressions. Or do a sensitivity analysis of different sized window sizes to make sure your chosen window is suitable.\nHow do you get your data off of the OD reader? I have written scripts to collate hundreds of plate readings into a single dataframe in R. Let me know if you would like me to do a blog post on that process! Thanks for reading."
  },
  {
    "objectID": "posts/climate_spiral/index.html",
    "href": "posts/climate_spiral/index.html",
    "title": "Recreate Ed Hawkins’ climate spiral in R",
    "section": "",
    "text": "Climate change is happening, and changes in average temperatures are happening quickly. The 5 hottest years on record have occurred in the last six years for which we have data. One of the best ways to demonstrate climate warming is through data visualisations, and one of the best examples of this are Ed Hawkins’ climate stripes.\n\n\n\nEd Hawkins’ Climate Stripes\n\n\nThis visualisation is made up of vertical stripes, each representing the average temperature for a single year, relative to the average temperature over the whole period (from 1880 to the most recent year for which we have data). Blue colours indicate cooler-than-average years, while red colours indicate years that were hotter than average. As you can see, the increasing redness on the right-hand side of the graphic shows the rapid heating over recent years.\nI love the simplicity of this graphic, and it has proved a very effective tool for improving people’s awareness of climate warming (see the #climatestripes for amazing examples of its use across the world).\nI also loved the climate spiral animation created by Ed and re-created by NASA to help people understand the concept of global warming and the change in global temperatures over time. The spiral presents a visual representation of temperature changes over the past century or more, by showing a line that spirals outwards from the center of the graph as time progresses.\n\nIn the centre of the spiral is the average temperature of a baseline period, typically the 20th century average. As the line moves outwards, the temperature increases, with each full rotation of the spiral representing one year of data. The thickness of the line represents the magnitude of the temperature change in each year, with thicker lines indicating larger temperature increases.\nHaving played with ggplot2 and gganimate a bit, I was interested if I could recreate this animation in R (unfortunately without the amazing axis flip at the end)!"
  },
  {
    "objectID": "posts/climate_spiral/index.html#make-the-climate-spiral-in-r",
    "href": "posts/climate_spiral/index.html#make-the-climate-spiral-in-r",
    "title": "Recreate Ed Hawkins’ climate spiral in R",
    "section": "Make the climate spiral in R",
    "text": "Make the climate spiral in R\nAt the time of writing this blog post, using gganimate for geom_path() and transition_reveal() is unfortunately bugged in version 1.0.8, meaning that some frames would be missing. As such we will install a previous version (1.0.7).\nThis post will walkthrough how to recreate Ed Hawkins’ famous climate spiral in R. We will mainly use the tidyverse packages and gganimate to do this. Firstly we will load in these required packages.\n\n# load packages\nlibrary(tidyverse)\nlibrary(gganimate) # remotes::install_github('thomasp85/gganimate@v1.0.7')\nlibrary(janitor)\nlibrary(MetBrewer)\nlibrary(ggforce)\n\nWe can next load in the dataset used from the NASA website.\n\n# load in dataset from NASA\nd <- read.csv('https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv', skip = 1)\nhead(d)\n\n  Year   Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec  J.D  D.N\n1 1880 -0.18 -.24 -.09 -.16 -.10 -.21 -.18 -.10 -.15 -.23 -.22 -.17 -.17  ***\n2 1881 -0.19 -.14  .03  .05  .06 -.18  .00 -.03 -.15 -.22 -.18 -.07 -.09 -.09\n3 1882  0.16  .14  .04 -.16 -.14 -.22 -.17 -.08 -.15 -.23 -.17 -.36 -.11 -.09\n4 1883 -0.29 -.37 -.12 -.18 -.18 -.07 -.07 -.14 -.22 -.11 -.24 -.11 -.18 -.20\n5 1884 -0.13 -.08 -.36 -.40 -.34 -.35 -.30 -.28 -.27 -.25 -.33 -.31 -.28 -.27\n6 1885 -0.58 -.33 -.26 -.42 -.45 -.43 -.34 -.31 -.28 -.23 -.24 -.10 -.33 -.35\n   DJF  MAM  JJA  SON\n1  *** -.11 -.16 -.20\n2 -.17  .05 -.07 -.18\n3  .08 -.08 -.16 -.18\n4 -.34 -.16 -.10 -.19\n5 -.11 -.37 -.31 -.28\n6 -.41 -.38 -.36 -.25\n\n\nThis dataset has month across the top with some other columns I do not necessarily understand, and the values in the month columns are the temperature anomaly. Right now all the columns have been read in character format, so we can do some data wrangling to get the data in the correct format for ggplot2. This includes going from wide to long format, stacking all the months on top of each other in a single column.\nThese commands could all be linked together using successive pipe commands, but I kept them separate to better document what each line is doing.\n\n# wrangle the data set\n# make all columns numeric - there are some *** that i dont know what they mean\nd <- mutate(d, across(everything(), as.numeric))\n\n# replace NAs with 0 - just make them to have no positive or negative anomaly\nd <- mutate(d, across(everything(), ~(replace_na(.x, 0))))\n\n# tidy the dataframe\nd <- janitor::clean_names(d) %>%\n  pivot_longer(cols = jan:dec, names_to = 'month', values_to = 'anomaly')\n\n# add a time column\nd <- mutate(d, time = paste(year, toupper(month), sep = '-'),\n            time = lubridate::ym(time),\n            month2 = lubridate::month(time)) %>%\n  # select only necessary columns\n  select(year, month, month2, time, anomaly)\n\n# look at dataset\nhead(d)\n\n# A tibble: 6 × 5\n   year month month2 time       anomaly\n  <dbl> <chr>  <dbl> <date>       <dbl>\n1  1880 jan        1 1880-01-01   -0.18\n2  1880 feb        2 1880-02-01   -0.24\n3  1880 mar        3 1880-03-01   -0.09\n4  1880 apr        4 1880-04-01   -0.16\n5  1880 may        5 1880-05-01   -0.1 \n6  1880 jun        6 1880-06-01   -0.21\n\n\nThe most recent data in the spreadsheet from 2023, but we will remove this at the moment because it is mid-year.\n\n# remove 2023\nd <- filter(d, year < 2023)\n\nWe are now ready to make our first plot. I have chosen a MetBrewer colour palette which goes from blue to red to represent the hot and cold colours. For the plot I will use a theme I defined previously in a random R package called theme_black() (lots of other themes are available).\n\n# create first plot\np1 <- ggplot(d, aes(month2, anomaly, col = anomaly)) +\n  geom_line(aes(group = year)) +\n  scale_color_gradientn('Temperature anomaly', colors = met.brewer(name='Homer1', direction=-1, override.order = FALSE)) +\n  MicrobioUoE::theme_black() +\n  theme(panel.grid.major = element_blank(),\n        axis.title.x = element_blank()) +\n  scale_x_continuous(breaks = 1:12, labels = c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'))\n\np1 + coord_polar()\n\n\n\n\nThis is looking quite good, but still quite far from the polished version on NASA’s YouTube. One of the problems we have is that geom_path() will not cross over the 0-360 line when using coord_polar(). This would ruin the whole visualisation and prevent us having a smooth transition from year to year. This was a problem I was not clever enough to solve, but luckily StackOverflow had the answer. We need to convert the polar coordinates to cartesian to allow the animation to go from December to January of the next year.\n\n# want to convert polar coordinates to cartesian to allow animation to go from December back to January\n# using this link https://stackoverflow.com/questions/55132352/geom-path-refuses-to-cross-over-the-0-360-line-in-coord-polar\n\n# calculate angles for each month, starting with January at 15º\nangles <- tibble(month = unique(d$month),\n            angle = seq(15, by = 30, length.out = 12))\n\nd <- left_join(d, angles)\n\nhead(d)\n\n# A tibble: 6 × 6\n   year month month2 time       anomaly angle\n  <dbl> <chr>  <dbl> <date>       <dbl> <dbl>\n1  1880 jan        1 1880-01-01   -0.18    15\n2  1880 feb        2 1880-02-01   -0.24    45\n3  1880 mar        3 1880-03-01   -0.09    75\n4  1880 apr        4 1880-04-01   -0.16   105\n5  1880 may        5 1880-05-01   -0.1    135\n6  1880 jun        6 1880-06-01   -0.21   165\n\n# add buffer to make everything positive - this can be anything really\nbuffer <- 3\n\n# calculate the position in x/y space for each anomaly using maths!!!\nd <- mutate(d, x_anomaly = sin(pi * angle/180) * (anomaly + buffer),\n               y_anomaly = cos(pi * angle/180) * (anomaly + buffer))\n\nhead(d)\n\n# A tibble: 6 × 8\n   year month month2 time       anomaly angle x_anomaly y_anomaly\n  <dbl> <chr>  <dbl> <date>       <dbl> <dbl>     <dbl>     <dbl>\n1  1880 jan        1 1880-01-01   -0.18    15     0.730     2.72 \n2  1880 feb        2 1880-02-01   -0.24    45     1.95      1.95 \n3  1880 mar        3 1880-03-01   -0.09    75     2.81      0.753\n4  1880 apr        4 1880-04-01   -0.16   105     2.74     -0.735\n5  1880 may        5 1880-05-01   -0.1    135     2.05     -2.05 \n6  1880 jun        6 1880-06-01   -0.21   165     0.722    -2.69 \n\n\nWe now have the position of each anomaly for each month in x/y space. This should allow the animation to smoothly move between December of one year to January of the next.\nOur original plot also does not have very nice labelling to tell us anything about the extent of the anomaly (1ºC above or below average etc.)\nHowever, we can add these ourselves by using some functions from ggforce to create circles on our plot and labelling them. We also create a dataframe to place the months at a nicer place on the plot.\nWe can control the font size of geom_text() in terms of font size used in the rest of the plot using the pts() function that I copied from Andrew Heiss.\n\n# create limits\nlimits <- c(-(max(d$anomaly) + buffer), max(d$anomaly) + buffer)\n\n# create circles for some temperatures\ncircles <- data.frame(\n  r = c(buffer, buffer + 1, buffer - 1)) %>%\n  mutate(r = round(r, 0),\n         x0 = 0,\n         y0 = 0,\n         label = c('0ºC', '+1ºC', '-1ºC'))\n\n# create dataset for months to be added to the plot\nmonths <- mutate(angles, anomaly = limits[2] + limits[2] * .1,\n                 x_anomaly = sin(pi * angle/180) * anomaly,\n                 y_anomaly = cos(pi * angle/180) * anomaly)\n\n# increase limits slightly\nlimits <- limits + limits*.1\n\n# create base plot\np1 <- ggplot(d, aes(x_anomaly, y_anomaly)) +\n  scale_x_continuous(limits = limits) +\n  scale_y_continuous(limits = limits) +\n  ggforce::theme_no_axes() +\n  theme(panel.background = element_rect(fill = 'black'), \n        legend.position = 'none') +\n  geom_text(aes(x_anomaly, y_anomaly, label = month), data = months, col = 'white', size = MicrobioUoE::pts(18)) +\n  geom_path(aes(col = anomaly), alpha = 0.9) +\n  scale_color_gradientn('Temperature anomaly', colors = met.brewer(name='Homer1', direction=-1, override.order = F)) +\n  ggforce::geom_circle(aes(x0 = x0, y0 = y0, r = r, linewidth = I(1.5)), fill = NA, data = circles, inherit.aes = FALSE,col = 'white') +\n  geom_label(aes(x0, r, label = label), circles, fill = 'white', size = MicrobioUoE::pts(18)) +\n  coord_equal() +\n  geom_text(aes(x = 0, y = 0, label = year, col = anomaly), size = MicrobioUoE::pts(30))\n\np1\n\n\n\n\nThis looks great and more like what NASA created. We can easily animate this using gganimate::transition_reveal() and adding the column for time that we created earlier.\nWe can add a frame for every single row of the dataframe using nframes = nrow(d). We will also add a point to make it more obvious where we are at any given time.\n\n# add animation layer using gganimate\np_anim <- p1 +\n  geom_point(shape = 21, fill = 'green', data = d, size = 4, col = 'white') +\n  transition_reveal(time)\n\n# save out the animation\nanim_save(filename = 'posts/climate_spiral/climate_spiral.gif', p_anim, nframes = nrow(d), duration = 45, width = 5.5, height = 6, units = 'in', renderer = gifski_renderer(), res = 150)\n\n\nThis looks amazing! There are probably ways I could improve the animation but this suits my motivations right now. For example, I have not linked the size of the line to the extent of the change, but this should be relatively easy to implement.\nI generally love this type of visualisation, and the spiral could be used on other data, such as atmospheric carbon dioxide levels or Arctic sea ice extent, to show how these variables have changed through time. By presenting data in this way, the Climate Spiral helps people to understand the steady, consistent increase in global temperatures over time, and the urgency of addressing climate change.\nLet me know if you found this helpful and if you any questions!"
  },
  {
    "objectID": "posts/nlsmultstart/index.html",
    "href": "posts/nlsmultstart/index.html",
    "title": "Doing robust nls regression in R",
    "section": "",
    "text": "With my research, I often use non-linear least squares regression to fit a model with biologically meaningful parameters to data. Specifically, I measure the thermal performance of phytoplankon growth, respiration and photosynthesis over a wide range of assay temperatures to see how the organisms are adapted to the temperatures they live at.\nThese thermal performance curves generally follow a unimodal shape and parameters for which are widely used in climate change research to predict whether organisms will be able to cope with increasing temperatures.\n\n\n\nExample Thermal Performance Curve\n\n\nThese curves can be modelled with a variety of equations, such as the Sharpe-Schoolfield equation, which I have log-transformed here:\n\\[log(rate) = lnc + E(\\frac{1}{T_{c}} - \\frac{1}{kT}) - ln(1 + e^{E_h(\\frac{1}{kT_h} - \\frac{1}{kT})})\\] where \\(lnc\\) is a normalisation constant at a common temperature, \\(T_{c}\\), \\(E\\) is an activation energy that describes the rate of increase before the optimum temperature, \\(T_{opt}\\). \\(k\\) is Boltzmann’s constant, \\(E_{h}\\) is the deactivation energy that controls the decline in rate past the optimum temperature and \\(T_{h}\\) is the temperature where, after the optimu, the rate is half of the maximal rate.\nSay I want to fit the same equation to 10, 50, or 100s of these curves. I could loop through a call to nls(), nlsLM(), or use nlsList() from nlme. However, non-linear least squares regression in R is sensitive to the start parameters, meaning that different start parameters can give different “best estimated parameters”. This becomes more likely when fitting more curves with only a single set of start parameters, where the variation in estimated parameter values is likely to be much larger. For example, some curves could have much higher rates (\\(lnc\\)), higher optimum temperatures (i.e. \\(T_{h}\\)) or have different values of temperature-dependence (\\(E\\)).\nTo combat this, I wrote an R package which allows for multiple start parameters for non-linear regression. I wrapped this method in an R package called nlsLoop and submitted it to The Journal of Open Source Software. Everything was good with the world and I went to a Christmas party.\nThe next day, I had an epiphany surrounding the redundancies and needless complexities of my R package, withdrew my submission and rewrote the entire package in a weekend to give rise to a single function package, nls.multstart::nls_multstart(). Essentially since I first wrote nlsLoop ~3 years ago I have realised that broom and purrr can do what I wrote clunkier functions to achieve. In contrast, nls.multstart works perfectly with the tools of the tidyverse to fit multiple models."
  },
  {
    "objectID": "posts/nlsmultstart/index.html#multiple-model-fitting-in-practice",
    "href": "posts/nlsmultstart/index.html#multiple-model-fitting-in-practice",
    "title": "Doing robust nls regression in R",
    "section": "Multiple model fitting in practice",
    "text": "Multiple model fitting in practice\nLoad in all packages that are used in this analysis. Packages can be installed from GitHub using devtools.\n\n# load packages\nlibrary(nls.multstart)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(nlstools)\n\nWe can then load in the data and have a look at it using glimpse(). Here we shall use a dataset of thermal performance curves of metabolism of Chlorella vulgaris from Padfield et al. 2016.\n\n# load in example data set\ndata(\"Chlorella_TRC\")\nglimpse(Chlorella_TRC)\n\nRows: 649\nColumns: 7\n$ curve_id    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2…\n$ growth.temp <dbl> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20…\n$ process     <chr> \"acclimation\", \"acclimation\", \"acclimation\", \"acclimation\"…\n$ flux        <chr> \"respiration\", \"respiration\", \"respiration\", \"respiration\"…\n$ temp        <dbl> 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 16, 19, 22…\n$ K           <dbl> 289.15, 292.15, 295.15, 298.15, 301.15, 304.15, 307.15, 31…\n$ ln.rate     <dbl> -2.06257833, -1.32437939, -0.95416807, -0.79443675, -0.182…\n\n\nNext we define the Sharpe-Schoolfield equation discussed earlier.\n\n# define the Sharpe-Schoolfield equation\nschoolfield_high <- function(lnc, E, Eh, Th, temp, Tc) {\n  Tc <- 273.15 + Tc\n  k <- 8.62e-5\n  boltzmann.term <- lnc + log(exp(E/k*(1/Tc - 1/temp)))\n  inactivation.term <- log(1/(1 + exp(Eh/k*(1/Th - 1/temp))))\n  return(boltzmann.term + inactivation.term)\n  }\n\nThere are 60 curves in this dataset, 30 each of photosynthesis and respiration. The treatments are growth temperature (20, 23, 27, 30, 33 ºC) and adaptive process (acclimation or adaptation) that reflects the number of generations cultures were grown at each temperature.\nWe can see how nls_multstart() works by subsetting the data for a single curve.\n\n# subset dataset\nd_1 <- subset(Chlorella_TRC, curve_id == 1)\n# run nls_multstart\nfit <- nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),\n                     data = d_1,\n                     iter = 500,\n                     start_lower = c(lnc = -10, E = 0.1, Eh = 0.2, Th = 285),\n                     start_upper = c(lnc = 10, E = 2, Eh = 5, Th = 330),\n                     supp_errors = 'Y',\n                     na.action = na.omit,\n                     lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))\nfit\n\nNonlinear regression model\n  model: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)\n   data: data\n     lnc        E       Eh       Th \n -1.3462   0.9877   4.3326 312.1887 \n residual sum-of-squares: 7.257\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 1.49e-08\n\n\nnls_multstart() allows boundaries for each parameter to be set. A uniform distribution between these values is created and start values for each iteration of the fitting process are then picked randomly. The function returns the best available model by picking the model with the lowest AIC score. Additional info on the function can be found here or by typing ?nls_multstart into the R console.\nThis fit can then be “tidied” in various ways using the R package broom. Each different function in broom returns a different set of information. tidy() returns the estimated parameters, augment() returns the predictions and glance() returns information about the model such as the AIC score and whether the model has reached convergence. Confidence intervals of non-linear regression can also be estimated using nlstools::confint2()\nThe amazing thing about these tools is the ease at which they can then be used on multiple curves at once, an approach Hadley Wickham has previously written about. The approach nests the data based on grouping variables using nest(), then creates a list column of the best fit for each curve using map().\n\n# fit over each set of groupings\nfits <- Chlorella_TRC %>%\n  group_by(., flux, growth.temp, process, curve_id) %>%\n  nest() %>%\n  mutate(fit = purrr::map(data, ~ nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),\n                                   data = .x,\n                                   iter = 1000,\n                                   start_lower = c(lnc = -10, E = 0.1, Eh = 0.2, Th = 285),\n                                   start_upper = c(lnc = 10, E = 2, Eh = 5, Th = 330),\n                                   supp_errors = 'Y',\n                                   na.action = na.omit,\n                                   lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))))\n\nIf you are confused, then you are not alone. This took me a long time to understand and I imagine there are still better ways for me to do it! However, to check it has worked, we can look at a single fit to check it looks ok. We can also look at fits to see that there is now a fit list column containing each of the non-linear fits for each combination of our grouping variables.\n\n# look at a single fit\nsummary(fits$fit[[1]])\n\n\nFormula: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)\n\nParameters:\n    Estimate Std. Error t value Pr(>|t|)    \nlnc  -1.3462     0.4656  -2.891   0.0202 *  \nE     0.9877     0.4521   2.185   0.0604 .  \nEh    4.3326     1.4878   2.912   0.0195 *  \nTh  312.1887     3.8782  80.499 6.32e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9524 on 8 degrees of freedom\n\nNumber of iterations to convergence: 19 \nAchieved convergence tolerance: 1.49e-08\n\n# look at output object\nselect(fits, curve_id, data, fit)\n\nAdding missing grouping variables: `flux`, `growth.temp`, `process`\n\n\n# A tibble: 60 × 6\n# Groups:   flux, growth.temp, process, curve_id [60]\n   flux        growth.temp process     curve_id data              fit   \n   <chr>             <dbl> <chr>          <dbl> <list>            <list>\n 1 respiration          20 acclimation        1 <tibble [12 × 3]> <nls> \n 2 respiration          20 acclimation        2 <tibble [12 × 3]> <nls> \n 3 respiration          23 acclimation        3 <tibble [12 × 3]> <nls> \n 4 respiration          27 acclimation        4 <tibble [9 × 3]>  <nls> \n 5 respiration          27 acclimation        5 <tibble [12 × 3]> <nls> \n 6 respiration          30 acclimation        6 <tibble [12 × 3]> <nls> \n 7 respiration          30 acclimation        7 <tibble [12 × 3]> <nls> \n 8 respiration          33 acclimation        8 <tibble [10 × 3]> <nls> \n 9 respiration          33 acclimation        9 <tibble [8 × 3]>  <nls> \n10 respiration          20 acclimation       10 <tibble [10 × 3]> <nls> \n# … with 50 more rows\n\n\nThese fits can be cleaned up using the broom functions and purrr::map() to iterate over the grouping variables.\n\n# get summary info\ninfo <- fits %>%\n  mutate(., info = map(fit, glance)) %>%\n  unnest(info)\n\n# get params\nparams <- fits %>%\n  mutate(., params = map(fit, tidy)) %>%\n  unnest(params)\n  \n# get confidence intervals\nCI <- fits %>% \n  mutate(., CI = map(fit, function(x)data.frame(confint2(x)))) %>%\n  unnest(CI) %>%\n  select(-data, -fit) %>%\n  rename(., conf.low = `X2.5..`, conf.high = `X97.5..`) %>%\n  group_by(., curve_id) %>%\n  mutate(., term = c('lnc', 'E', 'Eh', 'Th')) %>%\n  ungroup()\n# merge parameters and CI estimates\nparams <- merge(params, CI, by = intersect(names(params), names(CI)))\n\n# get predictions\npreds <- fits %>%\n  mutate(., preds = map(fit, augment)) %>%\n  unnest(preds)\n\nLooking at info allows us to see if all the models converged.\n\nungroup(info) %>% select(., curve_id, logLik, AIC, BIC, deviance, df.residual)\n\n# A tibble: 60 × 6\n   curve_id  logLik   AIC   BIC deviance df.residual\n      <dbl>   <dbl> <dbl> <dbl>    <dbl>       <int>\n 1        1 -14.0   38.0  40.4     7.26            8\n 2        2  -1.20  12.4  14.8     0.858           8\n 3        3  -7.39  24.8  27.2     2.41            8\n 4        4  -0.523 11.0  12.0     0.592           5\n 5        5 -10.8   31.7  34.1     4.29            8\n 6        6  -8.52  27.0  29.5     2.91            8\n 7        7  -1.29  12.6  15.0     0.871           8\n 8        8 -13.4   36.7  38.2     8.48            6\n 9        9   1.82   6.36  6.76    0.297           4\n10       10  -1.27  12.5  14.1     0.755           6\n# … with 50 more rows\n\n\nWhen plotting non-linear fits, I prefer to have a smooth curve, even when there are not many points underlying the fit. This can be achieved by including newdata in the augment() function and creating a higher resolution set of predictor values.\nHowever, when predicting for many different fits, it is not certain that each curve has the same range of predictor variables. We can get around this by setting the limits of each prediction by the min() and max() of the predictor variables.\n\n# new data frame of predictions\nnew_preds <- Chlorella_TRC %>%\n  do(., data.frame(K = seq(min(.$K), max(.$K), length.out = 150), stringsAsFactors = FALSE))\n# max and min for each curve\nmax_min <- group_by(Chlorella_TRC, curve_id) %>%\n  summarise(., min_K = min(K), max_K = max(K), .groups = 'drop')\n\n# create new predictions\npreds2 <- fits %>%\n  mutate(preds = map(fit, augment, newdata = new_preds)) %>%\n  unnest(preds) %>%\n  merge(., max_min, by = 'curve_id') %>%\n  group_by(., curve_id) %>%\n  filter(., K > unique(min_K) & K < unique(max_K)) %>%\n  rename(., ln.rate = .fitted) %>%\n  ungroup()\n\nThese can then be plotted using ggplot2.\n\n# plot\nggplot() +\n  geom_point(aes(K - 273.15, ln.rate, col = flux), size = 2, Chlorella_TRC) +\n  geom_line(aes(K - 273.15, ln.rate, col = flux, group = curve_id), alpha = 0.5, preds2) +\n  facet_wrap(~ growth.temp + process, labeller = labeller(.multi_line = FALSE)) +\n  scale_colour_manual(values = c('green4', 'black')) +\n  theme_bw(base_size = 12, base_family = 'Helvetica') +\n  ylab('log Metabolic rate') +\n  xlab('Assay temperature (ºC)') +\n  theme(legend.position = c(0.9, 0.15))\n\n\n\n\nThe confidence intervals of each parameter for each curve fit can also be easily visualised.\n\n# plot\nggplot(params, aes(col = flux)) +\n  geom_point(aes(curve_id, estimate)) +\n  facet_wrap(~ term, scale = 'free_x', ncol = 4) +\n  geom_linerange(aes(curve_id, ymin = conf.low, ymax = conf.high)) +\n  coord_flip() +\n  scale_color_manual(values = c('green4', 'black')) +\n  theme_bw(base_size = 12, base_family = 'Helvetica') +\n  theme(legend.position = 'top') +\n  xlab('curve') +\n  ylab('parameter estimate')\n\n\n\n\nThis method of modelling can be used for different data, different non-linear models (and linear models for that matter) and combined with the tidyverse can make very useful visualisations.\nThe next stage of these curve fits is to try and better understand the uncertainty of these curve fits and their predictions. One approach to achieve this could be bootstrapping new datasets from the existing data. I hope to demonstrate how this could be done soon in another post."
  },
  {
    "objectID": "posts/nlsmultstart/index.html#references",
    "href": "posts/nlsmultstart/index.html#references",
    "title": "Doing robust nls regression in R",
    "section": "References",
    "text": "References\n[1] Padfield, D., Yvon-durocher, G., Buckling, A., Jennings, S. & Yvon-durocher, G. (2016). Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton. Ecology Letters, 19(2), 133-142."
  },
  {
    "objectID": "posts/animate_rstrava/index.html",
    "href": "posts/animate_rstrava/index.html",
    "title": "rStrava and gganimate",
    "section": "",
    "text": "rStrava is an R package that allows you to access data from Strava using the Strava API. Some of the functions of rStrava scrape data from the public Strava website but to access your own data you will need a Strava profile and an authentication token. Details on obtaining your unique token can be found on the rStrava GitHub In addition to this key, we use rgbif::elevation() to calculate the elevation of each route. This requires a Google API key which can be created here.\nGot a Strava authentication token? Got a Google API key? We are ready to create some animations! To create our animations, we use gganimate that requires ImageMagick to be installed."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#loading-packages-and-defining-tokens",
    "href": "posts/animate_rstrava/index.html#loading-packages-and-defining-tokens",
    "title": "rStrava and gganimate",
    "section": "Loading packages and defining tokens",
    "text": "Loading packages and defining tokens\nFirst load the packages that are used in the script and our Strava and Google authentication tokens. The app_scope argument in strava_oauth() has to be one of “read” , “read_all”, “profile:read_all”, “profile:write”, “activity:read”, “activity:read_all” or “activity:write”. To access your activities, activity:read_all has to be included.\n\n# load packages ####\nlibrary(rStrava) # devtools::install_github('fawda123/rStrava')\nlibrary(gganimate)\nlibrary(tidyverse)\nlibrary(sp)\nlibrary(ggmap)\nlibrary(raster)\n\n# initial setup ####\n# Strava key\napp_name <- 'xxxx'\napp_client_id <- 'xxxxx'\napp_secret <- '\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"'\n\n# create strava token\nmy_token <- httr::config(token = strava_oauth(app_name, app_client_id, app_secret, app_scope = 'read_all,activity:read_all'))\n\n# Google elevation API key\nGoogleAPI <- 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n\nA browser window should open at this point saying Authentication complete. Please close this page and return to R. This means everything is going well!"
  },
  {
    "objectID": "posts/animate_rstrava/index.html#download-your-data",
    "href": "posts/animate_rstrava/index.html#download-your-data",
    "title": "rStrava and gganimate",
    "section": "Download your data",
    "text": "Download your data\nWe can then download our personal activity data using the rStrava::get_activity_list(). This function needs your strava token and your strava athlete id. For example, my strava id is 2140248.\n\n# download strava data\nmy_acts <- get_activity_list(my_token)\n\nlength(my_acts)\n\n[1] 1664\n\n\nThis returns a large list of all your previous activities. Mine has 1028 previous entries. If you want to explore your list, you can use View(my_acts) in RStudio which opens the Data Viewer window."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#compile-your-data-into-tidy-dataframe",
    "href": "posts/animate_rstrava/index.html#compile-your-data-into-tidy-dataframe",
    "title": "rStrava and gganimate",
    "section": "Compile your data into “tidy” dataframe",
    "text": "Compile your data into “tidy” dataframe\nrStrava has a function that compiles the information stored in the output of get_activity_list() to a “tidy” dataframe, with one row for each activity. compile_activities() finds all the columns across all activities and returns NA when a column is not present in a given activity. This means that if HR was not measured across all your strava activities, the function will still work!\n\n# compile activities into a tidy dataframe\nmy_acts <- compile_activities(my_acts)\n\n# have a look at the dataframe\ndplyr::glimpse(my_acts)\n\nRows: 1,664\nColumns: 60\n$ achievement_count             <dbl> 1, 11, 1, 23, 3, 2, 0, 0, 28, 10, 0, 0, …\n$ athlete_count                 <dbl> 1, 1, 1, 51, 1, 1, 1, 1, 1, 7, 1, 1, 1, …\n$ athlete.id                    <chr> \"2140248\", \"2140248\", \"2140248\", \"214024…\n$ athlete.resource_state        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ average_cadence               <chr> \"79.5\", \"81.3\", \"80.1\", \"84.9\", \"82.2\", …\n$ average_heartrate             <chr> \"142.1\", \"144.7\", \"153.4\", \"163.2\", \"134…\n$ average_speed                 <dbl> 12.1248, 12.7764, 12.2328, 15.4836, 12.7…\n$ comment_count                 <dbl> 2, 0, 0, 3, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0…\n$ commute                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ display_hide_heartrate_option <chr> \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", …\n$ distance                      <dbl> 6.3254, 15.3099, 3.0110, 5.0749, 3.9523,…\n$ elapsed_time                  <dbl> 1923, 4469, 895, 1180, 1597, 3372, 5257,…\n$ elev_high                     <dbl> 75.9, 102.7, 75.9, 53.6, 75.9, 22.5, 200…\n$ elev_low                      <dbl> 68.3, 45.1, 55.6, 49.9, 49.9, 3.4, 14.8,…\n$ end_latlng1                   <dbl> 51.91218, 51.91868, 51.91910, 51.91133, …\n$ end_latlng2                   <dbl> -2.053098, -2.048171, -2.049546, -2.0769…\n$ external_id                   <chr> \"garmin_ping_258943346472\", \"garmin_ping…\n$ flagged                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ from_accepted_tag             <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ gear_id                       <chr> \"g12286967\", \"g12438516\", \"g12286967\", \"…\n$ has_heartrate                 <chr> \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", \"TRUE\", …\n$ has_kudoed                    <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ heartrate_opt_out             <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ id                            <dbl> 8519266303, 8507709513, 8501999743, 8501…\n$ kudos_count                   <dbl> 3, 27, 5, 26, 4, 11, 14, 20, 37, 28, 1, …\n$ location_country              <chr> \"United Kingdom\", \"United Kingdom\", \"Uni…\n$ manual                        <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ map.id                        <chr> \"a8519266303\", \"a8507709513\", \"a85019997…\n$ map.resource_state            <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ map.summary_polyline          <chr> \"ec{{H|aoKPHP@bAb@h@L|ATh@@ZJb@?b@CJCrBA…\n$ max_heartrate                 <chr> \"166\", \"173\", \"169\", \"174\", \"147\", \"173\"…\n$ max_speed                     <dbl> 19.9656, 16.5240, 23.7168, 19.1880, 19.2…\n$ moving_time                   <dbl> 1878, 4314, 886, 1180, 1119, 3148, 5185,…\n$ name                          <chr> \"Morning Run\", \"Morning Run\", \"Morning R…\n$ photo_count                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pr_count                      <chr> \"0\", \"7\", \"0\", \"9\", \"0\", \"1\", \"0\", \"0\", …\n$ private                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ resource_state                <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ sport_type                    <chr> \"Run\", \"Run\", \"Run\", \"Run\", \"Run\", \"Run\"…\n$ start_date                    <chr> \"2023-02-07T08:29:54Z\", \"2023-02-05T08:2…\n$ start_date_local              <chr> \"2023-02-07T08:29:54Z\", \"2023-02-05T08:2…\n$ start_latlng1                 <dbl> 51.91904, 51.91906, 51.90978, 51.91065, …\n$ start_latlng2                 <dbl> -2.049192, -2.049080, -2.068779, -2.0750…\n$ suffer_score                  <chr> \"33\", \"86\", \"27\", \"55\", \"14\", \"50\", \"29\"…\n$ timezone                      <chr> \"(GMT+00:00) Europe/London\", \"(GMT+00:00…\n$ total_elevation_gain          <dbl> 15.0, 121.2, 25.1, 9.9, 5.1, 4.7, 463.0,…\n$ total_photo_count             <dbl> 0, 3, 0, 3, 0, 3, 0, 5, 3, 1, 0, 0, 0, 0…\n$ trainer                       <chr> \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FAL…\n$ type                          <chr> \"Run\", \"Run\", \"Run\", \"Run\", \"Run\", \"Run\"…\n$ upload_id                     <chr> \"9143955806\", \"9131036041\", \"9124588035\"…\n$ upload_id_str                 <chr> \"9143955806\", \"9131036041\", \"9124588035\"…\n$ utc_offset                    <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", …\n$ visibility                    <chr> \"everyone\", \"everyone\", \"everyone\", \"eve…\n$ average_temp                  <chr> NA, NA, NA, NA, NA, NA, \"9\", NA, NA, NA,…\n$ average_watts                 <dbl> NA, NA, NA, NA, NA, NA, 130.8, NA, NA, N…\n$ device_watts                  <chr> NA, NA, NA, NA, NA, NA, \"FALSE\", NA, NA,…\n$ kilojoules                    <dbl> NA, NA, NA, NA, NA, NA, 678.2, NA, NA, N…\n$ location_city                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ location_state                <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ workout_type                  <chr> NA, \"0\", NA, \"0\", NA, \"0\", NA, \"0\", \"0\",…\n\n\nThere are so many columns here, so I remove some columns I am not interested in for this post and do some data transformations to get the date in a correct format. I also do not want to run the example on all my rides, instead I filter them for just 2020.\n\n# columns to keep\ndesired_columns <- c('distance', 'elapsed_time', 'moving_time', 'start_date', 'start_date_local', 'type', 'map.summary_polyline', 'location_city', 'upload_id')\n\n# keep only desired columns\nmy_acts2 <- dplyr::select(my_acts, any_of(desired_columns))\n\n# transformations ####\nmy_acts <- mutate(my_acts,\n                  activity_no = seq(1,n(), 1),\n                  elapsed_time = elapsed_time/60/60,\n                  moving_time = moving_time/60/60, \n                  date = gsub(\"T.*$\", '', start_date) %>%\n                    as.POSIXct(., format = '%Y-%m-%d'),\n                  EUdate = format(date, '%d/%m/%Y'),\n                  month = format(date, \"%m\"),\n                  day = format(date, \"%d\"),\n                  year = format(date, \"%Y\")) %>%\n  mutate(., across(c(month, day), as.numeric)) %>%\n  filter(.,year == '2020')"
  },
  {
    "objectID": "posts/animate_rstrava/index.html#get-latitude-and-longitude-for-each-activity",
    "href": "posts/animate_rstrava/index.html#get-latitude-and-longitude-for-each-activity",
    "title": "rStrava and gganimate",
    "section": "Get latitude and longitude for each activity",
    "text": "Get latitude and longitude for each activity\nEach activity has a bunch of data associated with it. For mapping, I am interested in the map.summary_polyline, which is a Google polyline which allows the encoding of multiple latitude and longitude points as a single string. We can get the latitude and longitude for each of the activities by using get_latlon() which decodes the polylines and using dplyr and purrr to iterate over every activity in the dataframe. I add my Google API key as a column so that map() can easily find it.\n\n# get lat lon and distance of every ride ####\nlat_lon <- my_acts %>%\n  filter(!is.na(map.summary_polyline)) %>%\n  filter(type == 'Ride') %>%\n  mutate(key = GoogleAPI) %>%\n  group_by(activity_no) %>%\n  nest() %>%\n  mutate(coords = map(data, ~get_latlon(.$map.summary_polyline, key = .$key)),\n         dist = map(coords, ~get_dists(.x$lon, .x$lat))) %>%\n  unnest(., data) %>%\n  unnest(., c(coords, dist))\n\nHaving got the latitude and longitude for every ride, we can now get the elevation of each point and then calculate the gradient between points. To do this I use elevation() in the R package rgbif. To use this, you need to get a GeoNames username by registering for an account at http://www.geonames.org/login.\n\n# get elevation and calculate gradient between points\nlat_lon <- ungroup(lat_lon) %>%\n  mutate(., ele = rgbif::elevation(latitude = .$lat, longitude = .$lon, user = 'YOUR USERNAME HERE', model = 'srtm1')$elevation_geonames)\n\nlat_lon <- group_by(lat_lon, activity_no) %>%\n  mutate(., ele_diff = c(0, diff(ele)),\n         dist_diff = c(0, diff(dist)),\n         grad = c(0, (ele_diff[2:n()]/10)/dist_diff[2:n()])) %>%\n  ungroup() %>%\n  dplyr::select(., -c(ele_diff, dist_diff))\n\n\n\n\nThis now gives us a data frame of all my rides from 2020 with the latitude, longitude, cumulative distance, elevation and gradient. It would now be super easy to create elevation profiles, but I will save that for another post."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#create-a-gif-of-a-single-ride",
    "href": "posts/animate_rstrava/index.html#create-a-gif-of-a-single-ride",
    "title": "rStrava and gganimate",
    "section": "Create a gif of a single ride",
    "text": "Create a gif of a single ride\nWe now have almost all the components to create a gif of a single ride.\n\nlat_lon_single <- filter(lat_lon, activity_no == min(activity_no))\nnrow(lat_lon_single)\n\n[1] 343\n\n\nHowever, Google polylines do not give a consistent number of latitude and longitude points. This means it might be hard to get a smooth elevation profile for the ride and also for smooth transitions in a gif. To create a set number of points from the current polyline, we can use geospatial packages such as sp and raster to interpolate a desired number of points from the current ones. Here I create 250 points.\n\n# reorder columns so lat lon are first\nlat_lon_single <- dplyr::select(lat_lon_single, lat, lon, everything())\n\n# make new data with by interpolation\ninterp <- raster::spLines(as.matrix(lat_lon_single[,1:2])) %>%\n  sp::spsample(., n = 250, type = 'regular') %>%\n  data.frame() %>%\n  mutate(., dist = get_dists(lon, lat),\n         ele = rgbif::elevation(latitude = .$lat, longitude = .$lon, user = 'padpadpadpad', model = 'srtm1')$elevation_geoname,\n         ele_diff = c(0, diff(ele)),\n         dist_diff = c(0, diff(dist)),\n         grad = c(0, (ele_diff[2:n()]/10)/dist_diff[2:n()]),\n         n = row_number())\n\nWe can now put the gif together, using ggmap and ggplot2. We use gganimate to make the plot animated.\n\n# make bbox\nbbox <- ggmap::make_bbox(lon, lat, data = lat_lon_single, f = 1.3)\n\n# download map\nmap <- get_map(location = bbox, source = 'google', maptype = 'terrain')\n\nsingle_ride <- ggmap(map, darken = 0.15) +\n  geom_path(aes(x = lon, y = lat,  col = grad, group = 1), data = interp, size = 2, alpha = 1) +\n  scale_color_distiller('Gradient (%)', palette = 'Spectral') +\n  labs(title = '') +\n  coord_cartesian() +\n  ggforce::theme_no_axes(theme_bw(base_size = 16)) +\n  transition_reveal(dist)\n\n# animate plot\ngganimate::anim_save('where_to_save.gif', single_ride, width = 1000, height = 700)\n\nThe output of this code can be seen below."
  },
  {
    "objectID": "posts/animate_rstrava/index.html#create-a-gif-of-all-the-rides",
    "href": "posts/animate_rstrava/index.html#create-a-gif-of-all-the-rides",
    "title": "rStrava and gganimate",
    "section": "Create a gif of ALL the rides",
    "text": "Create a gif of ALL the rides\nWe can also make a gif of multiple activities. I will filter my activities to only be bike rides over 15km.\n\n# get a bbox for Cornwall\nbbox <- ggmap::make_bbox(lat_lon_single$lon, lat_lon_single$lat, f = 1.2)\n\n# add column for frame and total distance per ride\nlat_lon <- group_by(lat_lon, activity_no) %>%\n  mutate(n = 1:n(),\n         tot_dist = max(distance)) %>%\n  ungroup()\n\n# filter lat_lon for when points are within this\nlat_lon <- filter(lat_lon, between(start_longitude, bbox[1], bbox[3]) & between(start_latitude, bbox[2], bbox[4]) & type == 'Ride' & tot_dist > 15)\n\n# add column for frame\nlat_lon <- group_by(lat_lon, activity_no) %>%\n  mutate(n = 1:n()) %>%\n  ungroup()\n\n# make bbox again\nbbox <- ggmap::make_bbox(lon, lat, data = lat_lon, f = 0.1)\n\n# download map\nmap <- get_map(location = bbox, source = 'google', maptype = 'terrain')\n\nall_the_rides <- ggmap(map, darken = 0.15) +\n  geom_path(aes(x = lon, y = lat, group = activity_no), col = 'red', data = lat_lon, size = 1.25, alpha = 0.5) +\n  labs(title = 'All the rides') +\n  coord_cartesian() +\n  ggforce::theme_no_axes(theme_bw(base_size = 16)) +\n  theme(legend.position = 'none') +\n  transition_reveal(n)\n\n# animate plot\ngganimate::anim_save('where_to_save.gif', all_the_rides, width = 750, height = 700)\n\n\nAnd there we have it. A relatively simple way to animate your strava activities in R. I personally find that saving the output as .mp4 rather than .gif gives smaller and higher quality files when uploading them to Instagram, but these options are easy to change. Take back your own data and get plotting!\nThere are loads of other functions and uses for the rStrava package. I hope to blog more about them soon."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html",
    "href": "posts/bootstrap_nls/index.html",
    "title": "Bootstrap nls models in R",
    "section": "",
    "text": "This post was updated to reflect the improvement of using car to bootstrap nonlinear regressions."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#introduction",
    "href": "posts/bootstrap_nls/index.html#introduction",
    "title": "Bootstrap nls models in R",
    "section": "Introduction",
    "text": "Introduction\nFor my first academic publication, a reviewer asked for the \\(r^{2}\\) values of the thermal performance curves I fitted using non-linear regression. I bowed to the request as is often the case with reviewer comments, but would now resist as the \\(r^{2}\\) is not necessarily an effective goodness of fit measure for non-linear regression (see this SO answer). It does raise the question of how to determine how well a biologically meaningful model fits the data it is fitted to. I generally just plot every curve to its data, but it tells me nothing of the uncertainty around the curve.\nStep forward the bootstrap! (Non-parametric) bootstrapping is a robust way of computing parameter and model prediction confidence intervals. Bootstrapping involves simulating “new” datasets produced from either the original data (case resampling) or from the original model (residual resampling).\nThe same model is then fitted separately on each individual bootstrapped dataset. Doing this over and over allows us to visualise uncertainty of predictions and produce confidence intervals of estimated parameters. When previously implementing this, I used methods similar to previous blog posts by Andrew MacDonald and Hadley Wickham, as well as a broom vignette.\nHowever, I have since applied a more efficient method using the package car, which contains the function Boot() that provides a wrapper for the widely used function boot::boot() that is tailored to bootstrapping regression models."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#case-resampling-resampling-the-original-data-with-replacement",
    "href": "posts/bootstrap_nls/index.html#case-resampling-resampling-the-original-data-with-replacement",
    "title": "Bootstrap nls models in R",
    "section": "Case resampling: Resampling the original data with replacement",
    "text": "Case resampling: Resampling the original data with replacement\nBootstrapping using case resampling involves simulating “new” datasets produced from the existing data by sampling with replacement.\n\nCase resampling: When it works\nWe will demonstrate an example of when this case resampling approach works using data from a recent paper by Padfield et al. (2020), that measures the thermal performance of the bacteria, Pseudomonas fluorescens, in the presence and absence of its phage, \\(\\phi 2\\). In this study, each single growth rate estimate is a technical replicate. As such, all the data points within each phage treatment can be used to estimate the same curve. The data is in the R package rTPC and we can visualise one of the curves using ggplot2.\n\n# load packages\nlibrary(boot)\nlibrary(car)\nlibrary(rTPC) #remotes::install_github('padpadpadpad/rTPC')\nlibrary(nls.multstart)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(minpack.lm)\n\n# load in data\ndata(\"bacteria_tpc\")\n\n# keep just a single curve\nd <- filter(bacteria_tpc, phage == 'nophage')\n\n# show the data\nggplot(d, aes(temp, rate)) +\n  geom_point(size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n\n\n\nAs in the study, we can fit the Sharpe-Schoolfield model to the data. I take advantage of the packages nls.mulstart and rTPC to do this.\n\n# fit Sharpe-Schoolfield model\nd_fit <- nest(d, data = c(temp, rate)) %>%\n  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = .x,\n                        iter = c(3,3,3,3),\n                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,\n                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,\n                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        supp_errors = 'Y',\n                        convergence_count = FALSE)),\n         # create new temperature data\n         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),\n         # predict over that data,\n         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))\n\n# unnest predictions\nd_preds <- select(d_fit, preds) %>%\n  unnest(preds)\n\n# plot data and predictions\nggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n\n\n\nnls_multstart() is designed to fit models across a wide possible parameter space, but as it samples multiple start parameters for each model, using it with bootstrapping becomes computationally expensive. Instead, we refit the model using minpack.lm::nlsLM(), using the coefficients of nls_multstart() as the start values. The Boot() function then refits the model 999 times and stores the model coefficients.\n\n# refit model using nlsLM\nfit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = d,\n                        start = coef(d_fit$sharpeschoolhigh[[1]]),\n                        lower = get_lower_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                        weights = rep(1, times = nrow(d)))\n\n# bootstrap using case resampling\nboot1 <- Boot(fit_nlsLM, method = 'case')\n\n# look at the data\nhead(boot1$t)\n\n        r_tref         e       eh       th\n[1,] 0.2387067 0.9452492 2.367318 29.24874\n[2,] 0.2511824 0.7655751 2.547971 31.27085\n[3,] 0.3173423 0.7499641 2.081992 29.59478\n[4,] 0.2586626 0.7592931 2.564117 31.18532\n[5,] 0.2871170 0.6627851 2.483048 31.94504\n[6,] 0.2434468 0.8695365 2.432169 30.15619\n\n\nThe parameters of each bootstrapped refit are returned. All methods that are available for boot() and Boot() are supported for these objects. This includes the hist.boot() function which looks at the distribution of each parameter.\n\nhist(boot1, layout = c(2,2))\n\n\n\n\nWe can easily create predictions for each of these models and through this confidence intervals around the original fitted predictions. We can then plot (1) the bootstrapped fits and (2) the confidence regions around the model predictions.\n\n# create predictions of each bootstrapped model\nboot1_preds <- boot1$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d$temp), max(d$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot1_conf_preds <- group_by(boot1_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot1_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot1_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2\n\n\n\n\nThis method works well here, because there are many points beyond the peak of the curve and multiple independent points at each temperature.\n\n\nCase resampling: When it struggles\nThis method becomes more problematic when there is a small sample size and the coverage of temperature values beyond the optimum temperature is small. This means that many of the bootstrapped datasets will not have any points beyond the optimum, which is problematic for mathematical models that expect a unimodal shape. The effect of this can be seen by case resampling a curve from the chlorella_tpc dataset also in rTPC. Here we again fit the model using nls_multstart(), refit the model using nlsLM(), then bootstrap the model using Boot().\n\n# load in chlorella data\ndata('chlorella_tpc') \n\nd2 <- filter(chlorella_tpc, curve_id == 1)\n\n# fit Sharpe-Schoolfield model to raw data\nd_fit <- nest(d2, data = c(temp, rate)) %>%\n  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = .x,\n                        iter = c(3,3,3,3),\n                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,\n                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,\n                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                        supp_errors = 'Y',\n                        convergence_count = FALSE)),\n         # create new temperature data\n         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),\n         # predict over that data,\n         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))\n\n# refit model using nlsLM\nfit_nlsLM2 <- nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                        data = d2,\n                        start = coef(d_fit$sharpeschoolhigh[[1]]),\n                        lower = get_lower_lims(d2$temp, d2$rate, model_name = 'sharpeschoolhigh_1981'),\n                        upper = get_upper_lims(d2$temp, d2$rate, model_name = 'sharpeschoolhigh_1981'),\n                        control = nls.lm.control(maxiter=500),\n                        weights = rep(1, times = nrow(d2)))\n\n# bootstrap using case resampling\nboot2 <- Boot(fit_nlsLM2, method = 'case')\n\n\n Number of bootstraps was 995 out of 999 attempted \n\n\nWe can then create predictions for each bootstrapped model and calculate 95% confidence intervals around the predictions. Models that don’t fit and return NA for the parameter estimates are dropped.\n\n# unnest predictions of original model fit\nd_preds <- select(d_fit, preds) %>%\n  unnest(preds)\n\n# predict over new data\nboot2_preds <- boot2$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d2$temp), max(d2$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot2_conf_preds <- group_by(boot2_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot2_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot2_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2\n\n\n\n\nAs can be seen, bootstrapping-with-replacement with only a single point at each temperature can lead to a large variety of fits. In the second panel, we can see the variation of the curve fits, clustering around 4 possible paths for the decrease in rate beyond the optimum temperature. This occurs because in many instances there are no points sampled at the very high temperatures, leading to this clustering in curve fits."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#residual-resampling",
    "href": "posts/bootstrap_nls/index.html#residual-resampling",
    "title": "Bootstrap nls models in R",
    "section": "Residual resampling",
    "text": "Residual resampling\nCase resampling is the most common way of thinking about bootstrapping. However, bootstrapping ordinary least squares regression models is often done using bootstrapping residuals. This method - where the values of the predictors in a study remain fixed during resampling - is especially useful in a designed experiment where the values of the predictors are set by the experimenter.\nRe-sampling residuals, at its heart, follows a simple set of steps:\n\nFit the model and for each data point, \\(i\\), retain the fitted values \\(\\hat{y_{i}}\\) and the residuals, \\(\\hat{e_{i}} = y_{i} - \\hat{y_{i}}\\)\nFor each data pair, (\\(x_i\\), \\(y_i\\)), where \\(x_i\\) is the measured temperature value, we add a randomly re-sampled residual, \\(\\hat{e}\\) to the fitted value \\(\\hat{y_i}\\). This becomes the new \\(y_i\\) value, such that \\(y_i = \\hat{y_i} + \\hat{e}\\). The new response variable is created based on the random re-allocation of the variation around the original model fit\nThe model is refit using the newly created \\(y_i\\) response variable\nRepeat steps 2 and 3 a number of times\n\nThis method makes the assumption that the original model fit is a good representation of the data, and that the error terms in the model are normally distributed and independent. If the model is incorrectly specified – for example, if there is unmodelled non-linearity, non-constant error variance, or outliers – these characteristics will not carry over into the re-sampled data sets.\ncar::Boot() has an argument that allows us to easily implement residual resampling instead of case resampling, by setting method = 'residual'.\n\n# bootstrap using residual resampling\nboot3 <- Boot(fit_nlsLM2, method = 'residual')\n\n# predict over new data\nboot3_preds <- boot3$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d2$temp), max(d2$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n# calculate bootstrapped confidence intervals\nboot3_conf_preds <- group_by(boot3_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975),\n            .groups = 'drop')\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot3_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot3_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d2, size = 2) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\np1 + p2"
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#calculating-confidence-intervals-of-estimated-parameters",
    "href": "posts/bootstrap_nls/index.html#calculating-confidence-intervals-of-estimated-parameters",
    "title": "Bootstrap nls models in R",
    "section": "Calculating confidence intervals of estimated parameters",
    "text": "Calculating confidence intervals of estimated parameters\nBootstrapping can be used to estimate confidence intervals of the parameters explicitly modelled in the regression. We can compare these approaches to profiled confidence intervals (using confint-MASS) and asymptotic confidence intervals (using nlstools::confint2()). For the bootstrapped parameter distributions, confint.boot() supports the calculation of BCa, basic, normal, and percentile confidence intervals. We use BCa here, and we will calculate all CIs on the two models done previously in this vignette. First with the bacteria TPC.\n\n# First for the bacteria\n\n# get parameters of fitted model\nparam_bact <- broom::tidy(fit_nlsLM) %>%\n  select(param = term, estimate)\n\n# calculate confidence intervals of models\nci_bact1 <- nlstools::confint2(fit_nlsLM, method = 'asymptotic') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'asymptotic')\nci_bact2 <- confint(fit_nlsLM) %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'profile')\n\n# CIs from case resampling\nci_bact3 <- confint(boot1, method = 'bca') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'case bootstrap')\n\n# CIs from residual resampling\nci_bact4 <- Boot(fit_nlsLM, method = 'residual') %>%\n  confint(., method = 'bca') %>%\n  as.data.frame() %>%\n  rename(conf_lower = 1, conf_upper = 2) %>%\n  rownames_to_column(., var = 'param') %>%\n  mutate(method = 'residual bootstrap')\n\nci_bact <- bind_rows(ci_bact1, ci_bact2, ci_bact3, ci_bact4) %>%\n  left_join(., param_bact)\n\n# plot\nggplot(ci_bact, aes(forcats::fct_relevel(method, c('profile', 'asymptotic')), estimate, col = method)) +\n  geom_hline(aes(yintercept = conf_lower), linetype = 2, filter(ci_bact, method == 'profile')) +\n  geom_hline(aes(yintercept = conf_upper), linetype = 2, filter(ci_bact, method == 'profile')) +\n  geom_point(size = 4) +\n  geom_linerange(aes(ymin = conf_lower, ymax = conf_upper)) +\n  theme_bw() +\n  facet_wrap(~param, scales = 'free') +\n  scale_x_discrete('', labels = function(x) stringr::str_wrap(x, width = 10)) +\n  labs(title = 'Calculation of confidence intervals for model parameters',\n       subtitle = 'For the bacteria TPC; dashed lines are CI of profiling method')\n\n\n\n\nEach panel is a different explicitly modelled parameter. The dashed lines represent the 95% intervals for the profiling method. In general, the different bootstrap methods are similar to the profiled intervals, but not all parameters are the same. For example, r_tref and e give wider (and asymmetric) confidence intervals using the case resampling method. The residual method gives estimates that are more similar to those calculated from profiling."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#calculate-confidence-intervals-of-generated-parameters",
    "href": "posts/bootstrap_nls/index.html#calculate-confidence-intervals-of-generated-parameters",
    "title": "Bootstrap nls models in R",
    "section": "Calculate confidence intervals of generated parameters",
    "text": "Calculate confidence intervals of generated parameters\nCrucially, bootstrapping allows the calculation of confidence intervals for parameters derived from the model that were not present in the initial fitting process. For example, the optimum temperature of a thermal performance curve, \\(T_{opt}\\) is calculated as:\n\\[T_{opt} = \\frac{E_{h}T_{h}}{E_{h} + k T_{h} ln(\\frac{E_{h}}{E} - 1)}\\] We can calculate \\(T_{opt}\\) by writing a custom function that we feed into Boot(). We will do this using the case resampling approach for the first curve in this blog post.\n\nget_topt <- function(model){\n  coefs = coef(model)\n  \n  e = coefs[names(coefs) == 'e']\n  eh = coefs[names(coefs) == 'eh']\n  th = coefs[names(coefs) == 'th']\n  \n  return(((eh*(th + 273.15))/(eh + (8.62e-05 *(th + 273.15)*log((eh/e) - 1)))) - 273.15)\n}\n\ntopt <- Boot(fit_nlsLM, f = function(x){get_topt(x)}, labels = 'topt', R = 999, method = 'case')\n\nhist(topt, legend = 'none')\n\n\n\n\nThis approach of using purrr, nls.multstart, and car can easily be scaled up to multiple curves."
  },
  {
    "objectID": "posts/bootstrap_nls/index.html#references",
    "href": "posts/bootstrap_nls/index.html#references",
    "title": "Bootstrap nls models in R",
    "section": "References",
    "text": "References\n\nPadfield, D., Castledine, M., & Buckling, A. (2020). Temperature-dependent changes to host–parasite interactions alter the thermal performance of a bacterial host. The ISME Journal, 14(2), 389-398."
  },
  {
    "objectID": "publications/articles/padfield_2020a.html",
    "href": "publications/articles/padfield_2020a.html",
    "title": "Evolution of diversity explains the impact of pre-adaptation of a focal species on the structure of a natural microbial community",
    "section": "",
    "text": "Padfield, D., Vujakovic, A., Paterson, S., Griffiths, R., Buckling, A., & Hesse, E. (2020). Evolution of diversity explains the impact of pre-adaptation of a focal species on the structure of a natural microbial community. The ISME journal, 14(11), 2877-2889."
  },
  {
    "objectID": "publications/articles/padfield_2020a.html#abstract",
    "href": "publications/articles/padfield_2020a.html#abstract",
    "title": "Evolution of diversity explains the impact of pre-adaptation of a focal species on the structure of a natural microbial community",
    "section": "Abstract",
    "text": "Abstract\nRapid within-species evolution can alter community structure, yet the mechanisms underpinning this effect remain unknown. Populations that rapidly evolve large amounts of phenotypic diversity are likely to interact with more species and have the largest impact on community structure. However, the evolution of phenotypic diversity is, in turn, influenced by the presence of other species. Here, we investigate how microbial community structure changes as a consequence of rapidly evolved within-species diversity using Pseudomonas fluorescens as a focal species. Evolved P. fluorescens populations showed substantial phenotypic diversification in resource-use (and correlated genomic change) irrespective of whether they were pre-adapted in isolation or in a community context. Manipulating diversity revealed that more diverse P. fluorescens populations had the greatest impact on community structure, by suppressing some bacterial taxa, but facilitating others. These findings suggest that conditions that promote the evolution of high within-population diversity should result in a larger impact on community structure."
  },
  {
    "objectID": "publications/articles/hesse_2019.html",
    "href": "publications/articles/hesse_2019.html",
    "title": "Anthropogenic remediation of heavy metals selects against natural microbial remediation",
    "section": "",
    "text": "Hesse, E., Padfield, D., Bayer, F., Van Veen, E. M., Bryan, C. G., & Buckling, A. (2019). Anthropogenic remediation of heavy metals selects against natural microbial remediation. Proceedings of the Royal Society B, 286(1905), 20190804."
  },
  {
    "objectID": "publications/articles/hesse_2019.html#abstract",
    "href": "publications/articles/hesse_2019.html#abstract",
    "title": "Anthropogenic remediation of heavy metals selects against natural microbial remediation",
    "section": "Abstract",
    "text": "Abstract\nIn an era of unprecedented environmental change, there have been increasing ecological and global public health concerns associated with exposure to anthropogenic pollutants. While there is a pressing need to remediate polluted ecosystems, human intervention might unwittingly oppose selection for natural detoxification, which is primarily carried out by microbes. We test this possibility in the context of a ubiquitous chemical remediation strategy aimed at targeting metal pollution: the addition of lime-containing materials. Here, we show that raising pH by liming decreased the availability of toxic metals in acidic mine-degraded soils, but as a consequence selected against microbial taxa that naturally remediate soil through the production of metal-binding siderophores. Our results therefore highlight the crucial need to consider the eco-evolutionary consequences of human environmental strategies on microbial ecosystem services and other traits."
  },
  {
    "objectID": "publications/articles/castledine_2019.html",
    "href": "publications/articles/castledine_2019.html",
    "title": "A shared coevolutionary history does not alter the outcome of coalescence in experimental populations of Pseudomonas fluorescens",
    "section": "",
    "text": "Castledine, M., Buckling, A., & Padfield, D. (2019). A shared coevolutionary history does not alter the outcome of coalescence in experimental populations of Pseudomonas fluorescens. Journal of evolutionary biology, 32(1), 58-65."
  },
  {
    "objectID": "publications/articles/castledine_2019.html#abstract",
    "href": "publications/articles/castledine_2019.html#abstract",
    "title": "A shared coevolutionary history does not alter the outcome of coalescence in experimental populations of Pseudomonas fluorescens",
    "section": "Abstract",
    "text": "Abstract\nCommunity coalescence, the mixing of multiple communities, is ubiquitous in natural microbial communities. During coalescence, theory suggests the success of a population will be enhanced by the presence of species it has coevolved with (relative to foreign species), because coevolution will result in greater resource specialization to minimize competition. Thus, more coevolved communities should dominate over less coevolved communities during coalescence events. We test these hypotheses using the bacterium Pseudomonas fluorescens which diversifies into coexisting niche-specialist morphotypes. We first evolved replicate populations for ~40 generations and then isolated evolved genotypes. In a series of competition trials, we determined if using coevolved versus random genotypes affected the relative performance of “communities” of single and multiple genotypes. We found no effect of coevolutionary history on either genotype fitness or community performance, which suggests parallel (co)evolution between communities. However, fitness was enhanced by the presence of other genotypes of the same strain type (wild-type or an isogenic strain with a LacZ marker; the inclusion of the latter necessary to distinguish genotypes during competition), indicative of local adaptation with respect to genetic background. Our results are the first to investigate the effect of (co)evolution on the outcome of coalescence and suggest that when input populations are functionally similar and added at equal mixing ratios, the outcome community may not be asymmetrically dominated by either input population."
  },
  {
    "objectID": "publications/articles/padfield_2020b.html",
    "href": "publications/articles/padfield_2020b.html",
    "title": "Temperature-dependent changes to host–parasite interactions alter the thermal performance of a bacterial host",
    "section": "",
    "text": "Padfield, D., Castledine, M., & Buckling, A. (2020). Temperature-dependent changes to host–parasite interactions alter the thermal performance of a bacterial host. The ISME Journal, 14(2), 389-398."
  },
  {
    "objectID": "publications/articles/padfield_2020b.html#abstract",
    "href": "publications/articles/padfield_2020b.html#abstract",
    "title": "Temperature-dependent changes to host–parasite interactions alter the thermal performance of a bacterial host",
    "section": "Abstract",
    "text": "Abstract\nThermal performance curves (TPCs) are used to predict changes in species interactions, and hence, range shifts, disease dynamics and community composition, under forecasted climate change. Species interactions might in turn affect TPCs. Here, we investigate how temperature-dependent changes in a microbial host–parasite interaction (the bacterium Pseudomonas fluorescens, and its lytic bacteriophage, SBWΦ2) changes the host TPC and the ecological and evolutionary mechanisms underlying these changes. The bacteriophage had a narrower thermal tolerance for infection, with their critical thermal maximum ~6 ºC lower than those at which the bacteria still had high growth. Consequently, in the presence of phage, the host TPC changed, resulting in a lower maximum growth rate. These changes were not just driven by differences in thermal tolerance, with temperature-dependent costs of evolved resistance also playing a major role: the largest cost of resistance occurred at the temperature at which bacteria grew best in the absence of phage. Our work highlights how ecological and evolutionary mechanisms can alter the effect of a parasite on host thermal performance, even over very short timescales."
  },
  {
    "objectID": "publications/articles/castledine_2022a.html",
    "href": "publications/articles/castledine_2022a.html",
    "title": "Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro",
    "section": "",
    "text": "Castledine, M., Padfield, D., Sierocinski, P., Pascual, J. S., Hughes, A., Mäkinen, L., Friman, V.P., Pirnay, J.P., Merabishvili, M., De Vos, D., & Buckling, A. (2022). Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro. Elife, 11, e73679."
  },
  {
    "objectID": "publications/articles/castledine_2022a.html#abstract",
    "href": "publications/articles/castledine_2022a.html#abstract",
    "title": "Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro",
    "section": "Abstract",
    "text": "Abstract\nWith rising antibiotic resistance, there has been increasing interest in treating pathogenic bacteria with bacteriophages (phage therapy). One limitation of phage therapy is the ease at which bacteria can evolve resistance. Negative effects of resistance may be mitigated when resistance results in reduced bacterial growth and virulence, or when phage coevolves to overcome resistance. Resistance evolution and its consequences are contingent on the bacteria-phage combination and their environmental context, making therapeutic outcomes hard to predict. One solution might be to conduct ‘in vitro evolutionary simulations’ using bacteria-phage combinations from the therapeutic context. Overall, our aim was to investigate parallels between in vitro experiments and in vivo dynamics in a human participant. Evolutionary dynamics were similar, with high levels of resistance evolving quickly with limited evidence of phage evolution. Resistant bacteria—evolved in vitro and in vivo—had lower virulence. In vivo, this was linked to lower growth rates of resistant isolates, whereas in vitro phage resistant isolates evolved greater biofilm production. Population sequencing suggests resistance resulted from selection on de novo mutations rather than sorting of existing variants. These results highlight the speed at which phage resistance can evolve in vivo, and how in vitro experiments may give useful insights for clinical evolutionary outcomes."
  },
  {
    "objectID": "publications/articles/bartlett_2022.html",
    "href": "publications/articles/bartlett_2022.html",
    "title": "A comprehensive list of bacterial pathogens infecting humans",
    "section": "",
    "text": "Bartlett, A., Padfield, D., Lear, L., Bendall, R., & Vos, M. (2022). A comprehensive list of bacterial pathogens infecting humans. Microbiology, 168(12), 001269."
  },
  {
    "objectID": "publications/articles/bartlett_2022.html#abstract",
    "href": "publications/articles/bartlett_2022.html#abstract",
    "title": "A comprehensive list of bacterial pathogens infecting humans",
    "section": "Abstract",
    "text": "Abstract\nThere exists an enormous diversity of bacteria capable of human infection, but no up-to-date, publicly accessible list is available. Combining a pragmatic definition of pathogenicity with an extensive search strategy, we report 1513 bacterial pathogens known to infect humans described pre-2021. Of these, 73 % were regarded as established (have infected at least three persons in three or more references) and 27 % as putative (fewer than three known cases). Pathogen species belong to 10 phyla and 24 classes scattered throughout the bacterial phylogeny. We show that new human pathogens are discovered at a rapid rate. Finally, we discuss how our results could be expanded to a database, which could provide a useful resource for microbiologists. Our list is freely available and archived on GitHub and Zenodo and we have provided walkthroughs to facilitate access and use."
  },
  {
    "objectID": "publications/articles/castledine_2022b.html",
    "href": "publications/articles/castledine_2022b.html",
    "title": "Greater phage genotypic diversity constrains arms-race coevolution",
    "section": "",
    "text": "Castledine, M., Sierocinski, P., Inglis, M., Kay, S., Hayward, A., Buckling, A., & Padfield, D. (2022). Greater phage genotypic diversity constrains arms-race coevolution. Frontiers in Cellular and Infection Microbiology, 168."
  },
  {
    "objectID": "publications/articles/castledine_2022b.html#abstract",
    "href": "publications/articles/castledine_2022b.html#abstract",
    "title": "Greater phage genotypic diversity constrains arms-race coevolution",
    "section": "Abstract",
    "text": "Abstract\nAntagonistic coevolution between hosts and parasites, the reciprocal evolution of host resistance and parasite infectivity, has important implications in ecology and evolution. The dynamics of coevolution – notably whether host or parasite has an evolutionary advantage – is greatly affected by the relative amount of genetic variation in host resistance and parasite infectivity traits. While studies have manipulated genetic diversity during coevolution, such as by increasing mutation rates, it is unclear how starting genetic diversity affects host-parasite coevolution. Here, we (co)evolved the bacterium Pseudomonas fluorescens SBW25 and two bacteriophage genotypes of its lytic phage SBW25ɸ2 in isolation (one phage genotype) and together (two phage genotypes). Bacterial populations rapidly evolved phage resistance and phage reciprocally increased their infectivity in response. When phage populations were evolved with bacteria in isolation, bacterial resistance and phage infectivity increased through time, indicative of arms-race coevolution. In contrast, when both phage genotypes were together, bacteria did not increase their resistance in response to increasing phage infectivity. This was likely due to bacteria being unable to evolve resistance to both phage via the same mutations. These results suggest that increasing initial parasite genotypic diversity can give parasites an evolutionary advantage that arrests long-term coevolution. This study has important implications for the applied use of phage in phage therapy and in understanding host-parasite dynamics in broader ecological and evolutionary theory."
  },
  {
    "objectID": "publications/articles/silk_2021.html",
    "href": "publications/articles/silk_2021.html",
    "title": "CMRnet: An R package to derive networks of social interactions and movement from mark–recapture data",
    "section": "",
    "text": "Silk, M. J., McDonald, R. A., Delahay, R. J., Padfield, D., & Hodgson, D. J. (2021). CMRnet: An r package to derive networks of social interactions and movement from mark–recapture data. Methods in Ecology and Evolution, 12(1), 70-75."
  },
  {
    "objectID": "publications/articles/silk_2021.html#abstract",
    "href": "publications/articles/silk_2021.html#abstract",
    "title": "CMRnet: An R package to derive networks of social interactions and movement from mark–recapture data",
    "section": "Abstract",
    "text": "Abstract\n\nLong-term capture–mark–recapture data provide valuable information on the movements of individuals between locations, and the contemporary and/or co-located captures of individuals can be used to approximate the social structure of populations.\nWe introduce an r package (CMRnet) that generates social and movement networks from spatially explicit capture–mark–recapture data. It also provides functions for network and datastream permutations for these networks. Here we describe the package and key considerations for its application, providing two example case studies.\nThe conversion of spatially explicit mark–recapture data into social and movement networks will provide insights into the interplay between demography and behaviour in wild animal populations, with important applications in their management and conservation."
  },
  {
    "objectID": "publications/articles/mcnicol_2020.html",
    "href": "publications/articles/mcnicol_2020.html",
    "title": "Postrelease movement and habitat selection of translocated pine martens Martes martes",
    "section": "",
    "text": "McNicol, C. M., Bavin, D., Bearhop, S., Bridges, J., Croose, E., Gill, R., … & McDonald, R. A. (2020). Postrelease movement and habitat selection of translocated pine martens Martes martes. Ecology and evolution, 10(11), 5106-5118."
  },
  {
    "objectID": "publications/articles/mcnicol_2020.html#abstract",
    "href": "publications/articles/mcnicol_2020.html#abstract",
    "title": "Postrelease movement and habitat selection of translocated pine martens Martes martes",
    "section": "Abstract",
    "text": "Abstract\nMonitoring postrelease establishment and movement of animals is important in evaluating conservation translocations. We translocated 39 wild pine martens Martes martes (19 females, 20 males) from Scotland to Wales. We released them into forested areas with no conspecifics in 2015, followed by a second release in 2016, alongside the previously released animals. We used radio-tracking to describe postrelease movement and habitat selection. Six martens (15%) were not re-encountered during the tracking period, of which four undertook long-distance dispersal. For the remaining individuals, we characterized two phases of movement, “exploration” followed by “settlement,” that differed between releases. In the first release, martens remained in exploration phase for a mean of 14.5 days (SE = 3.9 days) and settled at a mean distance of 8.7 km (SE = 1.8 km) from release sites, whereas martens released in year two, alongside resident conspecifics, traveled away from release sites at a faster rate, settling sooner, at a mean of 6.6 days (SE = 1.8 days), but further, at a mean distance of 14.0 km (SE = 1.7 km) from release sites. Animals released in year one did not exhibit habitat preferences overall but within forests they favored recently felled areas, whereas animals released in year two showed strong selection for forested habitat but did not discriminate between forest types. The presence of conspecifics appeared influential for settlement and site fidelity of translocated martens and was associated with more rapid but more distant dispersal of the later cohort. Releases of animals in close proximity appeared to promote site fidelity and rapid establishment of ranges in the recipient environment."
  },
  {
    "objectID": "publications/articles/padfield_2017.html",
    "href": "publications/articles/padfield_2017.html",
    "title": "Metabolic compensation constrains the temperature dependence of gross primary production",
    "section": "",
    "text": "Padfield, D., Lowe, C., Buckling, A., Ffrench‐Constant, R., Student Research Team, Jennings, S., … & Yvon‐Durocher, G. (2017). Metabolic compensation constrains the temperature dependence of gross primary production. Ecology Letters, 20(10), 1250-1260."
  },
  {
    "objectID": "publications/articles/padfield_2017.html#abstract",
    "href": "publications/articles/padfield_2017.html#abstract",
    "title": "Metabolic compensation constrains the temperature dependence of gross primary production",
    "section": "Abstract",
    "text": "Abstract\nGross primary production (GPP) is the largest flux in the carbon cycle, yet its response to global warming is highly uncertain. The temperature dependence of GPP is directly linked to photosynthetic physiology, but the response of GPP to warming over longer timescales could also be shaped by ecological and evolutionary processes that drive variation in community structure and functional trait distributions. Here, we show that selection on photosynthetic traits within and across taxa dampens the effects of temperature on GPP across a catchment of geothermally heated streams. Autotrophs from cold streams had higher photosynthetic rates and after accounting for differences in biomass among sites, biomass-specific GPP was independent of temperature in spite of a 20 °C thermal gradient. Our results suggest that temperature compensation of photosynthetic rates constrains the long-term temperature dependence of GPP, and highlights the importance of considering physiological, ecological and evolutionary mechanisms when predicting how ecosystem-level processes respond to warming."
  },
  {
    "objectID": "publications/articles/padfield_2016.html",
    "href": "publications/articles/padfield_2016.html",
    "title": "Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton",
    "section": "",
    "text": "Padfield, D., Yvon‐Durocher, G., Buckling, A., Jennings, S., & Yvon‐Durocher, G. (2016). Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton. Ecology letters, 19(2), 133-142."
  },
  {
    "objectID": "publications/articles/padfield_2016.html#abstract",
    "href": "publications/articles/padfield_2016.html#abstract",
    "title": "Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton",
    "section": "Abstract",
    "text": "Abstract\nUnderstanding the mechanisms that determine how phytoplankton adapt to warming will substantially improve the realism of models describing ecological and biogeochemical effects of climate change. Here, we quantify the evolution of elevated thermal tolerance in the phytoplankton, Chlorella vulgaris. Initially, population growth was limited at higher temperatures because respiration was more sensitive to temperature than photosynthesis meaning less carbon was available for growth. Tolerance to high temperature evolved after ≈ 100 generations via greater down-regulation of respiration relative to photosynthesis. By down-regulating respiration, phytoplankton overcame the metabolic constraint imposed by the greater temperature sensitivity of respiration and more efficiently allocated fixed carbon to growth. Rapid evolution of carbon-use efficiency provides a potentially general mechanism for thermal adaptation in phytoplankton and implies that evolutionary responses in phytoplankton will modify biogeochemical cycles and hence food web structure and function under warming. Models of climate futures that ignore adaptation would usefully be revisited."
  },
  {
    "objectID": "publications/articles/barneche_2021.html",
    "href": "publications/articles/barneche_2021.html",
    "title": "Warming impairs trophic transfer efficiency in a long-term field experiment",
    "section": "",
    "text": "Barneche, D. R., Hulatt, C. J., Dossena, M., Padfield, D., Woodward, G., Trimmer, M., & Yvon-Durocher, G. (2021). Warming impairs trophic transfer efficiency in a long-term field experiment. Nature, 592(7852), 76-79."
  },
  {
    "objectID": "publications/articles/barneche_2021.html#abstract",
    "href": "publications/articles/barneche_2021.html#abstract",
    "title": "Warming impairs trophic transfer efficiency in a long-term field experiment",
    "section": "Abstract",
    "text": "Abstract\nIn ecosystems, the efficiency of energy transfer from resources to consumers determines the biomass structure of food webs. As a general rule, about 10% of the energy produced in one trophic level makes it up to the next1,2,3. Recent theory suggests that this energy transfer could be further constrained if rising temperatures increase metabolic growth costs4, although experimental confirmation in whole ecosystems is lacking. Here we quantify nitrogen transfer efficiency—a proxy for overall energy transfer—in freshwater plankton in artificial ponds that have been exposed to seven years of experimental warming. We provide direct experimental evidence that, relative to ambient conditions, 4 °C of warming can decrease trophic transfer efficiency by up to 56%. In addition, the biomass of both phytoplankton and zooplankton was lower in the warmed ponds, which indicates major shifts in energy uptake, transformation and transfer5,6. These findings reconcile observed warming-driven changes in individual-level growth costs and in carbon-use efficiency across diverse taxa4,7,8,9,10 with increases in the ratio of total respiration to gross primary production at the ecosystem level11,12,13. Our results imply that an increasing proportion of the carbon fixed by photosynthesis will be lost to the atmosphere as the planet warms, impairing energy flux through food chains, which will have negative implications for larger consumers and for the functioning of entire ecosystems."
  },
  {
    "objectID": "publications/articles/lear_2022a.html",
    "href": "publications/articles/lear_2022a.html",
    "title": "Disturbance‐mediated invasions are dependent on community resource abundance",
    "section": "",
    "text": "Lear, L., Padfield, D., Inamine, H., Shea, K., & Buckling, A. (2022). Disturbance‐mediated invasions are dependent on community resource abundance. Ecology, 103(8), e3728."
  },
  {
    "objectID": "publications/articles/lear_2022a.html#abstract",
    "href": "publications/articles/lear_2022a.html#abstract",
    "title": "Disturbance‐mediated invasions are dependent on community resource abundance",
    "section": "Abstract",
    "text": "Abstract\nDisturbances can facilitate biological invasions, with the associated increase in resource availability being a proposed cause. Here, we experimentally tested the interactive effects of disturbance regime (different frequencies of biomass removal at equal intensities) and resource abundance on invasion success using a factorial design containing five disturbance frequencies and three resource levels. We invaded populations of the bacterium Pseudomonas fluorescens with two ecologically different invader morphotypes: a fast-growing “colonizer” type and a slower growing “competitor” type. As resident populations were altered by the treatments, we additionally tested their effect on invader success. Disturbance frequency and resource abundance interacted to affect the success of both invaders, but this interaction differed between the invader types. The success of the colonizer type was positively affected by disturbance under high resources but negatively under low. However, disturbance negatively affected the success of the competitor type under high resource abundance but not under low or medium. Resident population changes did not alter invader success beyond direct treatment effects. We therefore demonstrate that the same disturbance regime can either be beneficial or detrimental for an invader depending on both community resource abundance and its life history. These results may help to explain some of the inconsistencies found in the disturbance-invasion literature."
  },
  {
    "objectID": "publications/articles/padfield_2021.html",
    "href": "publications/articles/padfield_2021.html",
    "title": "rTPC and nls.multstart: A new pipeline to fit thermal performance curves in R",
    "section": "",
    "text": "Padfield, D., O’Sullivan, H., & Pawar, S. (2021). rTPC and nls. multstart: a new pipeline to fit thermal performance curves in R. Methods in Ecology and Evolution, 12(6), 1138-1143."
  },
  {
    "objectID": "publications/articles/padfield_2021.html#abstract",
    "href": "publications/articles/padfield_2021.html#abstract",
    "title": "rTPC and nls.multstart: A new pipeline to fit thermal performance curves in R",
    "section": "Abstract",
    "text": "Abstract\n\nQuantifying thermal performance curves (TPCs) for biological rates has many applications to important problems such as predicting responses of biological systems—from individuals to communities—to directional climate change or climatic fluctuations.\nCurrent software tools for fitting TPC models to data are not adequate for dealing with the immense size of new datasets that are increasingly becoming available. We require tools capable of tackling this issue in a simple, reproducible and accessible way.\nWe present a new, reproducible pipeline in r that allows for relatively simple fitting of 24 different TPC models using nonlinear least squares (NLLS) regression. The pipeline consists of two packages—rTPC and nls.multstart—that provide functions which conveniently address common problems with NLLS fitting such as the NLLS parameter starting values problem. rTPC also includes functions to set starting values, estimate key TPC parameters and calculate uncertainty around parameter estimates as well as the fitted model as a whole.\nWe demonstrate how this pipeline can be combined with other packages in r to robustly and reproducibly fit multiple mathematical models to multiple TPC datasets at once. In addition, we show how model selection or averaging, weighted model fitting and bootstrapping can be easily implemented within the pipeline.\nThis new pipeline provides a flexible and reproducible approach that makes the challenging task of fitting multiple TPC models to data accessible to a wide range of users across ecology and evolution."
  },
  {
    "objectID": "publications/articles/sierocinski_2021.html",
    "href": "publications/articles/sierocinski_2021.html",
    "title": "The impact of propagule pressure on whole community invasions in biomethane-producing communities",
    "section": "",
    "text": "Sierocinski, P., Pascual, J. S., Padfield, D., Salter, M., & Buckling, A. (2021). The impact of propagule pressure on whole community invasions in biomethane-producing communities. Iscience, 24(6), 102659."
  },
  {
    "objectID": "publications/articles/sierocinski_2021.html#abstract",
    "href": "publications/articles/sierocinski_2021.html#abstract",
    "title": "The impact of propagule pressure on whole community invasions in biomethane-producing communities",
    "section": "Abstract",
    "text": "Abstract\nMicrobes can invade as whole communities, but the ecology of whole community invasions is poorly understood. Here, we investigate how invader propagule pressure (the number of invading organisms) affects the composition and function of invaded laboratory methanogenic communities. An invading community was equally successful at establishing itself in a resident community regardless of propagule pressure, which varied between 0.01 and 10% of the size resident community. Invasion resulted in enhanced biogas production (to the level of the pure invading community) but only when propagule pressure was 1% or greater. This inconsistency between invasion success and changes in function can be explained by a lower richness of invading taxa at lower propagule pressures, and an important functional role of the taxa that were absent. Our results highlight that whole community invasion ecology cannot simply be extrapolated from our understanding of single species invasions. Moreover, we show that methane production can be enhanced by invading poorly performing reactors with a better performing community at levels that may be practical in industrial settings."
  },
  {
    "objectID": "publications/articles/sierocinski_2021.html#graphical-abstract",
    "href": "publications/articles/sierocinski_2021.html#graphical-abstract",
    "title": "The impact of propagule pressure on whole community invasions in biomethane-producing communities",
    "section": "Graphical Abstract",
    "text": "Graphical Abstract"
  },
  {
    "objectID": "publications/articles/swan_2022.html",
    "href": "publications/articles/swan_2022.html",
    "title": "Associations between abundances of free-roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season",
    "section": "",
    "text": "Swan, G. J., Bearhop, S., Redpath, S. M., Silk, M. J., Padfield, D., Goodwin, C. E., & McDonald, R. A. (2022). Associations between abundances of free‐roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season. Ecology and Evolution, 12(5), e8877."
  },
  {
    "objectID": "publications/articles/swan_2022.html#abstract",
    "href": "publications/articles/swan_2022.html#abstract",
    "title": "Associations between abundances of free-roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season",
    "section": "Abstract",
    "text": "Abstract\nReleasing gamebirds in large numbers for sport shooting may directly or indirectly influence the abundance, distribution and population dynamics of native wildlife. The abundances of generalist predators have been positively associated with the abundance of gamebirds. These relationships have implications for prey populations, with the potential for indirect impacts of gamebird releases on wider biodiversity. To understand the basis of these associations, we investigated variation in territory size, prey provisioning to chicks, and breeding success of common buzzards Buteo buteo, and associations with variation in the abundances of free-roaming gamebirds, primarily pheasants Phasianus colchicus, and of rabbits Oryctolagus cuniculus and field voles Microtus agrestis, as important prey for buzzards. The relative abundance of gamebirds, but not those of rabbits or voles, was weakly but positively correlated with our index of buzzard territory size. Gamebirds were rarely brought to the nest. Rabbits and voles, and not gamebirds, were provisioned to chicks in proportion to their relative abundance. The number of buzzard chicks increased with provisioning rates of rabbits, in terms of both provisioning frequency and biomass, but not with provisioning rates for gamebirds or voles. Associations between the abundances of buzzards and gamebirds may not be a consequence of the greater availability of gamebirds as prey during the buzzard breeding season. Instead, the association may arise either from habitat or predator management leading to higher densities of alternative prey (in this instance, rabbits), or from greater availability of gamebirds as prey or carrion during the autumn and winter shooting season. The interactions between gamebird releases and associated practices of predator control and shooting itself require better understanding to more effectively intervene in any one aspect of this complex social-ecological system."
  },
  {
    "objectID": "publications/articles/lear_2022.html",
    "href": "publications/articles/lear_2022.html",
    "title": "Bacterial colonisation dynamics of household plastics in a coastal environment",
    "section": "",
    "text": "Lear, L., Padfield, D., Dowsett, T., Jones, M., Kay, S., Hayward, A., & Vos, M. (2022). Bacterial colonisation dynamics of household plastics in a coastal environment. Science of the Total Environment, 838, 156199."
  },
  {
    "objectID": "publications/articles/lear_2022.html#abstract",
    "href": "publications/articles/lear_2022.html#abstract",
    "title": "Bacterial colonisation dynamics of household plastics in a coastal environment",
    "section": "Abstract",
    "text": "Abstract\nAccumulation of plastics in the marine environment has widespread detrimental consequences for ecosystems and wildlife. Marine plastics are rapidly colonised by a wide diversity of bacteria, including human pathogens, posing potential risks to health. Here, we investigate the effect of polymer type, residence time and estuarine location on bacterial colonisation of common household plastics, including pathogenic bacteria. We submerged five main household plastic types: low-density PE (LDPE), high-density PE (HDPE), polypropylene (PP), polyvinyl chloride (PVC) and polyethylene terephthalate (PET) at an estuarine site in Cornwall (U.K.) and tracked bacterial colonisation dynamics. Using both culture-dependent and culture-independent approaches, we found that bacteria rapidly colonised plastics irrespective of polymer type, reaching culturable densities of up to 1000 cells cm3 after 7 weeks. Community composition of the biofilms changed over time, but not among polymer types. The presence of pathogenic bacteria, quantified using the insect model Galleria mellonella, increased dramatically over a five-week period, with Galleria mortality increasing from 4% in week one to 65% in week five. No consistent differences in virulence were observed between polymer types. Pathogens isolated from plastic biofilms using Galleria enrichment included Serratia and Enterococcus species and they harboured a wide range of antimicrobial resistance genes. Our findings show that plastics in coastal waters are rapidly colonised by a wide diversity of bacteria independent of polymer type. Further, our results show that marine plastic biofilms become increasingly associated with virulent bacteria over time."
  },
  {
    "objectID": "publications/articles/lear_2022.html#graphical-abstract",
    "href": "publications/articles/lear_2022.html#graphical-abstract",
    "title": "Bacterial colonisation dynamics of household plastics in a coastal environment",
    "section": "Graphical Abstract",
    "text": "Graphical Abstract"
  },
  {
    "objectID": "publications/articles/vanhoute_2021.html",
    "href": "publications/articles/vanhoute_2021.html",
    "title": "Compost spatial heterogeneity promotes evolutionary diversification of a bacterium",
    "section": "",
    "text": "van Houte, S., Padfield, D., Gómez, P., Luján, A. M., Brockhurst, M. A., Paterson, S., & Buckling, A. (2021). Compost spatial heterogeneity promotes evolutionary diversification of a bacterium. Journal of Evolutionary Biology, 34(2), 246-255."
  },
  {
    "objectID": "publications/articles/vanhoute_2021.html#abstract",
    "href": "publications/articles/vanhoute_2021.html#abstract",
    "title": "Compost spatial heterogeneity promotes evolutionary diversification of a bacterium",
    "section": "Abstract",
    "text": "Abstract\nSpatial resource heterogeneity is expected to be a key driver for the evolution of diversity. However, direct empirical support for this prediction is limited to studies carried out in simplified laboratory environments. Here, we investigate how altering spatial heterogeneity of potting compost—by the addition of water and mixing—affects the evolutionary diversification of a bacterial species, Pseudomonas fluorescens, that is naturally found in the environment. There was a greater propensity of resource specialists to evolve in the unmanipulated compost, while more generalist phenotypes dominated the compost–water mix. Genomic data were consistent with these phenotypic findings. Competition experiments strongly suggest these results are due to diversifying selection as a result of resource heterogeneity, as opposed to other covariables. Overall, our findings corroborate theoretical and in vitro findings, but in semi-natural, more realistic conditions."
  },
  {
    "objectID": "publications/articles/padfield_2018.html",
    "href": "publications/articles/padfield_2018.html",
    "title": "Linking phytoplankton community metabolism to the individual size distribution",
    "section": "",
    "text": "Padfield, D., Buckling, A., Warfield, R., Lowe, C., & Yvon‐Durocher, G. (2018). Linking phytoplankton community metabolism to the individual size distribution. Ecology Letters, 21(8), 1152-1161."
  },
  {
    "objectID": "publications/articles/padfield_2018.html#abstract",
    "href": "publications/articles/padfield_2018.html#abstract",
    "title": "Linking phytoplankton community metabolism to the individual size distribution",
    "section": "Abstract",
    "text": "Abstract\nQuantifying variation in ecosystem metabolism is critical to predicting the impacts of environmental change on the carbon cycle. We used a metabolic scaling framework to investigate how body size and temperature influence phytoplankton community metabolism. We tested this framework using phytoplankton sampled from an outdoor mesocosm experiment, where communities had been either experimentally warmed (+ 4 °C) for 10 years or left at ambient temperature. Warmed and ambient phytoplankton communities differed substantially in their taxonomic composition and size structure. Despite this, the response of primary production and community respiration to long- and short-term warming could be estimated using a model that accounted for the size- and temperature dependence of individual metabolism, and the community abundance-body size distribution. This work demonstrates that the key metabolic fluxes that determine the carbon balance of planktonic ecosystems can be approximated using metabolic scaling theory, with knowledge of the individual size distribution and environmental temperature."
  },
  {
    "objectID": "publications/articles/schaum_2018.html",
    "href": "publications/articles/schaum_2018.html",
    "title": "Temperature‐driven selection on metabolic traits increases the strength of an algal–grazer interaction in naturally warmed streams",
    "section": "",
    "text": "Schaum, C. E., Student Research Team, ffrench‐Constant, R., Lowe, C., Ólafsson, J. S., Padfield, D., … & Wrigglesworth, E. (2018). Temperature‐driven selection on metabolic traits increases the strength of an algal–grazer interaction in naturally warmed streams. Global Change Biology, 24(4), 1793-1803."
  },
  {
    "objectID": "publications/articles/schaum_2018.html#abstract",
    "href": "publications/articles/schaum_2018.html#abstract",
    "title": "Temperature‐driven selection on metabolic traits increases the strength of an algal–grazer interaction in naturally warmed streams",
    "section": "Abstract",
    "text": "Abstract"
  },
  {
    "objectID": "publications/articles/garciacarreras_2018.html",
    "href": "publications/articles/garciacarreras_2018.html",
    "title": "Role of carbon allocation efficiency in the temperature dependence of autotroph growth rates",
    "section": "",
    "text": "García-Carreras, B., Sal, S., Padfield, D., Kontopoulos, D. G., Bestion, E., Schaum, C. E., … & Pawar, S. (2018). Role of carbon allocation efficiency in the temperature dependence of autotroph growth rates. Proceedings of the National Academy of Sciences, 115(31), E7361-E7368."
  },
  {
    "objectID": "publications/articles/garciacarreras_2018.html#abstract",
    "href": "publications/articles/garciacarreras_2018.html#abstract",
    "title": "Role of carbon allocation efficiency in the temperature dependence of autotroph growth rates",
    "section": "Abstract",
    "text": "Abstract\nRelating the temperature dependence of photosynthetic biomass production to underlying metabolic rates in autotrophs is crucial for predicting the effects of climatic temperature fluctuations on the carbon balance of ecosystems. We present a mathematical model that links thermal performance curves (TPCs) of photosynthesis, respiration, and carbon allocation efficiency to the exponential growth rate of a population of photosynthetic autotroph cells. Using experiments with the green alga, Chlorella vulgaris, we apply the model to show that the temperature dependence of carbon allocation efficiency is key to understanding responses of growth rates to warming at both ecological and longer-term evolutionary timescales. Finally, we assemble a dataset of multiple terrestrial and aquatic autotroph species to show that the effects of temperature-dependent carbon allocation efficiency on potential growth rate TPCs are expected to be consistent across taxa. In particular, both the thermal sensitivity and the optimal temperature of growth rates are expected to change significantly due to temperature dependence of carbon allocation efficiency alone. Our study provides a foundation for understanding how the temperature dependence of carbon allocation determines how population growth rates respond to temperature."
  },
  {
    "objectID": "publications/articles/castledine_2020a.html",
    "href": "publications/articles/castledine_2020a.html",
    "title": "Experimental (co)evolution in a multi‐species microbial community results in local maladaptation",
    "section": "",
    "text": "Castledine, M., Padfield, D., & Buckling, A. (2020). Experimental (co) evolution in a multi‐species microbial community results in local maladaptation. Ecology Letters, 23(11), 1673-1681."
  },
  {
    "objectID": "publications/articles/castledine_2020a.html#abstract",
    "href": "publications/articles/castledine_2020a.html#abstract",
    "title": "Experimental (co)evolution in a multi‐species microbial community results in local maladaptation",
    "section": "Abstract",
    "text": "Abstract\nInterspecific coevolutionary interactions can result in rapid biotic adaptation, but most studies have focused only on species pairs. Here, we (co)evolved five microbial species in replicate polycultures and monocultures and quantified local adaptation. Specifically, growth rate assays were used to determine adaptations of each species’ populations to (1) the presence of the other four species in general and (2) sympatric vs. allopatric communities. We found that species did not show an increase in net biotic adaptation:ancestral, polyculture- and monoculture-evolved populations did not have significantly different growth rates within communities. However, 4/5 species’ growth rates were significantly lower within the community they evolved in relative to an allopatric community. ‘Local maladaptation’ suggests that species evolved increased competitive interactions to sympatric species’ populations. This increased competition did not affect community stability or productivity. Our results suggest that (co)evolution within communities can increase competitive interactions that are specific to (co)evolved community members."
  },
  {
    "objectID": "publications/articles/castledine_2020b.html",
    "href": "publications/articles/castledine_2020b.html",
    "title": "Community coalescence: an eco-evolutionary perspective",
    "section": "",
    "text": "Castledine, M., Sierocinski, P., Padfield, D., & Buckling, A. (2020). Community coalescence: an eco-evolutionary perspective. Philosophical Transactions of the Royal Society B, 375(1798), 20190252."
  },
  {
    "objectID": "publications/articles/castledine_2020b.html#abstract",
    "href": "publications/articles/castledine_2020b.html#abstract",
    "title": "Community coalescence: an eco-evolutionary perspective",
    "section": "Abstract",
    "text": "Abstract\nCommunity coalescence, the mixing of different communities, is widespread throughout microbial ecology. Coalescence can result in approximately equal contributions from the founding communities or dominance of one community over another. These different outcomes have ramifications for community structure and function in natural communities, and the use of microbial communities in biotechnology and medicine. However, we have little understanding of when a particular outcome might be expected. Here, we integrate existing theory and data to speculate on how a crucial characteristic of microbial communities—the type of species interaction that dominates the community—might affect the outcome of microbial community coalescence. Given the often comparable timescales of microbial ecology and microevolution, we explicitly consider ecological and evolutionary dynamics, and their interplay, in determining coalescence outcomes."
  },
  {
    "objectID": "article_template.html#abstract",
    "href": "article_template.html#abstract",
    "title": "DanPadLab",
    "section": "Abstract",
    "text": "Abstract"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DanPadLab’s website",
    "section": "",
    "text": "I am Daniel Padfield and I know this is a terrible name for a research group but it will do for now. My group is based at the University of Exeter (at the Cornwall campus by the sea) and we are broadly interested in understanding how microbial communities respond to environmental change. To do this, we use a variety of sequencing, experimental, and modelling approaches.\nWe are also committed to reproducible and open science. All of the code and data from our projects are freely available online through GitHub and archived on Zenodo.\nThe aim of this website is to allow us to document our work and to provide a space to share useful code and walk-throughs. I also hope to use this as an outlet to document my academic journey as I attempt to manage being an academic alongside being a long-time carer."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "DanPadLab",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n  \n    A comprehensive list of bacterial pathogens infecting humans\n    Abigail Bartlett, Daniel Padfield, Luke Lear, Richard Bendall and Michiel Vos\n    Microbiology\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Associations between abundances of free-roaming gamebirds and common buzzards Buteo buteo are not driven by consumption of gamebirds in the buzzard breeding season\n    George JF Swan, Stuart Bearhop, Stephen M Redpath, Matthew J Silk, Daniel Padfield, Cecily ED Goodwin, Robbie A McDonald\n    Ecology & Evolution\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n  \n  \n    Bacterial colonisation dynamics of household plastics in a coastal environment\n    Luke Lear, Daniel Padfield, Tirion Dowsett, Maia Jones, Suzanne Kay, Alex Hayward, Michiel Vos\n    Science of the Total Environment\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Disturbance‐mediated invasions are dependent on community resource abundance\n    Luke Lear, Daniel Padfield, Hidetoshi Inamine, Katriona Shea, Angus Buckling\n    Ecology\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Greater phage genotypic diversity constrains arms-race coevolution\n    Meaghan Castledine, Pawel Sierocinski, Mhairi Inglis, Suzanne Kay, Alex Hayward, Angus Buckling, Daniel Padfield\n    Frontiers in Cellular and Infection Microbiology\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Parallel evolution of Pseudomonas aeruginosa phage resistance and virulence loss in response to phage treatment in vivo and in vitro\n    Meaghan Castledine, Daniel Padfield, Pawel Sierocinski, Jesica Soria Pascual, Adam Hughes, Lotta Mäkinen, Ville-Petri Friman, Jean-Paul Pirnay, Maya Merabishvili, Daniel De Vos, Angus Buckling\n    eLife\n    (2022)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    CMRnet: An R package to derive networks of social interactions and movement from mark–recapture data\n    Matthew J Silk, Robbie A McDonald, Richard J Delahay, Daniel Padfield, David J Hodgson\n    Methods in Ecology & Evolution\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n  \n  \n    Compost spatial heterogeneity promotes evolutionary diversification of a bacterium\n    Stineke van Houte, Daniel Padfield, Pedro Gómez, Adela M Luján, Michael A Brockhurst, Steve Paterson, Angus Buckling\n    Journal of Evolutionary Biology\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    The impact of propagule pressure on whole community invasions in biomethane-producing communities\n    Pawel Sierocinski, Jesica Soria Pascual, Daniel Padfield, Mike Salter, Angus Buckling\n    iScience\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Data\n    \n    \n  \n  \n    Warming impairs trophic transfer efficiency in a long-term field experiment\n    Diego R Barneche, Chris J Hulatt, Matteo Dossena, Daniel Padfield, Guy Woodward, Mark Trimmer, Gabriel Yvon-Durocher\n    Nature\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Open Access\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    rTPC and nls.multstart: A new pipeline to fit thermal performance curves in R\n    Daniel Padfield, Hannah O'Sullivan, Samraat Pawar\n    Methods in Ecology & Evolution\n    (2021)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n  \n  \n    Community coalescence: an eco-evolutionary perspective\n    Meaghan Castledine, Pawel Sierocinski, Daniel Padfield, Angus Buckling\n    Philosophical Transactions of the Royal Society B\n    (2020)\n    \n      Details\n    \n    \n      DOI\n    \n    \n  \n  \n    Evolution of diversity explains the impact of pre-adaptation of a focal species on the structure of a natural microbial community\n    Daniel Padfield, Alex Vujakovic, Steve Paterson, Rob Griffiths, Angus Buckling, Elze Hesse\n    ISME\n    (2020)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Open Access\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Experimental (co)evolution in a multi‐species microbial community results in local maladaptation\n    Meaghan Castledine, Daniel Padfield, Angus Buckling\n    Ecology Letters\n    (2020)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n    \n  \n  \n    Postrelease movement and habitat selection of translocated pine martens Martes martes\n    Catherine M McNicol, David Bavin, Stuart Bearhop, Josie Bridges, Elizabeth Croose, Robin Gill, Cecily ED Goodwin, John Lewis, Jenny MacPherson, Daniel Padfield, Henry Schofield, Matthew J Silk, Alexandra J Tomlinson, Robbie A McDonald\n    Ecology & Evolution\n    (2020)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Data\n    \n    \n  \n  \n    Temperature-dependent changes to host–parasite interactions alter the thermal performance of a bacterial host\n    Daniel Padfield, Meaghan Castledine, Angus Buckling\n    ISME\n    (2020)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Open Access\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    A shared coevolutionary history does not alter the outcome of coalescence in experimental populations of Pseudomonas fluorescens\n    Meaghan Castledine, Angus Buckling, Daniel Padfield\n    Journal of Evolutionary Biology\n    (2019)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Anthropogenic remediation of heavy metals selects against natural microbial remediation\n    Elze Hesse, Daniel Padfield, Florian Bayer, Eleanor M Van Veen, Christopher G Bryan, Angus Buckling\n    Proceedings of the Royal Society B\n    (2019)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Data\n    \n    \n  \n  \n    Linking phytoplankton community metabolism to the individual size distribution\n    Daniel Padfield, Angus Buckling, Ruth Warfield, Chris Lowe, Gabriel Yvon‐Durocher\n    Ecology Letters\n    (2018)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Code\n    \n    \n  \n  \n    Role of carbon allocation efficiency in the temperature dependence of autotroph growth rates\n    Bernardo García-Carreras, Sofía Sal, Daniel Padfield, Dimitrios-Georgios Kontopoulos, Elvire Bestion, C-Elisa Schaum, Gabriel Yvon-Durocher, Samraat Pawar\n    PNAS\n    (2018)\n    \n      Details\n    \n    \n  \n  \n    Temperature‐driven selection on metabolic traits increases the strength of an algal–grazer interaction in naturally warmed streams\n    C Elisa Schaum, Student Research Team, Richard ffrench‐Constant, Chris Lowe, Jón S Ólafsson, Daniel Padfield, Gabriel Yvon‐Durocher, Yasmin Ashton, Romina Botoli, Peter Coles, Joe Crisp, Emma Dwan, Stella Enoch‐Pledger, Briony Flello, Kate Freegard, Ceri Haines, Matthew Holland, Luke Lear, Emma Lokuciejewski, Heather McPhee, Toby Newport, Liisa Pahk, Sienna Somers, Caspar Swindells, Maria Wild, Ethan Wrigglesworth\n    Global Change Biology\n    (2018)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n  \n  \n    Metabolic compensation constrains the temperature dependence of gross primary production\n    Daniel Padfield, Chris Lowe, Angus Buckling, Richard Ffrench‐Constant, Student Research Team, Simon Jennings, Felicity Shelley, Jón S Ólafsson, Gabriel Yvon‐Durocher\n    Ecology Letters\n    (2017)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Code\n    \n    \n       Data\n    \n    \n  \n  \n    Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton\n    Daniel Padfield, Genevieve Yvon‐Durocher, Angus Buckling, Simon Jennings, Gabriel Yvon‐Durocher\n    Ecology Letters\n    (2016)\n    \n      Details\n    \n    \n      DOI\n    \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Daniel Padfield",
    "section": "",
    "text": "I am a microbial ecologist working at the University of Exeter in Penryn, Cornwall. I have broad interests that span climate change, evolution, and community ecology. My research has concentrated on how environmental change alters the interplay of ecological and evolutionary dynamics in microbial communities. I do this by combining theory and simulations with experimental evolution. In January 2023 I started a NERC Independent Research Fellowship at the University of Exeter to explore the links between climate warming and antibiotic resistance in microbial communities.\nI am an avid R programmer - authoring the R packages nls.multstart and rTPC - and specialise in the wrangling and manipulation of large datasets and statistical analysis. I also have experience in bioinformatics from 16S sequencing to de novo genome assembly, the latter using bash scripting.\nI am also a carer for my partner. She has full-body neuropathic pain, severe ME/CFS and has been bed-bound for around 4 years. She lives with her parents in Cheltenham and I visit for long weekends every three to four weeks.\nIf you are interested in collaborating or have any questions or comments I would love to hear from you."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "nls.multstart is an R package that allows more robust and reproducible non-linear regression compared to nls() or nlsLM(). These functions allow only a single starting value, meaning that it can be hard to get the best estimated model. This is especially true if the same model is fitted over the levels of a factor, which may have the same shape of curve, but be much different in terms of parameter estimates.\nnls_multstart() is the main function of nls.multstart. Similar to the R package nls2, it allows multiple starting values for each parameter and then iterates through multiple starting values, attempting a fit with each set of start parameters. The best model is then picked on AIC score. This results in a more reproducible and reliable method of fitting non-linear least squares regression in R.\nThis package is designed to work with the tidyverse, harnessing the functions within broom, tidyr, dplyr and purrr to extract estimates and plot things easily with ggplot2.\n  GitHub    CRAN"
  },
  {
    "objectID": "software.html#rtpc",
    "href": "software.html#rtpc",
    "title": "Software",
    "section": "rTPC",
    "text": "rTPC\nrTPC is an R package that helps fit thermal performance curves (TPCs) in R. rTPC contains 24 model formulations previously used to fit TPCs and has helper functions to help set sensible start parameters, upper and lower parameter limits and estimate parameters useful in downstream analyses, such as cardinal temperatures, maximum rate and optimum temperature.\nThe idea behind rTPC is to make fitting thermal performance curves easier, to provide workflows and examples of fitting TPCs without saying which model works best. Which model and which workflow is “best” is going to be down to the question that is being asked. Throughout the vignettes, Things to consider sections give some key considerations about what to consider before and during the analysis.\nWhen developing rTPC, we made a conscious decision not to repeat code and methods that are already optimised and available in the R ecosystem. Consequently, the workflows take advantage of nls.multstart for fitting non-linear least squares regression and packages from the tidyverse for data manipulation, fitting multiple models, and visualisation. The R package car is used extensively for the bootstrapping approaches.\n  GitHub    website    publication"
  },
  {
    "objectID": "software.html#rstrava",
    "href": "software.html#rstrava",
    "title": "Software",
    "section": "rStrava",
    "text": "rStrava\nI contributed to the R package rStrava, which provides an interface to the Strava API in R. Using rStrava, you can access and manipulate data related to your own activities, as well as retrieve data about athletes and activities on Strava. Some of the main functionality of rStrava includes:\n\nAuthenticating and connecting to the Strava API\nRetrieving information about the user’s profile and activities\nRetrieving information about other athletes and their activities\nDownloading data from activities and working with it in R\nEasy visualisation of ride maps and elevation profiles\n\nThe main developer and maintainer of rStrava is Marcus Beck.\n  GitHub"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Recreate Ed Hawkins’ climate spiral in R\n\n\n\n\n\nUse ggplot2 and gganimate to visualise climate change over the last\n\n\n\n\n\n\n2023-02-16\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nRolling regression to estimate microbial growth rate\n\n\n\n\n\nCalculating microbial growth rates from OD measurements using rolling regression in the tidyverse\n\n\n\n\n\n\n2019-11-15\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nrStrava and gganimate\n\n\n\n\n\nAnimate your Strava activities in R using rStrava and gganimate\n\n\n\n\n\n\n2018-05-18\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nBootstrap nls models in R\n\n\n\n\n\nBootstrap non-linear least squares regression in R with purrr and car\n\n\n\n\n\n\n2018-01-21\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nDoing robust nls regression in R\n\n\n\n\n\nFitting non-linear regressions with broom, purrr and nls.multstart\n\n\n\n\n\n\n2018-01-07\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  }
]